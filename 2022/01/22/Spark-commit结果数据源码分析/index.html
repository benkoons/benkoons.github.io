<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="源码版本：Apache Spark 2.4.7 导读：本文主要介绍 Spark 中 InsertIntoHiveTable 类操作生成数据的 commit 流程，包括整体执行流程介绍、committer 对象的生成、Task 和 Job Commit、以及最后 Hive 层 load 数据几个流程。如果想知道具体目录数据的流转情况，可以直接看总结部分。">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark commit结果数据源码分析">
<meta property="og:url" content="http://yoursite.com/2022/01/22/Spark-commit%E7%BB%93%E6%9E%9C%E6%95%B0%E6%8D%AE%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="笨小康的博客">
<meta property="og:description" content="源码版本：Apache Spark 2.4.7 导读：本文主要介绍 Spark 中 InsertIntoHiveTable 类操作生成数据的 commit 流程，包括整体执行流程介绍、committer 对象的生成、Task 和 Job Commit、以及最后 Hive 层 load 数据几个流程。如果想知道具体目录数据的流转情况，可以直接看总结部分。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-01-22T14:07:24.000Z">
<meta property="article:modified_time" content="2022-01-22T14:11:16.897Z">
<meta property="article:author" content="笨小康">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="Spark源码">
<meta property="article:tag" content="SparkSQL">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2022/01/22/Spark-commit%E7%BB%93%E6%9E%9C%E6%95%B0%E6%8D%AE%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Spark commit结果数据源码分析 | 笨小康的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">笨小康的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">星辰大海, 如期而至</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签云</a>

  </li>
        <li class="menu-item menu-item-flomo">

    <a href="/categories/flomo/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想录</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于我</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜文章
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/01/22/Spark-commit%E7%BB%93%E6%9E%9C%E6%95%B0%E6%8D%AE%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="笨小康">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笨小康的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark commit结果数据源码分析
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-22 22:07:24" itemprop="dateCreated datePublished" datetime="2022-01-22T22:07:24+08:00">2022-01-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>源码版本：Apache Spark 2.4.7</p>
<p>导读：本文主要介绍 Spark 中 InsertIntoHiveTable 类操作生成数据的 commit 流程，包括整体执行流程介绍、committer 对象的生成、Task 和 Job Commit、以及最后 Hive 层 load 数据几个流程。如果想知道具体目录数据的流转情况，可以直接看总结部分。</p>
</blockquote>
<p>本文主要介绍 Spark 中执行 insert overwrite 语句结果数据的生成细节，以 orc 格式的分区表为例，向分区表中插入数据。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test.tmp_table_par_kw (</span><br><span class="line">id <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> <span class="type">int</span>)</span><br><span class="line">stored <span class="keyword">as</span> orc;</span><br><span class="line"></span><br><span class="line"># 插入数据</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> test.tmp_table_par_kw <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="number">20220113</span>) <span class="keyword">values</span>(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p>通过 exlpain 查看物理执行计划，可以发现底层执行的是 InsertIntoHiveTable 类的 Command 逻辑。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span> (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">insert</span> overwrite <span class="keyword">table</span> tmp_table_par_kw <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="number">20220113</span>) <span class="keyword">values</span>(<span class="number">1</span>);</span><br><span class="line"><span class="operator">=</span><span class="operator">=</span> Physical Plan <span class="operator">=</span><span class="operator">=</span></span><br><span class="line"><span class="keyword">Execute</span> InsertIntoHiveTable InsertIntoHiveTable `test`.`tmp_table_par_kw`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, Map(<span class="keyword">day</span> <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">Some</span>(<span class="number">20220113</span>)), <span class="literal">true</span>, <span class="literal">false</span>, [id]</span><br><span class="line"><span class="operator">+</span><span class="operator">-</span> LocalTableScan [id#<span class="number">5</span>]</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.641</span> seconds, Fetched <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<h1 id="1-InsertIntoHiveTable整体流程"><a href="#1-InsertIntoHiveTable整体流程" class="headerlink" title="1. InsertIntoHiveTable整体流程"></a>1. InsertIntoHiveTable整体流程</h1><p>通过 Physical Plan 发现最终的执行逻辑在 InsertIntoHiveTable 类，进入到 InsertIntoHiveTable 类逻辑，processInsert 方式是处理数据插入的关键。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertIntoHiveTable</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    table: <span class="type">CatalogTable</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    partition: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Option</span>[<span class="type">String</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">    query: <span class="type">LogicalPlan</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    overwrite: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    ifPartitionNotExists: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    outputColumnNames: <span class="type">Seq</span>[<span class="type">String</span>]</span>) <span class="keyword">extends</span> <span class="title">SaveAsHiveFile</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(sparkSession: <span class="type">SparkSession</span>, child: <span class="type">SparkPlan</span>): <span class="type">Seq</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> externalCatalog = sparkSession.sharedState.externalCatalog</span><br><span class="line">    <span class="keyword">val</span> hadoopConf = sparkSession.sessionState.newHadoopConf()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> hiveQlTable = <span class="type">HiveClientImpl</span>.toHiveTable(table)</span><br><span class="line">    <span class="keyword">val</span> tableDesc = <span class="keyword">new</span> <span class="type">TableDesc</span>(</span><br><span class="line">      hiveQlTable.getInputFormatClass,</span><br><span class="line">      hiveQlTable.getOutputFormatClass,</span><br><span class="line">      hiveQlTable.getMetadata</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> tableLocation = hiveQlTable.getDataLocation</span><br><span class="line">    <span class="keyword">val</span> tmpLocation = getExternalTmpPath(sparkSession, hadoopConf, tableLocation)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 关键：处理Job数据插入</span></span><br><span class="line">      processInsert(sparkSession, externalCatalog, hadoopConf, tableDesc, tmpLocation, child)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      deleteExternalTmpPath(hadoopConf)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">CommandUtils</span>.uncacheTableOrView(sparkSession, table.identifier.quotedString)</span><br><span class="line">    sparkSession.sessionState.catalog.refreshTable(table.identifier)</span><br><span class="line"></span><br><span class="line">    <span class="type">CommandUtils</span>.updateTableStats(sparkSession, table)</span><br><span class="line"></span><br><span class="line">    <span class="type">Seq</span>.empty[<span class="type">Row</span>]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>整体上分为两个步骤，首先是 SQL 语句最终生成的数据，然后调用 Hive 的 api 将数据移动到分区目录，并向 Hive MetaStore 元数据中更新分区信息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processInsert</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      externalCatalog: <span class="type">ExternalCatalog</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      tableDesc: <span class="type">TableDesc</span>,</span><br><span class="line">      tmpLocation: <span class="type">Path</span>,</span><br><span class="line">      child: <span class="type">SparkPlan</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 关键：saveAsHiveFile写数据入口</span></span><br><span class="line">    <span class="keyword">val</span> writtenParts = saveAsHiveFile(</span><br><span class="line">      sparkSession = sparkSession,</span><br><span class="line">      plan = child,</span><br><span class="line">      hadoopConf = hadoopConf,</span><br><span class="line">      fileSinkConf = fileSinkConf,</span><br><span class="line">      outputLocation = tmpLocation.toString,</span><br><span class="line">      partitionAttributes = partitionAttributes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (partition.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">if</span> (numDynamicPartitions &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment">// 动态分区插入数据load分区</span></span><br><span class="line">        externalCatalog.loadDynamicPartitions(</span><br><span class="line">          db = table.database,</span><br><span class="line">          table = table.identifier.table,</span><br><span class="line">          tmpLocation.toString,</span><br><span class="line">          partitionSpec,</span><br><span class="line">          overwrite,</span><br><span class="line">          numDynamicPartitions)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          ...</span><br><span class="line">          <span class="comment">// 静态分区插入数据load分区</span></span><br><span class="line">          externalCatalog.loadPartition(</span><br><span class="line">            table.database,</span><br><span class="line">            table.identifier.table,</span><br><span class="line">            tmpLocation.toString,</span><br><span class="line">            partitionSpec,</span><br><span class="line">            isOverwrite = doHiveOverwrite,</span><br><span class="line">            inheritTableSpecs = inheritTableSpecs,</span><br><span class="line">            isSrcLocal = <span class="literal">false</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 非分区表load数据</span></span><br><span class="line">      externalCatalog.loadTable(</span><br><span class="line">        table.database,</span><br><span class="line">        table.identifier.table,</span><br><span class="line">        tmpLocation.toString, <span class="comment">// <span class="doctag">TODO:</span> URI</span></span><br><span class="line">        overwrite,</span><br><span class="line">        isSrcLocal = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>生成结果数据的逻辑是创建 committer 对象，用于结果数据的 commit 操作，然后通过 FileFowmatWriter.write() 方法写入数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">saveAsHiveFile</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      fileSinkConf: <span class="type">FileSinkDesc</span>,</span><br><span class="line">      outputLocation: <span class="type">String</span>,</span><br><span class="line">      customPartitionLocations: <span class="type">Map</span>[<span class="type">TablePartitionSpec</span>, <span class="type">String</span>] = <span class="type">Map</span>.empty,</span><br><span class="line">      partitionAttributes: <span class="type">Seq</span>[<span class="type">Attribute</span>] = <span class="type">Nil</span>): <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成committer对象</span></span><br><span class="line">    <span class="keyword">val</span> committer = <span class="type">FileCommitProtocol</span>.instantiate(</span><br><span class="line">      sparkSession.sessionState.conf.fileCommitProtocolClass,</span><br><span class="line">      jobId = java.util.<span class="type">UUID</span>.randomUUID().toString,</span><br><span class="line">      outputPath = outputLocation)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关键：write数据，内部涉及job和task commit数据的逻辑</span></span><br><span class="line">    <span class="type">FileFormatWriter</span>.write(</span><br><span class="line">      sparkSession = sparkSession,</span><br><span class="line">      plan = plan,</span><br><span class="line">      fileFormat = <span class="keyword">new</span> <span class="type">HiveFileFormat</span>(fileSinkConf),</span><br><span class="line">      committer = committer,</span><br><span class="line">      outputSpec =</span><br><span class="line">        <span class="type">FileFormatWriter</span>.<span class="type">OutputSpec</span>(outputLocation, customPartitionLocations, outputColumns),</span><br><span class="line">      hadoopConf = hadoopConf,</span><br><span class="line">      partitionColumns = partitionAttributes,</span><br><span class="line">      bucketSpec = <span class="type">None</span>,</span><br><span class="line">      statsTrackers = <span class="type">Seq</span>(basicWriteJobStatsTracker(hadoopConf)),</span><br><span class="line">      options = <span class="type">Map</span>.empty)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>数据的生成分为 Job Commit 和 Task Commit，Task 是 Job 的子集，executeTask() 方法是 write 数据的关键，然后 Task Commit 和 Job Commit 操作将 write 的数据 move 到相应的目录。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      fileFormat: <span class="type">FileFormat</span>,</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>,</span><br><span class="line">      outputSpec: <span class="type">OutputSpec</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      partitionColumns: <span class="type">Seq</span>[<span class="type">Attribute</span>],</span><br><span class="line">      bucketSpec: <span class="type">Option</span>[<span class="type">BucketSpec</span>],</span><br><span class="line">      statsTrackers: <span class="type">Seq</span>[<span class="type">WriteJobStatsTracker</span>],</span><br><span class="line">      options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>])</span><br><span class="line">    : <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Job commit准备</span></span><br><span class="line">    committer.setupJob(job)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> rdd = <span class="keyword">if</span> (orderingMatched) &#123;</span><br><span class="line">        plan.execute()</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// SPARK-21165: the `requiredOrdering` is based on the attributes from analyzed plan, and</span></span><br><span class="line">        <span class="comment">// the physical plan may have different attribute ids due to optimizer removing some</span></span><br><span class="line">        <span class="comment">// aliases. Here we bind the expression ahead to avoid potential attribute ids mismatch.</span></span><br><span class="line">        <span class="keyword">val</span> orderingExpr = requiredOrdering</span><br><span class="line">          .map(<span class="type">SortOrder</span>(_, <span class="type">Ascending</span>))</span><br><span class="line">          .map(<span class="type">BindReferences</span>.bindReference(_, outputSpec.outputColumns))</span><br><span class="line">        <span class="type">SortExec</span>(</span><br><span class="line">          orderingExpr,</span><br><span class="line">          global = <span class="literal">false</span>,</span><br><span class="line">          child = plan).execute()</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// SPARK-23271 If we are attempting to write a zero partition rdd, create a dummy single</span></span><br><span class="line">      <span class="comment">// partition rdd to make sure we at least set up one write task to write the metadata.</span></span><br><span class="line">      <span class="keyword">val</span> rddWithNonEmptyPartitions = <span class="keyword">if</span> (rdd.partitions.length == <span class="number">0</span>) &#123;</span><br><span class="line">        sparkSession.sparkContext.parallelize(<span class="type">Array</span>.empty[<span class="type">InternalRow</span>], <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        rdd</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> jobIdInstant = <span class="keyword">new</span> <span class="type">Date</span>().getTime</span><br><span class="line">      <span class="keyword">val</span> ret = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">WriteTaskResult</span>](rddWithNonEmptyPartitions.partitions.length)</span><br><span class="line">      <span class="comment">// Task commit数据逻辑</span></span><br><span class="line">      sparkSession.sparkContext.runJob(</span><br><span class="line">        rddWithNonEmptyPartitions,</span><br><span class="line">        (taskContext: <span class="type">TaskContext</span>, iter: <span class="type">Iterator</span>[<span class="type">InternalRow</span>]) =&gt; &#123;</span><br><span class="line">          executeTask(</span><br><span class="line">            description = description,</span><br><span class="line">            jobIdInstant = jobIdInstant,</span><br><span class="line">            sparkStageId = taskContext.stageId(),</span><br><span class="line">            sparkPartitionId = taskContext.partitionId(),</span><br><span class="line">            sparkAttemptNumber = taskContext.taskAttemptId().toInt &amp; <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>,</span><br><span class="line">            committer,</span><br><span class="line">            iterator = iter)</span><br><span class="line">        &#125;,</span><br><span class="line">        rddWithNonEmptyPartitions.partitions.indices,</span><br><span class="line">        (index, res: <span class="type">WriteTaskResult</span>) =&gt; &#123;</span><br><span class="line">          committer.onTaskCommit(res.commitMsg)</span><br><span class="line">          ret(index) = res</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> commitMsgs = ret.map(_.commitMsg)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 执行 Job commit 数据逻辑</span></span><br><span class="line">      committer.commitJob(job, commitMsgs)</span><br><span class="line">      logInfo(<span class="string">s&quot;Write Job <span class="subst">$&#123;description.uuid&#125;</span> committed.&quot;</span>)</span><br><span class="line"></span><br><span class="line">      processStats(description.statsTrackers, ret.map(_.summary.stats))</span><br><span class="line">      logInfo(<span class="string">s&quot;Finished processing stats for write job <span class="subst">$&#123;description.uuid&#125;</span>.&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// return a set of all the partition paths that were updated during this job</span></span><br><span class="line">      ret.map(_.summary.updatedPartitions).reduceOption(_ ++ _).getOrElse(<span class="type">Set</span>.empty)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123; <span class="keyword">case</span> cause: <span class="type">Throwable</span> =&gt;</span><br><span class="line">      logError(<span class="string">s&quot;Aborting job <span class="subst">$&#123;description.uuid&#125;</span>.&quot;</span>, cause)</span><br><span class="line">      committer.abortJob(job)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;Job aborted.&quot;</span>, cause)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，InsertIntoHiveTable 的整体流程就介绍完毕。</p>
<h1 id="2-Committer对象创建"><a href="#2-Committer对象创建" class="headerlink" title="2. Committer对象创建"></a>2. Committer对象创建</h1><p>Task Commit 和 Job Commit 是内部生成数据的关键，而生成数据是需要相应的 committer 对象作为入口，用来 commitTask 和 commitTask。</p>
<p>通过 FileCommitProtocol.instantiate() 方法生成 committer 对象，committer 对象由 <code>sparkSession.sessionState.conf.fileCommitProtoclClass</code>决定，这个值由参数 spark.sql.sources.commitProtocolClass 控制， 默认为 SQLHadoopMapReduceCommitProtocol，可以通过这里实现自定义的 commitProtocol。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">saveAsHiveFile</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      fileSinkConf: <span class="type">FileSinkDesc</span>,</span><br><span class="line">      outputLocation: <span class="type">String</span>,</span><br><span class="line">      customPartitionLocations: <span class="type">Map</span>[<span class="type">TablePartitionSpec</span>, <span class="type">String</span>] = <span class="type">Map</span>.empty,</span><br><span class="line">      partitionAttributes: <span class="type">Seq</span>[<span class="type">Attribute</span>] = <span class="type">Nil</span>): <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成committer对象</span></span><br><span class="line">    <span class="keyword">val</span> committer = <span class="type">FileCommitProtocol</span>.instantiate(</span><br><span class="line">      sparkSession.sessionState.conf.fileCommitProtocolClass,</span><br><span class="line">      jobId = java.util.<span class="type">UUID</span>.randomUUID().toString,</span><br><span class="line">      outputPath = outputLocation)</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><code>sparkSession.sessionState.conf.fileCommitProtocolClass</code>使用的具体类的定义。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/internal/SQLConf.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fileCommitProtocolClass</span></span>: <span class="type">String</span> = getConf(<span class="type">SQLConf</span>.<span class="type">FILE_COMMIT_PROTOCOL_CLASS</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> <span class="type">FILE_COMMIT_PROTOCOL_CLASS</span> =</span><br><span class="line">    buildConf(<span class="string">&quot;spark.sql.sources.commitProtocolClass&quot;</span>)</span><br><span class="line">      .internal()</span><br><span class="line">      .stringConf</span><br><span class="line">      .createWithDefault(</span><br><span class="line">        <span class="string">&quot;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>这里通过 Java 的反射机制生成 committer 对象，为 SQLHadoopMapReduceCommitProtocol 对象，SQLHadoopMapReduceCommitProtocol 类是继承自 HadoopMapReduceCommitProtocol 类，只是重写了其 setupCommitter 方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/FileCommitProtocol.scala</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FileCommitProtocol</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">TaskCommitMessage</span>(<span class="params">val obj: <span class="type">Any</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span></span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">EmptyTaskCommitMessage</span> <span class="keyword">extends</span> <span class="title">TaskCommitMessage</span>(<span class="params">null</span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Instantiates a FileCommitProtocol using the given className.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">instantiate</span></span>(</span><br><span class="line">      className: <span class="type">String</span>,</span><br><span class="line">      jobId: <span class="type">String</span>,</span><br><span class="line">      outputPath: <span class="type">String</span>,</span><br><span class="line">      dynamicPartitionOverwrite: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FileCommitProtocol</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(className).asInstanceOf[<span class="type">Class</span>[<span class="type">FileCommitProtocol</span>]]</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> ctor = clazz.getDeclaredConstructor(classOf[<span class="type">String</span>], classOf[<span class="type">String</span>], classOf[<span class="type">Boolean</span>])</span><br><span class="line">      logDebug(<span class="string">&quot;Using (String, String, Boolean) constructor&quot;</span>)</span><br><span class="line">      ctor.newInstance(jobId, outputPath, dynamicPartitionOverwrite.asInstanceOf[java.lang.<span class="type">Boolean</span>])</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">NoSuchMethodException</span> =&gt;</span><br><span class="line">        logDebug(<span class="string">&quot;Falling back to (String, String) constructor&quot;</span>)</span><br><span class="line">        require(!dynamicPartitionOverwrite,</span><br><span class="line">          <span class="string">&quot;Dynamic Partition Overwrite is enabled but&quot;</span> +</span><br><span class="line">            <span class="string">s&quot; the committer <span class="subst">$&#123;className&#125;</span> does not have the appropriate constructor&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> ctor = clazz.getDeclaredConstructor(classOf[<span class="type">String</span>], classOf[<span class="type">String</span>])</span><br><span class="line">        ctor.newInstance(jobId, outputPath)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在介绍 InsertIntoHiveTable 流程中，我们发现在 FileFormatWriter 类中 <code>committer.setupJob(job)</code>方法内部又会创建一个 committer 对象，其中 setupCommitter 方法的作用是生成 HadoopMapReduceCommitProtocol 内部的 committer 对象，而这个对象是 OutputCommitter 类型，其 commitTask 和 commitJob 方法会被用于 commit 每个 task 的结果和整个 job 的结果。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setupJob</span></span>(jobContext: <span class="type">JobContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Setup IDs</span></span><br><span class="line">    <span class="keyword">val</span> jobId = <span class="type">SparkHadoopWriterUtils</span>.createJobID(<span class="keyword">new</span> <span class="type">Date</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> taskId = <span class="keyword">new</span> <span class="type">TaskID</span>(jobId, <span class="type">TaskType</span>.<span class="type">MAP</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> taskAttemptId = <span class="keyword">new</span> <span class="type">TaskAttemptID</span>(taskId, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set up the configuration object</span></span><br><span class="line">    jobContext.getConfiguration.set(<span class="string">&quot;mapreduce.job.id&quot;</span>, jobId.toString)</span><br><span class="line">    jobContext.getConfiguration.set(<span class="string">&quot;mapreduce.task.id&quot;</span>, taskAttemptId.getTaskID.toString)</span><br><span class="line">    jobContext.getConfiguration.set(<span class="string">&quot;mapreduce.task.attempt.id&quot;</span>, taskAttemptId.toString)</span><br><span class="line">    jobContext.getConfiguration.setBoolean(<span class="string">&quot;mapreduce.task.ismap&quot;</span>, <span class="literal">true</span>)</span><br><span class="line">    jobContext.getConfiguration.setInt(<span class="string">&quot;mapreduce.task.partition&quot;</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> taskAttemptContext = <span class="keyword">new</span> <span class="type">TaskAttemptContextImpl</span>(jobContext.getConfiguration, taskAttemptId)</span><br><span class="line">    committer = setupCommitter(taskAttemptContext)</span><br><span class="line">    committer.setupJob(jobContext)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>注意，这里有两层 committer 对象，一层就是 HadoopMapReduceCommitProtocol 本身，另一层是 HadoopMapReduceCommitProtocol 内部的 committer，为 OutputCommitter 类型。在 HadoopMapReduceCommitProtocol 的 commitTask 和 commitJob 方法中都会直接或间接地调用其内部 commiter 对象的对应方法。</p>
<h1 id="3-Task-Commit流程"><a href="#3-Task-Commit流程" class="headerlink" title="3. Task Commit流程"></a>3. Task Commit流程</h1><p>Task Commit 逻辑是用于生成一个 Job 的结果数据，并通过 commit 逻辑生成数据到相关目录。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      fileFormat: <span class="type">FileFormat</span>,</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>,</span><br><span class="line">      outputSpec: <span class="type">OutputSpec</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      partitionColumns: <span class="type">Seq</span>[<span class="type">Attribute</span>],</span><br><span class="line">      bucketSpec: <span class="type">Option</span>[<span class="type">BucketSpec</span>],</span><br><span class="line">      statsTrackers: <span class="type">Seq</span>[<span class="type">WriteJobStatsTracker</span>],</span><br><span class="line">      options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>])</span><br><span class="line">    : <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">			...</span><br><span class="line">      <span class="comment">// Task commit数据逻辑</span></span><br><span class="line">      sparkSession.sparkContext.runJob(</span><br><span class="line">        rddWithNonEmptyPartitions,</span><br><span class="line">        (taskContext: <span class="type">TaskContext</span>, iter: <span class="type">Iterator</span>[<span class="type">InternalRow</span>]) =&gt; &#123;</span><br><span class="line">          executeTask(</span><br><span class="line">            description = description,</span><br><span class="line">            jobIdInstant = jobIdInstant,</span><br><span class="line">            sparkStageId = taskContext.stageId(),</span><br><span class="line">            sparkPartitionId = taskContext.partitionId(),</span><br><span class="line">            sparkAttemptNumber = taskContext.taskAttemptId().toInt &amp; <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>,</span><br><span class="line">            committer,</span><br><span class="line">            iterator = iter)</span><br><span class="line">        &#125;,</span><br><span class="line">        rddWithNonEmptyPartitions.partitions.indices,</span><br><span class="line">        (index, res: <span class="type">WriteTaskResult</span>) =&gt; &#123;</span><br><span class="line">          committer.onTaskCommit(res.commitMsg)</span><br><span class="line">          ret(index) = res</span><br><span class="line">        &#125;)</span><br><span class="line">			...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>executeTask() 中会通过 committer 对象初始化 Task Commit 相关环境信息，接着通过 DataWriter 写入数据，最后调用 dataWriter.commit() 执行 Task 的 Commit 逻辑。这里的 commiter 对象是 HadoopMapReduceCommitProtocol 类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line">  <span class="comment">/** Writes data out in a single Spark task. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">executeTask</span></span>(</span><br><span class="line">      description: <span class="type">WriteJobDescription</span>,</span><br><span class="line">      jobIdInstant: <span class="type">Long</span>,</span><br><span class="line">      sparkStageId: <span class="type">Int</span>,</span><br><span class="line">      sparkPartitionId: <span class="type">Int</span>,</span><br><span class="line">      sparkAttemptNumber: <span class="type">Int</span>,</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>,</span><br><span class="line">      iterator: <span class="type">Iterator</span>[<span class="type">InternalRow</span>]): <span class="type">WriteTaskResult</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置Task信息</span></span><br><span class="line">    committer.setupTask(taskAttemptContext)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataWriter =</span><br><span class="line">      <span class="keyword">if</span> (sparkPartitionId != <span class="number">0</span> &amp;&amp; !iterator.hasNext) &#123;</span><br><span class="line">        <span class="comment">// In case of empty job, leave first partition to save meta for file format like parquet.</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">EmptyDirectoryDataWriter</span>(description, taskAttemptContext, committer)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (description.partitionColumns.isEmpty &amp;&amp; description.bucketIdExpression.isEmpty) &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SingleDirectoryDataWriter</span>(description, taskAttemptContext, committer)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">DynamicPartitionDataWriter</span>(description, taskAttemptContext, committer)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="type">Utils</span>.tryWithSafeFinallyAndFailureCallbacks(block = &#123;</span><br><span class="line">        <span class="comment">// Execute the task to write rows out and commit the task.</span></span><br><span class="line">        <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">          <span class="comment">//通过 DataWriter写数据</span></span><br><span class="line">          dataWriter.write(iterator.next())</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 写完数据后进行commit</span></span><br><span class="line">        dataWriter.commit()</span><br><span class="line">      &#125;)(catchBlock = &#123;</span><br><span class="line">        <span class="comment">// If there is an error, abort the task</span></span><br><span class="line">        dataWriter.abort()</span><br><span class="line">        logError(<span class="string">s&quot;Job <span class="subst">$jobId</span> aborted.&quot;</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">FetchFailedException</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;Task failed while writing rows.&quot;</span>, t)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里的 commiter 对象还是 HadoopMapReduceCommitProtocol 类型，通过 committer 对象的 commitTask 提交 Task Commit。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatDataWriter.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">commit</span></span>(): <span class="type">WriteTaskResult</span> = &#123;</span><br><span class="line">    releaseResources()</span><br><span class="line">    <span class="keyword">val</span> summary = <span class="type">ExecutedWriteSummary</span>(</span><br><span class="line">      updatedPartitions = updatedPartitions.toSet,</span><br><span class="line">      stats = statsTrackers.map(_.getFinalStats()))</span><br><span class="line">    <span class="type">WriteTaskResult</span>(committer.commitTask(taskAttemptContext), summary)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>通过 SparkHadoopMapRedUtil 类的 commitTask 间接执行 commitTask 操作。此时 commitTask 方法中的 committer 参数已经是 HadoopMapReduceCommitProtocol 类内部的 commiter 对象，为 OutputCommitter 类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">commitTask</span></span>(taskContext: <span class="type">TaskAttemptContext</span>): <span class="type">TaskCommitMessage</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> attemptId = taskContext.getTaskAttemptID</span><br><span class="line">    <span class="type">SparkHadoopMapRedUtil</span>.commitTask(</span><br><span class="line">      committer, taskContext, attemptId.getJobID.getId, attemptId.getTaskID.getId)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">TaskCommitMessage</span>(addedAbsPathFiles.toMap -&gt; partitionPaths.toSet)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里通过 HadoopMapReduceCommitProtocol 类内部的 OutputCommitter 类型的 commiter 对象，调用 committer.commitTask() 方法，当然 commit 前还会进行一些 check 操作，判断是否能够执行 commit 操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/mapred/SparkHadoopMapRedUtil.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">commitTask</span></span>(</span><br><span class="line">      committer: <span class="type">MapReduceOutputCommitter</span>,</span><br><span class="line">      mrTaskContext: <span class="type">MapReduceTaskAttemptContext</span>,</span><br><span class="line">      jobId: <span class="type">Int</span>,</span><br><span class="line">      splitId: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mrTaskAttemptID = mrTaskContext.getTaskAttemptID</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Called after we have decided to commit</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">performCommit</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        committer.commitTask(mrTaskContext)</span><br><span class="line">        logInfo(<span class="string">s&quot;<span class="subst">$mrTaskAttemptID</span>: Committed&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> cause: <span class="type">IOException</span> =&gt;</span><br><span class="line">          logError(<span class="string">s&quot;Error committing the output of task: <span class="subst">$mrTaskAttemptID</span>&quot;</span>, cause)</span><br><span class="line">          committer.abortTask(mrTaskContext)</span><br><span class="line">          <span class="keyword">throw</span> cause</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// First, check whether the task&#x27;s output has already been committed by some other attempt</span></span><br><span class="line">    <span class="keyword">if</span> (committer.needsTaskCommit(mrTaskContext)) &#123;</span><br><span class="line">      <span class="keyword">val</span> shouldCoordinateWithDriver: <span class="type">Boolean</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="type">SparkEnv</span>.get.conf</span><br><span class="line">        sparkConf.getBoolean(<span class="string">&quot;spark.hadoop.outputCommitCoordination.enabled&quot;</span>, defaultValue = <span class="literal">true</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (shouldCoordinateWithDriver) &#123;</span><br><span class="line">        <span class="keyword">val</span> outputCommitCoordinator = <span class="type">SparkEnv</span>.get.outputCommitCoordinator</span><br><span class="line">        <span class="keyword">val</span> ctx = <span class="type">TaskContext</span>.get()</span><br><span class="line">        <span class="keyword">val</span> canCommit = outputCommitCoordinator.canCommit(ctx.stageId(), ctx.stageAttemptNumber(),</span><br><span class="line">          splitId, ctx.attemptNumber())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (canCommit) &#123;</span><br><span class="line">          performCommit()</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> message =</span><br><span class="line">            <span class="string">s&quot;<span class="subst">$mrTaskAttemptID</span>: Not committed because the driver did not authorize commit&quot;</span></span><br><span class="line">          logInfo(message)</span><br><span class="line">          <span class="comment">// We need to abort the task so that the driver can reschedule new attempts, if necessary</span></span><br><span class="line">          committer.abortTask(mrTaskContext)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">CommitDeniedException</span>(message, ctx.stageId(), splitId, ctx.attemptNumber())</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 执行commit逻辑</span></span><br><span class="line">        performCommit()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Some other attempt committed the output, so we do nothing and signal success</span></span><br><span class="line">      logInfo(<span class="string">s&quot;No need to commit output of task because needsTaskCommit=false: <span class="subst">$mrTaskAttemptID</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>commitTask 的真正执行逻辑，也就是前面提到的 Task Commit 操作，这里会将 Task 任务结束 write 的数据移动到 Task Commit 逻辑的目标目录。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.class</span></span><br><span class="line">  public void commitTask(<span class="type">TaskAttemptContext</span> context) </span><br><span class="line">  <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">    commitTask(context, <span class="literal">null</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void commitTask(<span class="type">TaskAttemptContext</span> context, <span class="type">Path</span> taskAttemptPath) </span><br><span class="line">  <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">    <span class="type">TaskAttemptID</span> attemptId = context.getTaskAttemptID();</span><br><span class="line">    <span class="keyword">if</span> (hasOutputPath()) &#123;</span><br><span class="line">      context.progress();</span><br><span class="line">      <span class="keyword">if</span>(taskAttemptPath == <span class="literal">null</span>) &#123;</span><br><span class="line">        taskAttemptPath = getTaskAttemptPath(context);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">Path</span> committedTaskPath = getCommittedTaskPath(context);</span><br><span class="line">      <span class="type">FileSystem</span> fs = taskAttemptPath.getFileSystem(context.getConfiguration());</span><br><span class="line">      <span class="keyword">if</span> (fs.exists(taskAttemptPath)) &#123;</span><br><span class="line">        <span class="keyword">if</span>(fs.exists(committedTaskPath)) &#123;</span><br><span class="line">          <span class="keyword">if</span>(!fs.delete(committedTaskPath, <span class="literal">true</span>)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">&quot;Could not delete &quot;</span> + committedTaskPath);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关键：将 Task 生成的数据 move 到目标 HDFS 目录</span></span><br><span class="line">        <span class="keyword">if</span>(!fs.rename(taskAttemptPath, committedTaskPath)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">&quot;Could not rename &quot;</span> + taskAttemptPath + <span class="string">&quot; to &quot;</span></span><br><span class="line">              + committedTaskPath);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">LOG</span>.info(<span class="string">&quot;Saved output of task &#x27;&quot;</span> + attemptId + <span class="string">&quot;&#x27; to &quot;</span> + </span><br><span class="line">            committedTaskPath);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">LOG</span>.warn(<span class="string">&quot;No Output found for &quot;</span> + attemptId);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">LOG</span>.warn(<span class="string">&quot;Output Path is null in commitTask()&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，Task Commit 的逻辑已执行结束。</p>
<h1 id="4-Job-Commit流程"><a href="#4-Job-Commit流程" class="headerlink" title="4. Job Commit流程"></a>4. Job Commit流程</h1><p>Job Commit 逻辑并不涉及到写数据的逻辑，然后将 Task Commit 生成的数据进一步移动到目标目录。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      fileFormat: <span class="type">FileFormat</span>,</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>,</span><br><span class="line">      outputSpec: <span class="type">OutputSpec</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      partitionColumns: <span class="type">Seq</span>[<span class="type">Attribute</span>],</span><br><span class="line">      bucketSpec: <span class="type">Option</span>[<span class="type">BucketSpec</span>],</span><br><span class="line">      statsTrackers: <span class="type">Seq</span>[<span class="type">WriteJobStatsTracker</span>],</span><br><span class="line">      options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>])</span><br><span class="line">    : <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Job commit准备</span></span><br><span class="line">    committer.setupJob(job)</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 执行 Job commit 数据逻辑</span></span><br><span class="line">    committer.commitJob(job, commitMsgs)</span><br><span class="line">    logInfo(<span class="string">s&quot;Write Job <span class="subst">$&#123;description.uuid&#125;</span> committed.&quot;</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>  Job Commit 逻辑依赖内部 OutputCommitter 类型的 commiter 对象，并进行相应的 commitJob 操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">commitJob</span></span>(jobContext: <span class="type">JobContext</span>, taskCommits: <span class="type">Seq</span>[<span class="type">TaskCommitMessage</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Job Commit 逻辑依赖内部的 commiter 对象（即 OutputCommitter 类型的对象）</span></span><br><span class="line">    committer.commitJob(jobContext)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (hasValidPath) &#123;</span><br><span class="line">      <span class="keyword">val</span> (allAbsPathFiles, allPartitionPaths) =</span><br><span class="line">        taskCommits.map(_.obj.asInstanceOf[(<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>], <span class="type">Set</span>[<span class="type">String</span>])]).unzip</span><br><span class="line">      <span class="keyword">val</span> fs = stagingDir.getFileSystem(jobContext.getConfiguration)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> filesToMove = allAbsPathFiles.foldLeft(<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]())(_ ++ _)</span><br><span class="line">      logDebug(<span class="string">s&quot;Committing files staged for absolute locations <span class="subst">$filesToMove</span>&quot;</span>)</span><br><span class="line">      <span class="keyword">if</span> (dynamicPartitionOverwrite) &#123;</span><br><span class="line">        <span class="keyword">val</span> absPartitionPaths = filesToMove.values.map(<span class="keyword">new</span> <span class="type">Path</span>(_).getParent).toSet</span><br><span class="line">        logDebug(<span class="string">s&quot;Clean up absolute partition directories for overwriting: <span class="subst">$absPartitionPaths</span>&quot;</span>)</span><br><span class="line">        absPartitionPaths.foreach(fs.delete(_, <span class="literal">true</span>))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">for</span> ((src, dst) &lt;- filesToMove) &#123;</span><br><span class="line">        fs.rename(<span class="keyword">new</span> <span class="type">Path</span>(src), <span class="keyword">new</span> <span class="type">Path</span>(dst))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (dynamicPartitionOverwrite) &#123;</span><br><span class="line">        <span class="keyword">val</span> partitionPaths = allPartitionPaths.foldLeft(<span class="type">Set</span>[<span class="type">String</span>]())(_ ++ _)</span><br><span class="line">        logDebug(<span class="string">s&quot;Clean up default partition directories for overwriting: <span class="subst">$partitionPaths</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> (part &lt;- partitionPaths) &#123;</span><br><span class="line">          <span class="keyword">val</span> finalPartPath = <span class="keyword">new</span> <span class="type">Path</span>(path, part)</span><br><span class="line">          <span class="keyword">if</span> (!fs.delete(finalPartPath, <span class="literal">true</span>) &amp;&amp; !fs.exists(finalPartPath.getParent)) &#123;</span><br><span class="line">            fs.mkdirs(finalPartPath.getParent)</span><br><span class="line">          &#125;</span><br><span class="line">          fs.rename(<span class="keyword">new</span> <span class="type">Path</span>(stagingDir, part), finalPartPath)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      fs.delete(stagingDir, <span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>前面也说了 commitJob 并不 write 数据，而是将 Task Commit 生成的数据移动到目标目录，所以 Job Commit 的关键就在于移动所有 Task 任务生成的数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.class</span></span><br><span class="line">  public void commitJob(<span class="type">JobContext</span> context) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (hasOutputPath()) &#123;</span><br><span class="line">      <span class="type">Path</span> finalOutput = getOutputPath();</span><br><span class="line">      <span class="type">FileSystem</span> fs = finalOutput.getFileSystem(context.getConfiguration());</span><br><span class="line">      <span class="keyword">for</span>(<span class="type">FileStatus</span> stat: getAllCommittedTaskPaths(context)) &#123;</span><br><span class="line">        <span class="comment">// 将所有 Task Commit 生成的数据移动到目标目录</span></span><br><span class="line">        mergePaths(fs, stat, finalOutput);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// delete the _temporary folder and create a _done file in the o/p folder</span></span><br><span class="line">      cleanupJob(context);</span><br><span class="line">      <span class="comment">// True if the job requires output.dir marked on successful job.</span></span><br><span class="line">      <span class="comment">// Note that by default it is set to true.</span></span><br><span class="line">      <span class="keyword">if</span> (context.getConfiguration().getBoolean(<span class="type">SUCCESSFUL_JOB_OUTPUT_DIR_MARKER</span>, <span class="literal">true</span>)) &#123;</span><br><span class="line">        <span class="type">Path</span> markerPath = <span class="keyword">new</span> <span class="type">Path</span>(outputPath, <span class="type">SUCCEEDED_FILE_NAME</span>);</span><br><span class="line">        <span class="comment">// 创建 _SUCCESS 文件，表示 Job 执行成功</span></span><br><span class="line">        fs.create(markerPath).close();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">LOG</span>.warn(<span class="string">&quot;Output Path is null in commitJob()&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="5-Hive-load数据流程"><a href="#5-Hive-load数据流程" class="headerlink" title="5. Hive load数据流程"></a>5. Hive load数据流程</h1><p>InsertIntoHiveTable 的 processInsert() 方法会调用 SaveAsHiveFile.saveAsHiveFile 进行 Hive 文件的写入，写入的文件最终都会 commit 到 ./.hive-staging_*/-ext-10000 目录中，那么 .hive-staging_*目录又是怎么被 move 到 Hive table 的 location 目录下的呢？这个工作是在 processInsert 方法调用完 SaveAsHiveFile.saveAsHiveFile 方法后，再通过调用 org.apache.hadoop.hive.ql.metadata.Hive 的 load 方法完成的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processInsert</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      externalCatalog: <span class="type">ExternalCatalog</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      tableDesc: <span class="type">TableDesc</span>,</span><br><span class="line">      tmpLocation: <span class="type">Path</span>,</span><br><span class="line">      child: <span class="type">SparkPlan</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 关键：saveAsHiveFile写数据入口</span></span><br><span class="line">    <span class="keyword">val</span> writtenParts = saveAsHiveFile(</span><br><span class="line">      sparkSession = sparkSession,</span><br><span class="line">      plan = child,</span><br><span class="line">      hadoopConf = hadoopConf,</span><br><span class="line">      fileSinkConf = fileSinkConf,</span><br><span class="line">      outputLocation = tmpLocation.toString,</span><br><span class="line">      partitionAttributes = partitionAttributes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (partition.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">if</span> (numDynamicPartitions &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment">// 动态分区插入数据load分区</span></span><br><span class="line">        externalCatalog.loadDynamicPartitions(</span><br><span class="line">          db = table.database,</span><br><span class="line">          table = table.identifier.table,</span><br><span class="line">          tmpLocation.toString,</span><br><span class="line">          partitionSpec,</span><br><span class="line">          overwrite,</span><br><span class="line">          numDynamicPartitions)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          ...</span><br><span class="line">          <span class="comment">// 静态分区插入数据load分区</span></span><br><span class="line">          externalCatalog.loadPartition(</span><br><span class="line">            table.database,</span><br><span class="line">            table.identifier.table,</span><br><span class="line">            tmpLocation.toString,</span><br><span class="line">            partitionSpec,</span><br><span class="line">            isOverwrite = doHiveOverwrite,</span><br><span class="line">            inheritTableSpecs = inheritTableSpecs,</span><br><span class="line">            isSrcLocal = <span class="literal">false</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 非分区表load数据</span></span><br><span class="line">      externalCatalog.loadTable(</span><br><span class="line">        table.database,</span><br><span class="line">        table.identifier.table,</span><br><span class="line">        tmpLocation.toString, <span class="comment">// <span class="doctag">TODO:</span> URI</span></span><br><span class="line">        overwrite,</span><br><span class="line">        isSrcLocal = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h1><p>Spark SQL 中 InsertIntoHiveTable commit 数据流程大致分为如下几步：</p>
<ul>
<li><p>构造 committer 对象，包括 HadoopMapReduceCommitProtocol 自身的 committer 对象和内部的 OutputCommiter 的 committer 对象；</p>
</li>
<li><p>commit 流程分为 Task Commit 和 Job Commit，Task Commit 先操作 Task 级别的数据，然后 Job Commit 把 Task Commit 的结果数据进行操作；</p>
</li>
<li><p>commit 前创建<code>/db/table/.hive-staging_hive_*/-ext-10000/</code>目录；</p>
</li>
<li><p>Task Commit 的操作逻辑为把 <code>/db/table/.hive-staging_*/-ext-10000/_temporary/0/_temporary/task_20220119172519_0006_m_000000/</code>目录移动到 <code>/db/table/.hive-staging_*/-ext-10000/_temporary/0/task_20220119172519_0006_m_000000/</code>目录，如果有多个 Task 目录则执行相同的操作；</p>
</li>
<li><p>Job Commit 的操作逻辑为把<code>/db/table/.hive-staging_*/-ext-10000/_temporary/0/task_20220119172519_0006_m_000000/</code>目录的数据移动到<code>/db/table/.hive-staging_*/-ext-10000/</code>目录，然后生成 <code>_SUCCESS</code>文件；</p>
</li>
<li><p>Hive load 数据的操作逻辑为把<code>/db/table/.hive-staging_*/-ext-10000/</code>目录的数据 load 到对应分区目录 <code>/db/table/.hive-staging_*/-ext-10000/pr=520</code>，并添加对应分区信息。</p>
</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/01ab5f0f22df">Spark InsertIntoHiveTable如何commit结果数据</a></li>
</ul>

    </div>

    
    
    

    <div>
      
      <div>
	 
		<div style="text-align:center;color:#bfbfbf;font-size:16px;"> 
			<span>-------- 本文结束 </span> <i class="fa fa-paw"></i> <span> 感谢阅读 --------</span>
		</div> 
	
</div>
      
    </div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    请我喝杯咖啡~
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="笨小康 微信打赏">
        <p>微信打赏</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="笨小康 支付宝打赏">
        <p>支付宝打赏</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Spark/" rel="tag"><i class="fa fa-tag"></i> Spark</a>
              <a href="/tags/Spark%E6%BA%90%E7%A0%81/" rel="tag"><i class="fa fa-tag"></i> Spark源码</a>
              <a href="/tags/SparkSQL/" rel="tag"><i class="fa fa-tag"></i> SparkSQL</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/22/Spark-insert-overwrite%E6%9D%83%E9%99%90%E8%A2%AB%E4%BF%AE%E6%94%B9%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/" rel="prev" title="Spark insert overwrite权限被修改问题分析">
      <i class="fa fa-chevron-left"></i> Spark insert overwrite权限被修改问题分析
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/05/08/Spark-HDFS%E8%B7%AF%E5%BE%84%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E5%AE%9E%E7%8E%B0/" rel="next" title="Spark HDFS路径血缘关系实现">
      Spark HDFS路径血缘关系实现 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  




          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-InsertIntoHiveTable%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="nav-text">1. InsertIntoHiveTable整体流程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Committer%E5%AF%B9%E8%B1%A1%E5%88%9B%E5%BB%BA"><span class="nav-text">2. Committer对象创建</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Task-Commit%E6%B5%81%E7%A8%8B"><span class="nav-text">3. Task Commit流程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Job-Commit%E6%B5%81%E7%A8%8B"><span class="nav-text">4. Job Commit流程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Hive-load%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="nav-text">5. Hive load数据流程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-%E6%80%BB%E7%BB%93"><span class="nav-text">6. 总结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">笨小康</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">27</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">笨小康</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
