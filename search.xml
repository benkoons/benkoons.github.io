<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hexo+Github Pages博客搭建手册</title>
    <url>/2021/11/09/Hexo-Github-Pages%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</url>
    <content><![CDATA[<p>花了几天时间，终于搭建了心满意足的个人网站，欢迎来访：<a href="https://benkoons.github.io/">笨小康的个人网站</a>。本文主要介绍如何使用 Hexo + Github Pages 搭建个人网站，并发布自己的第一篇文章。</p>
<h1 id="1-Hexo-Github-Pages介绍"><a href="#1-Hexo-Github-Pages介绍" class="headerlink" title="1. Hexo/Github Pages介绍"></a>1. Hexo/Github Pages介绍</h1><p>Hexo是一款基于Node.js的静态博客框架，可以方便的生成静态网页托管在GitHub上，是搭建博客的首选框架。大家可以进入 <a href="https://hexo.io/zh-cn/docs/">Hexo</a> 官网进行详细查看，因为Hexo的创建者是台湾人，对中文的支持很友好，可以选择中文进行查看。</p>
<p>Github Pages 是 Github 提供的一个静态站点托管服务，旨在直接从 GitHub 仓库中直接托管您的个人、组织或项目页面。</p>
<h1 id="2-Hexo搭建博客"><a href="#2-Hexo搭建博客" class="headerlink" title="2. Hexo搭建博客"></a>2. Hexo搭建博客</h1><p>本博客搭建文档主要是针对 mac 电脑。</p>
<h2 id="2-1-安装Homebrew"><a href="#2-1-安装Homebrew" class="headerlink" title="2.1 安装Homebrew"></a>2.1 安装Homebrew</h2><p>mac 系统和 Linux 比较类似，安装软件也可以通过 shell 命令来完成，和 Linux 的 yum install 命令类似，mac 提供了 brew install 的方式安装软件，brew 命令使用需要安装 Homebrew。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</span><br></pre></td></tr></table></figure>

<h2 id="2-2-安装git"><a href="#2-2-安装git" class="headerlink" title="2.2 安装git"></a>2.2 安装git</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装命令</span></span><br><span class="line">brew install git</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看版本</span></span><br><span class="line">git -v</span><br></pre></td></tr></table></figure>

<h2 id="2-3-安装nodejs"><a href="#2-3-安装nodejs" class="headerlink" title="2.3 安装nodejs"></a>2.3 安装nodejs</h2><p>Hexo是基于nodeJS编写的，所以需要安装一下nodeJs和里面的npm工具。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装命令</span></span><br><span class="line">brew install node</span><br><span class="line">brew install npm</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看版本</span></span><br><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure>

<h2 id="2-4-安装Hexo"><a href="#2-4-安装Hexo" class="headerlink" title="2.4 安装Hexo"></a>2.4 安装Hexo</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装命令</span></span><br><span class="line">npm install -g hexo-cli</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看版本</span></span><br><span class="line">hexo -v</span><br></pre></td></tr></table></figure>

<h2 id="2-5-初始化Hexo"><a href="#2-5-初始化Hexo" class="headerlink" title="2.5 初始化Hexo"></a>2.5 初始化Hexo</h2><p>首先，对 Hexo 目录（目录名自定义，作为本地博客存储仓库）进行初始化，并安装相关依赖。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 初始化 Hexo，blog 名字自定义</span></span><br><span class="line">hexo init blog</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入到 blog 文件夹</span></span><br><span class="line">cd blog</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装依赖</span></span><br><span class="line">npm install</span><br></pre></td></tr></table></figure>

<p>文件夹下包括一些文件和目录：</p>
<ul>
<li><p>node_modules: 依赖包</p>
</li>
<li><p>public：存放生成的页面</p>
</li>
<li><p>scaffolds：生成文章的一些模板</p>
</li>
<li><p>source：用来存放你的文章</p>
</li>
<li><p>themes：主题，Hexo可以扩展其他主题风格</p>
</li>
<li><p>_config.yml: 博客的全局配置文件</p>
</li>
</ul>
<p>其次，部署 Hexo 文件，并在本地启动服务器。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 部署形成的文件</span></span><br><span class="line">hexo g（hexo generate 简写）</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动服务器</span></span><br><span class="line">hexo s (hexo server 简写）</span><br></pre></td></tr></table></figure>

<p>最后，查看本地部署结果，可以在浏览器访问 <a href="http://localhost:4000/">http://localhost:4000/</a> 查看，效果如下（这里只表明在本地部署成功，无法在外网访问，还需要绑定 Github）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/09/df2e38dca622ea00495961e453bdfaac-1635943805021-e4322cfd-6908-4805-b52c-e8d10a3bca2f-1678db.png" alt="img"></p>
<h2 id="2-6-Github创建个人仓库"><a href="#2-6-Github创建个人仓库" class="headerlink" title="2.6 Github创建个人仓库"></a>2.6 Github创建个人仓库</h2><p>在 GitHub.com 中通过 New repository 创建一个仓库，创建一个和你用户名相同的仓库，后面加 .github.io。只有这样，将 Hexo 部署到GitHub page 时才会被识别，也就是xxxx.github.io，其中xxx就是你注册GitHub的用户名，<strong>其他信息保持默认即可，也可以勾选创建 README 文件。</strong>仓库创建如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/09/9cede1d05042f66fd3056045501f434d-1636388333521-0cc72286-84e4-4617-9d37-f06c5917e18f-d9afa4.png" alt="img"></p>
<h2 id="2-7-生成SSH添加到Github"><a href="#2-7-生成SSH添加到Github" class="headerlink" title="2.7 生成SSH添加到Github"></a>2.7 生成SSH添加到Github</h2><h3 id="2-7-1-配置个人信息"><a href="#2-7-1-配置个人信息" class="headerlink" title="2.7.1 配置个人信息"></a>2.7.1 配置个人信息</h3><p>回到 git bash 中，配置用户名和邮箱，用户名为 Github 用户名，邮箱为 Github 使用的邮箱，主要用于 Github 验证你的账号信息。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;yourname&quot;</span><br><span class="line">git config --global user.email &quot;youremail&quot;</span><br></pre></td></tr></table></figure>

<p>check 用户名和邮箱是否正确。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git config user.name</span><br><span class="line">git config user.email</span><br></pre></td></tr></table></figure>

<h3 id="2-7-2-创建SSH-Token"><a href="#2-7-2-创建SSH-Token" class="headerlink" title="2.7.2 创建SSH Token"></a>2.7.2 创建SSH Token</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;youremail&quot;</span></span><br></pre></td></tr></table></figure>

<p>.ssh 目录是你电脑生成的公钥 id_rsa.pub 和私钥 id_rsa，公钥 id_rsa.pub 是公开的，可以对外给别人看，Github 也正是使用这个公钥来允许来自你电脑的访问。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">benkoonsMacBook-Pro:_posts benkoons$ ls -l ~/.ssh/</span><br><span class="line">total 32</span><br><span class="line">-rw-------  1 sshull  staff  1876 10 30 22:51 id_rsa</span><br><span class="line">-rw-r--r--  1 sshull  staff   401 10 30 22:51 id_rsa.pub</span><br><span class="line">-rw-------  1 sshull  staff   671 10 31 02:00 known_hosts</span><br></pre></td></tr></table></figure>

<h3 id="2-7-3-Github配置公钥"><a href="#2-7-3-Github配置公钥" class="headerlink" title="2.7.3 Github配置公钥"></a>2.7.3 Github配置公钥</h3><p>在GitHub的setting中，找到SSH keys的设置选项，点击 <code>New SSH key</code>把你的 <code>id_rsa_pub</code>里面的信息复制进去。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/09/74f3e883cee1c34c22324cfd0671ffda-1636389388364-21b0b065-c70d-40be-b2b9-3a78d54177ad-99de59.png" alt="img"></p>
<h3 id="2-7-4-测试连接"><a href="#2-7-4-测试连接" class="headerlink" title="2.7.4 测试连接"></a>2.7.4 测试连接</h3><p>使用如下命令测试连接 Github 是否成功，命令不需要修改。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh -T git@github.com   // 不修改</span><br></pre></td></tr></table></figure>

<h2 id="2-8-将Hexo部署到Github"><a href="#2-8-将Hexo部署到Github" class="headerlink" title="2.8 将Hexo部署到Github"></a>2.8 将Hexo部署到Github</h2><p>部署前先安装 deployer-git 依赖，然后才能用命令将 Hexo 部署到 Github。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<p>将hexo和GitHub关联起来，也就是将hexo生成的文章部署到GitHub上，打开站点配置文件 _config.yml，翻到最后，需改 deploy 信息。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/one-command-deployment</span></span><br><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: git@github.com:benkoons/benkoons.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure>

<p>部署之后发布站点。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean	<span class="comment"># 清除之前生成的东西，可不加</span></span><br><span class="line">hexo g  		<span class="comment"># 生成静态文章</span></span><br><span class="line">hexo d			<span class="comment"># 部署网站</span></span><br></pre></td></tr></table></figure>

<p>部署时可能需要输入 ssh 密码（即电脑密码），发布成功后可通过 <a href="https://benkoons.github.io/">https://benkoons.github.io/</a> 查看网站。</p>
<h1 id="3-Hexo常用命令"><a href="#3-Hexo常用命令" class="headerlink" title="3. Hexo常用命令"></a>3. Hexo常用命令</h1><p>常见命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;postName&quot;</span> 	<span class="comment"># 新建文章</span></span><br><span class="line">hexo new page <span class="string">&quot;pageName&quot;</span> 	<span class="comment"># 新建页面</span></span><br><span class="line">hexo generate 		<span class="comment"># 生成静态页面至public目录</span></span><br><span class="line">hexo server 			<span class="comment"># 开启预览访问端口（默认端口4000，&#x27;ctrl + c&#x27;关闭server）</span></span><br><span class="line">hexo deploy 			<span class="comment"># 部署到GitHub</span></span><br><span class="line">hexo <span class="built_in">help</span>  				<span class="comment"># 查看帮助</span></span><br><span class="line">hexo version  		<span class="comment"># 查看Hexo的版本</span></span><br></pre></td></tr></table></figure>

<p>缩写：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo n == hexo new</span><br><span class="line">hexo g == hexo generate</span><br><span class="line">hexo s == hexo server</span><br><span class="line">hexo d == hexo deploy</span><br></pre></td></tr></table></figure>

<p>组合命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo s -g 	<span class="comment"># 生成并本地预览</span></span><br><span class="line">hexo d -g 	<span class="comment"># 生成并上传</span></span><br></pre></td></tr></table></figure>

<h1 id="4-发布第一篇文章"><a href="#4-发布第一篇文章" class="headerlink" title="4. 发布第一篇文章"></a>4. 发布第一篇文章</h1><p>在 blog 目录下通过 Hexo 命令新建一篇文章。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;笨小康的第一篇文章&quot;</span></span><br></pre></td></tr></table></figure>

<p>一般情况下，文章的格式如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 笨小康的第一篇文章</span><br><span class="line">date: 2021-11-09 00:51:07</span><br><span class="line">tags:</span><br><span class="line">- 标签1</span><br><span class="line">- 标签2</span><br><span class="line">categories:</span><br><span class="line">- 分类1</span><br><span class="line">- 分类2</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">以下是正文内容～～</span><br></pre></td></tr></table></figure>

<p>发布查看</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo s		<span class="comment"># 发布本地预览</span></span><br><span class="line">hexo d -g	<span class="comment"># 发布Githuh查看</span></span><br></pre></td></tr></table></figure>

<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://blog.csdn.net/sinat_37781304/article/details/82729029/">hexo史上最全搭建教程</a></li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown语法手册</title>
    <url>/2021/11/08/Markdown%E8%AF%AD%E6%B3%95%E6%89%8B%E5%86%8C/</url>
    <content><![CDATA[<p>介绍 Markdown 写文章的一些常见用法。</p>
<h1 id="1-标题"><a href="#1-标题" class="headerlink" title="1. 标题"></a>1. 标题</h1><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="params">#</span> 一级标题</span><br><span class="line"><span class="params">##</span> 二级标题</span><br><span class="line"><span class="params">###</span> 三级标题</span><br><span class="line"><span class="params">####</span> 四级标题</span><br><span class="line"><span class="params">#####</span> 五级标题</span><br><span class="line"><span class="params">######</span> 六级标题</span><br></pre></td></tr></table></figure>

<h1 id="2-段落格式"><a href="#2-段落格式" class="headerlink" title="2. 段落格式"></a>2. 段落格式</h1><h2 id="2-1-字体"><a href="#2-1-字体" class="headerlink" title="2.1 字体"></a>2.1 字体</h2><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">*斜体文本*  </span><br><span class="line">**粗体文本**  </span><br><span class="line">***斜粗体文本***</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<p><em>斜体文本</em><br><strong>粗体文本</strong><br><em><strong>斜粗体文本</strong></em></p>
<h2 id="2-2-分割线"><a href="#2-2-分割线" class="headerlink" title="2.2 分割线"></a>2.2 分割线</h2><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">***</span><br><span class="line">或者</span><br><span class="line">****</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<hr>
<hr>
<h2 id="2-3-删除线"><a href="#2-3-删除线" class="headerlink" title="2.3 删除线"></a>2.3 删除线</h2><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">~~google.com~~</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<p><del>google.com</del></p>
<h2 id="2-4-下划线"><a href="#2-4-下划线" class="headerlink" title="2.4 下划线"></a>2.4 下划线</h2><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">&lt;u&gt;我是下划线&lt;/u&gt;</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<p><u>我是下划线</u></p>
<h2 id="2-5-文本居中"><a href="#2-5-文本居中" class="headerlink" title="2.5 文本居中"></a>2.5 文本居中</h2><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">&lt;center&gt;我是居中文本&lt;/center&gt;</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<center>我是居中文本</center>

<h1 id="3-列表"><a href="#3-列表" class="headerlink" title="3. 列表"></a>3. 列表</h1><h2 id="3-1-无序列表"><a href="#3-1-无序列表" class="headerlink" title="3.1 无序列表"></a>3.1 无序列表</h2><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">- 一级列表</span><br><span class="line">  - 二级列表</span><br><span class="line">  - 二级列表</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<ul>
<li>一级列表<ul>
<li>二级列表</li>
<li>二级列表</li>
</ul>
</li>
</ul>
<h2 id="3-2-有序列表"><a href="#3-2-有序列表" class="headerlink" title="3.2 有序列表"></a>3.2 有序列表</h2><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">1. 一级列表</span><br><span class="line">   1. 二级列表</span><br><span class="line">   2. 二级列表</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<ol>
<li>一级列表<ol>
<li>二级列表</li>
<li>二级列表</li>
</ol>
</li>
</ol>
<h2 id="3-3-todo列表"><a href="#3-3-todo列表" class="headerlink" title="3.3 todo列表"></a>3.3 todo列表</h2><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">- [ ] 事项1 </span><br><span class="line">  - [x] 子事项1（已完成）</span><br><span class="line">  - [ ] 子事项2（未完成）</span><br><span class="line">- [ ] 事项2</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<ul>
<li><p><input disabled="" type="checkbox">  事项1</p>
<ul>
<li><input disabled="" type="checkbox"> 子事项1（已完成）</li>
<li><input disabled="" type="checkbox"> 子事项2（未完成）</li>
</ul>
</li>
<li><p><input disabled="" type="checkbox">  事项2</p>
</li>
</ul>
<h1 id="4-区块"><a href="#4-区块" class="headerlink" title="4. 区块"></a>4. 区块</h1><h2 id="4-1-嵌套区块"><a href="#4-1-嵌套区块" class="headerlink" title="4.1 嵌套区块"></a>4.1 嵌套区块</h2><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">&gt; 区块1</span><br><span class="line">&gt;&gt; 区块11  </span><br><span class="line">&gt;&gt; 区块12 </span><br></pre></td></tr></table></figure>

<p>效果：</p>
<blockquote>
<p>区块1</p>
<blockquote>
<p>区块11<br>区块12 </p>
</blockquote>
</blockquote>
<h2 id="4-2-多行序列区块"><a href="#4-2-多行序列区块" class="headerlink" title="4.2 多行序列区块"></a>4.2 多行序列区块</h2><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">&gt; 区块2</span><br><span class="line">&gt; - 区块21</span><br><span class="line">&gt; 1. 区块22</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<blockquote>
<p>区块2</p>
<ul>
<li>区块21</li>
</ul>
<ol>
<li>区块22</li>
</ol>
</blockquote>
<h1 id="4-3-行内引用"><a href="#4-3-行内引用" class="headerlink" title="4.3 行内引用"></a>4.3 行内引用</h1><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">你说`我就是个傻子`，对不对</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<p>你说<code>我就是个傻子</code>，对不对</p>
<h1 id="5-代码"><a href="#5-代码" class="headerlink" title="5. 代码"></a>5. 代码</h1><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">```java</span><br><span class="line">System.out.print(&quot;hello world&quot;)</span><br><span class="line">```</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.out.print(<span class="string">&quot;hello world&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="6-链接"><a href="#6-链接" class="headerlink" title="6. 链接"></a>6. 链接</h1><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">方式一：[链接名称](链接地址)</span><br><span class="line">1. 这是一个链接 [谷歌搜索](https://www.google.com)  </span><br><span class="line"></span><br><span class="line">方式二：&lt;链接地址&gt;</span><br><span class="line">2. &lt;https://www.google.com&gt;</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<ol>
<li>这是一个链接 <a href="https://www.google.com/">谷歌搜索</a>  </li>
<li><a href="https://www.google.com/">https://www.google.com</a></li>
</ol>
<h1 id="7-图片"><a href="#7-图片" class="headerlink" title="7. 图片"></a>7. 图片</h1><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">方式：![属性文本](图片地址 &quot;可选标题&quot;)</span><br><span class="line"></span><br><span class="line">![面具防护](https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/08/2fb74afdc4483015dddd039aa2189fe5-mask-5932015<span class="built_in">_</span><span class="built_in">_</span>340-39f087.png &quot;good idea&quot;)</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/08/2fb74afdc4483015dddd039aa2189fe5-mask-5932015__340-39f087.png" alt="面具防护" title="good idea"></p>
<h1 id="8-表格"><a href="#8-表格" class="headerlink" title="8. 表格"></a>8. 表格</h1><p>语法：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">| 左对齐 | 右对齐 | 居中对齐 |</span><br><span class="line">| :-----| ----: | :----: |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<table>
<thead>
<tr>
<th align="left">左对齐</th>
<th align="right">右对齐</th>
<th align="center">居中对齐</th>
</tr>
</thead>
<tbody><tr>
<td align="left">单元格</td>
<td align="right">单元格</td>
<td align="center">单元格</td>
</tr>
<tr>
<td align="left">单元格</td>
<td align="right">单元格</td>
<td align="center">单元格</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title>PicGo+Typora配置Github图床手册</title>
    <url>/2021/11/17/PicGo-Typora%E9%85%8D%E7%BD%AEGithub%E5%9B%BE%E5%BA%8A%E6%89%8B%E5%86%8C/</url>
    <content><![CDATA[<p>最近用 Hexo + Github 搭建的个人博客写文章，发现文章里的图片在浏览器中没法加载，也不会自动上传到 Github 存稿，在网上找了一些资料，发现可以用图床来满足这个需求。图床，顾名思义，就是存储图片的服务器，能够把自己的所有照片保存在个人图床里。</p>
<p>实现图床的方式网上资料有很多，本文主要选用 Github + PicGo + Typora 的方式实现个人图床，Github 用于图片的存储，PicGo管理图床的工具，Typora是文本编辑器，直接配置图片上传。</p>
<p>先来直接看看效果。</p>
<p>这张图片是 PicGo 的 UI 界面图片，我们将照片拷贝到 Typora 编辑器中，然后右键“上传图片”就将图片上传到 Github 图床了，发布的博客也可以正常访问，是不是很简洁很方便呀。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/17/87408aac9a9abc2ff76a132c622f6fb4-image-20211117001921095-621a7f.png" alt="image-20211117001921095"></p>
<h1 id="1-Github图床准备"><a href="#1-Github图床准备" class="headerlink" title="1. Github图床准备"></a>1. Github图床准备</h1><h2 id="1-1-新建Github仓库"><a href="#1-1-新建Github仓库" class="headerlink" title="1.1 新建Github仓库"></a>1.1 新建Github仓库</h2><p>Github图床当然也需要一个仓库，仓库名随便取，这里我的图床仓库命名为”blog-imgs”，由于之前创建好了，这里提示已存在，其他保持默认配置即可。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/17/8efc22bd846677cb3e9df7ec5cbf33dd-8efc22bd846677cb3e9df7ec5cbf33dd-image-20211117002608528-b6b53b-8ab79f.png" alt="image-20211117002608528"></p>
<h2 id="1-2-创建个人Token"><a href="#1-2-创建个人Token" class="headerlink" title="1.2 创建个人Token"></a>1.2 创建个人Token</h2><p>在 Github 网站的 “Settings -&gt; Developer settings -&gt; Personal access tokens“ 标签中，点击“Generate new token” 按钮新建个人 token 信息。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/17/595e06f77bf855b5f9f373f414f1eb1d-image-20211117003451831-d40ef3.png" alt="image-20211117003451831"></p>
<p>最后点击 “generate token” 即可完成 token 的创建。</p>
<p>注意，生成的 token 在页面之后显示一次，一定要把 token 信息复制保存。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/17/2d692e03546a1eaa1832ab93b0108099-image-20211117003550115-b71a12.png" alt="image-20211117003550115"></p>
<p>至此，Github 图床已经创建完成，对外访问图床的个人 token 也创建好了。</p>
<h1 id="2-PicGo配置Github图床"><a href="#2-PicGo配置Github图床" class="headerlink" title="2. PicGo配置Github图床"></a>2. PicGo配置Github图床</h1><h2 id="2-1-下载PicGo"><a href="#2-1-下载PicGo" class="headerlink" title="2.1 下载PicGo"></a>2.1 下载PicGo</h2><p>官网下载地址：（根据电脑版本下载对应文件即可）</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">https://github.com/Molunerfinn/PicGo/releases</span><br></pre></td></tr></table></figure>

<p>PicGo 使用手册：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">https://picgo.github.io/PicGo-Doc/zh/guide/</span><br></pre></td></tr></table></figure>

<h2 id="2-2-配置Github图床"><a href="#2-2-配置Github图床" class="headerlink" title="2.2 配置Github图床"></a>2.2 配置Github图床</h2><p>我是用的 mac 电脑，第一次使用都不知道怎么打开 PicGo UI 界面，如下图，可以通过 mac 状态栏的“打开详细窗口”进到 UI 界面。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/17/75ead9fc3b8c5119b805a2ab49713fe4-image-20211117004309731-551504.png" alt="image-20211117004309731"></p>
<p>接下来就是如何通过 PicGo 配置 Github 图床，UI 中可以看到 PicGo 支持多种图床配置，这里我们选择配置 Github 图床。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="params">#</span> Github 图床配置</span><br><span class="line">设定仓库名（必须）：根据 1.1 小节创建的 Gihub 仓库地址自定义配置。</span><br><span class="line">设定分支名（必须）：main。Github 仓库默认的主分支就是 main，保持不变即可。</span><br><span class="line">设定Token（必须）：根据 1.2 小节创建的个人 Token 信息自定义修改。</span><br><span class="line">指定存储路径（非必须）：图片在仓库中的存储目录，默认是根目录存储所有图片。</span><br><span class="line">设定自定义域名（非必须）：免费的 cdn 访问加速工具，可配置上。格式为:https://cdn.jsdelivr.net/gh/&lt;用户名&gt;/&lt;仓库名&gt;，其实就是第一项信息。</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/17/7c51cb4df5ed344c6db7ac14756f9cf9-image-20211117004525265-f66ab5.png" alt="image-20211117004525265"></p>
<h2 id="2-3-PicGo上传图片"><a href="#2-3-PicGo上传图片" class="headerlink" title="2.3 PicGo上传图片"></a>2.3 PicGo上传图片</h2><p>配置好 Github 图床后，我们可以通过 PicGo UI 界面上传图片到 Github 图床。PicGo 也提供了多种上传方式，我个人选择了两张方式做验证，上传成功后就可以取 Github 图床（其实就是往仓库里提交图片信息）中查看照片是否成功上传。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/17/7da063c4cea021de4986d618f5484e27-image-20211117005551971-f2db47.png" alt="image-20211117005551971"></p>
<h1 id="3-Typora配置PicGo上传"><a href="#3-Typora配置PicGo上传" class="headerlink" title="3. Typora配置PicGo上传"></a>3. Typora配置PicGo上传</h1><p>用 PicGo 上传图片后，我又遇到一个问题，我在本地编辑器中插入一张图片，我需要先将图片通过 PicGo 上传到 Github 图床，然后在本地编辑器中通过地址插入图片，还是特别不方便。</p>
<h2 id="3-1-配置-PicGo"><a href="#3-1-配置-PicGo" class="headerlink" title="3.1 配置 PicGo"></a>3.1 配置 PicGo</h2><p>好在 Typora 提供了一个图片上传的服务配置，打开 Typora 工具 ”偏好设置 -&gt; 图像“，插入图片选项选择“无特殊操作”，然后在 “上传服务设定” 选项中，我们选择自定义上传命令。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">自定义上传命令：node picgo upload</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">node 命令是 nodeJs 环境，mac 下通过 brew install node 安装。</span><br><span class="line">picgo 命令是 PicGo 环境，mac 下通过 brew install npm; npm install picgo -g 安装。</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/17/c57ffd946f0653f8229eb9d16eafe67d-image-20211117010049415-2f137e.png" alt="image-20211117010049415"></p>
<p>配置好命令后，点击”验证图片上传选项“，若上传成功则出现如下结果。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/17/723961c0b72aef8de035e1da4e7b6014-image-20211117010148554-f22b03.png" alt="image-20211117010148554"></p>
<h2 id="3-2-Typora上传图片"><a href="#3-2-Typora上传图片" class="headerlink" title="3.2 Typora上传图片"></a>3.2 Typora上传图片</h2><p>在文章开头也提过到效果，特别简单，两步操作即可完成：</p>
<ol>
<li>复制图片到 Typora 编辑器；</li>
<li>选中图片右键“上传图片”。</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/17/87408aac9a9abc2ff76a132c622f6fb4-image-20211117001921095-621a7f.png" alt="image-20211117001921095"></p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark RDD编程实例</title>
    <url>/2021/11/22/Spark-RDD%E7%BC%96%E7%A8%8B%E5%AE%9E%E4%BE%8B/</url>
    <content><![CDATA[<p>在上一篇文章 <a href="%5Bhttp://localhost:4000/2021/11/21/Spark-RDD%E7%AE%97%E5%AD%90%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/%5D(http://localhost:4000/2021/11/21/Spark-RDD%E7%AE%97%E5%AD%90%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/)">Spark算子使用手册</a> 中详细介绍了 Spark RDD 常见算子的使用，本文主要是介绍在实际案例如何灵活运用这些算子解决一些问题。</p>
<h1 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h1><p>统计文本中每个单词的出现次数。</p>
<p>输入输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line">hello spark</span><br><span class="line">hello hadoop</span><br><span class="line">hello flink</span><br><span class="line">spark flink</span><br><span class="line">scala spark</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(spark,3)</span><br><span class="line">(hello,3)</span><br><span class="line">(flink,2)</span><br><span class="line">(scala,1)</span><br><span class="line">(hadoop,1)</span><br></pre></td></tr></table></figure>

<p>程序代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wordCount</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">&quot;Hello Scala Spark Flink&quot;</span>, <span class="string">&quot;Hello Scala Hive&quot;</span>, <span class="string">&quot;Hello Scala&quot;</span>, <span class="string">&quot;Hello Hive Spark&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> rdd = sc.parallelize(list)</span><br><span class="line">  rdd.flatMap(x =&gt; x.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    .map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">    .reduceByKey((x,y) =&gt; x + y)</span><br><span class="line">    .sortBy(x =&gt; x._2, <span class="literal">false</span>)</span><br><span class="line">    .take(<span class="number">5</span>)</span><br><span class="line">    .foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="求TopN值"><a href="#求TopN值" class="headerlink" title="求TopN值"></a>求TopN值</h1><p>文本包含很多行数据，每行数据由4个字段的值构成，不同值之间用逗号隔开，4个字段分别为orderid、userid、payment和productid，要求求出 TopN 个 payment 值。</p>
<p>输入输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line"><span class="meta">#</span><span class="bash"> file1.txt</span></span><br><span class="line">1,1768,50,155</span><br><span class="line">2,1218,600,211</span><br><span class="line">3,2239,788,242</span><br><span class="line">4,3101,28,599</span><br><span class="line">5,4899,290,129</span><br><span class="line">6,3110,54,1201</span><br><span class="line">7,4436,259,877</span><br><span class="line">8,2369,7890,27</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> file2.txt</span></span><br><span class="line">100,4287,226,233</span><br><span class="line">101,6562,489,124</span><br><span class="line">102,1124,33,17</span><br><span class="line">101,3267,159,179</span><br><span class="line">103,4569,57,125</span><br><span class="line">105,1438,37,116</span><br><span class="line"></span><br><span class="line">输出：Top-5 payment值</span><br><span class="line">1,7890</span><br><span class="line">2,788</span><br><span class="line">3,600</span><br><span class="line">4,489</span><br><span class="line">5,290</span><br></pre></td></tr></table></figure>

<p>程序代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topN</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;./data/topn/&quot;</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">var</span> num = <span class="number">0</span></span><br><span class="line">  rdd.filter(line =&gt;</span><br><span class="line">          (line.split(<span class="string">&quot;,&quot;</span>).length &gt; <span class="number">0</span> &amp;&amp; line.trim.split(<span class="string">&quot;,&quot;</span>).size == <span class="number">4</span>))</span><br><span class="line">      .map(line =&gt; line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">2</span>))</span><br><span class="line">      .map(x =&gt; x.toInt)</span><br><span class="line">      .sortBy(x =&gt; x, <span class="literal">false</span>)</span><br><span class="line">      .take(<span class="number">5</span>)</span><br><span class="line">      .foreach(x =&gt; &#123;</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        println(num + <span class="string">&quot;,&quot;</span> + x)</span><br><span class="line">      &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="求最大-小值"><a href="#求最大-小值" class="headerlink" title="求最大/小值"></a>求最大/小值</h1><p>求多个文件中的数字的最大值和最小值。</p>
<p>输入输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line"><span class="meta">#</span><span class="bash"> file1.txt</span></span><br><span class="line">129</span><br><span class="line">54</span><br><span class="line">167</span><br><span class="line">324</span><br><span class="line">111</span><br><span class="line">54</span><br><span class="line">26</span><br><span class="line">697</span><br><span class="line">4856</span><br><span class="line">3418</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> file2.txt</span></span><br><span class="line">5</span><br><span class="line">329</span><br><span class="line">14</span><br><span class="line">4567</span><br><span class="line">2186</span><br><span class="line">457</span><br><span class="line">35</span><br><span class="line">267</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">min = 5</span><br><span class="line">max = 4856</span><br></pre></td></tr></table></figure>

<p>程序代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxAndMin</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;./data/minandmax/&quot;</span>,<span class="number">2</span>)</span><br><span class="line">  <span class="keyword">var</span> min = <span class="type">Integer</span>.<span class="type">MAX_VALUE</span></span><br><span class="line">  <span class="keyword">var</span> max = <span class="type">Integer</span>.<span class="type">MIN_VALUE</span></span><br><span class="line">  rdd.map(num =&gt; (<span class="number">1</span>, num.toInt))</span><br><span class="line">    .groupByKey()</span><br><span class="line">    .mapValues(nums =&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (num &lt;- nums) &#123;</span><br><span class="line">        <span class="keyword">if</span> (num &lt; min) min = num</span><br><span class="line">        <span class="keyword">if</span> (num &gt; max) max = num</span><br><span class="line">      &#125;</span><br><span class="line">      (min,max)</span><br><span class="line">    &#125;)</span><br><span class="line">    .map(x =&gt; x._2)</span><br><span class="line">    .foreach(x =&gt; &#123;</span><br><span class="line">      println(<span class="string">&quot;min = &quot;</span> + x._1)</span><br><span class="line">      println(<span class="string">&quot;max = &quot;</span> + x._2)</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="求平均值"><a href="#求平均值" class="headerlink" title="求平均值"></a>求平均值</h1><p>给定一组文件，每个文件表示一天每类书籍的销量，其中key表示图书名称，value表示某天图书销量，计算每种图书的每天的平均销量。</p>
<p>输入输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line"><span class="meta">#</span><span class="bash"> file1.txt</span></span><br><span class="line">spark,10</span><br><span class="line">hadoop,20</span><br><span class="line">hive,12</span><br><span class="line">flink,10</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> file2.txt</span></span><br><span class="line">spark,30</span><br><span class="line">hadoop,18</span><br><span class="line">hive,12</span><br><span class="line">flink,10</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> file3.txt</span></span><br><span class="line">spark,20</span><br><span class="line">hadoop,7</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">spark书籍3天平均销量：20</span><br><span class="line">hadoop书籍3天平均销量：15</span><br><span class="line">flink书籍2天平均销量：10</span><br><span class="line">hive书籍2天平均销量：12</span><br></pre></td></tr></table></figure>

<p>程序代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">avgValue</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;./data/avgvalue/&quot;</span>,<span class="number">3</span>)</span><br><span class="line">  rdd.map(line =&gt; line.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">    .map(x =&gt; (x(<span class="number">0</span>), x(<span class="number">1</span>).toInt))</span><br><span class="line">    .groupByKey()</span><br><span class="line">    .sortBy(x =&gt; x._2.size, <span class="literal">false</span>)</span><br><span class="line">    .foreach(x =&gt; &#123;</span><br><span class="line">      println(x._1 + <span class="string">&quot;书籍&quot;</span> + x._2.size + <span class="string">&quot;天平均销量：&quot;</span> + (x._2.sum / x._2.size))</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="二次排序"><a href="#二次排序" class="headerlink" title="二次排序"></a>二次排序</h1><p>对多个文件中的多个值进行排序，比如按第一列升序，第二列降序，或者两列都升序/降序。</p>
<p>输入输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line"><span class="meta">#</span><span class="bash"> file1.txt</span></span><br><span class="line">5 3</span><br><span class="line">1 6</span><br><span class="line">4 9</span><br><span class="line">8 3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> file2.txt</span></span><br><span class="line">4 7</span><br><span class="line">5 6</span><br><span class="line">3 2</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">----------sortByKey自定义排序器------------</span><br><span class="line">1 6</span><br><span class="line">3 2</span><br><span class="line">4 9</span><br><span class="line">4 7</span><br><span class="line">5 6</span><br><span class="line">5 3</span><br><span class="line">8 3</span><br><span class="line">----------sortBy自带排序------------</span><br><span class="line">1 6</span><br><span class="line">3 2</span><br><span class="line">4 9</span><br><span class="line">4 7</span><br><span class="line">5 6</span><br><span class="line">5 3</span><br><span class="line">8 3</span><br></pre></td></tr></table></figure>

<p>程序代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">secondSort</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;./data/secondsort/&quot;</span>,<span class="number">2</span>)</span><br><span class="line">  println(<span class="string">&quot;----------sortByKey自定义排序器------------&quot;</span>)</span><br><span class="line">  rdd.map(line =&gt; &#123;</span><br><span class="line">       (<span class="keyword">new</span> <span class="type">SecondarySortKey</span>(line.split(<span class="string">&quot; &quot;</span>)(<span class="number">0</span>).toInt, line.split(<span class="string">&quot; &quot;</span>)(<span class="number">1</span>).toInt), line)</span><br><span class="line">    &#125;)</span><br><span class="line">    .sortByKey()</span><br><span class="line">    .map(x =&gt; x._2)</span><br><span class="line">    .foreach(println)</span><br><span class="line"></span><br><span class="line">  println(<span class="string">&quot;----------sortBy自带排序------------&quot;</span>)</span><br><span class="line">  rdd.map(line =&gt; (line.trim.split(<span class="string">&quot; &quot;</span>)(<span class="number">0</span>).toInt,line.trim.split(<span class="string">&quot; &quot;</span>)(<span class="number">1</span>).toInt,line.trim))</span><br><span class="line">    <span class="comment">// 根据第一个元素升序、第二个元素降序的规则进行排序</span></span><br><span class="line">    .sortBy(x =&gt; (x._1, -x._2), <span class="literal">true</span>)</span><br><span class="line">    .map(x =&gt; x._3)</span><br><span class="line">    .foreach(println)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 继承 Ordered 类实现自定义排序规则</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 或者通过实现 Ordering 接口实现排序</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param first</span></span><br><span class="line"><span class="comment">  * @param second</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SecondarySortKey</span>(<span class="params">val first:<span class="type">Int</span>,val second:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">SecondarySortKey</span>] <span class="keyword">with</span> <span class="title">Serializable</span></span>&#123;</span><br><span class="line">  <span class="comment">// 根据第一个元素升序、第二个元素降序的规则进行排序</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(other: <span class="type">SecondarySortKey</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.first - other.first != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">this</span>.first - other.first</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      -(<span class="keyword">this</span>.second - other.second)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="数据去重"><a href="#数据去重" class="headerlink" title="数据去重"></a>数据去重</h1><p>给定多个文件的行文本数据，对文本中存在行数据一致的行进行过滤去重，保证每一行相同的数据只输出一次。</p>
<p>输入输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line"><span class="meta">#</span><span class="bash"> file1.txt</span></span><br><span class="line">20170101 x</span><br><span class="line">20170102 y</span><br><span class="line">20170103 x</span><br><span class="line">20170104 y</span><br><span class="line">20170105 z</span><br><span class="line">20170106 z</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> file2.txt</span></span><br><span class="line">20170101 y</span><br><span class="line">20170102 y</span><br><span class="line">20170103 x</span><br><span class="line">20170104 z</span><br><span class="line">20170105 y</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">20170101 x</span><br><span class="line">20170101 y</span><br><span class="line">20170102 y</span><br><span class="line">20170103 x</span><br><span class="line">20170104 y</span><br><span class="line">20170104 z</span><br><span class="line">20170105 y</span><br><span class="line">20170105 z</span><br><span class="line">20170106 z</span><br></pre></td></tr></table></figure>

<p>程序代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataDeDuplicate</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;./data/datadeduplicate/&quot;</span>,<span class="number">2</span>)</span><br><span class="line">  rdd.filter(x =&gt; x.length &gt; <span class="number">0</span>)</span><br><span class="line">    .map(x =&gt; (x.trim, <span class="number">1</span>))</span><br><span class="line">    .groupByKey()</span><br><span class="line">    .sortByKey()</span><br><span class="line">    .keys</span><br><span class="line">    .foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://study.163.com/course/courseMain.htm?courseId=1005031005">林子雨, 《Spark编程基础（Scala版本)</a></li>
<li><a href="https://helloliwen.github.io/b4d2d389.html">Spark常见操作及实例代码</a></li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>YARN NodeManager总体架构</title>
    <url>/2021/11/06/YARN-NodeManager%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<p>NodeManager（NM）是 YARN 中单个节点上的代理，它管理 Hadoop 集群中单个计算节点，功能包括与 ResourceManager 保持通信、管理 Container 的生命周期、监控每个 Container 的资源使用情况、追踪节点健康状况、管理日志和不同应用程序用到的附属服务（auxiliary service）</p>
<h2 id="1-NodeManager-基本职能"><a href="#1-NodeManager-基本职能" class="headerlink" title="1. NodeManager 基本职能"></a>1. NodeManager 基本职能</h2><p>整体上讲，NodeManager 需要通过两个 RPC 协议与 ResourceManager 服务和各个应用程序的 ApplicationMaster 交互，如下图所示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/07/9aa367bbb1f16e341ca9aa315f6961d2-1372882-20200902212501158-334281140-0f82f8.png" alt="img"></p>
<center>NodeManager 相关 RPC 协议</center>

<p><strong>ResourceTrackerProtocol 协议</strong>：NodeManager 通过该 RPC 协议向 ResourceManager 注册、汇报节点健康状况和 Container 运行状态，并领取 ResourceManager 下达的命令，包括重新初始化、清理 Container 占用资源等。在该协议中，ResourceManager 扮演 RPC server 的角色，而 NodeManager 扮演 RPC Client 的角色（由内部组件 NodeStatusUpdater 实现），换句话说，NodeManager 与 ResourceManager 之间采用 “pull 模型”，NodeManager 总是周期性地主动向 ResourceManager 发起请求，并领取下达给自己的命令。</p>
<p><strong>ContainerManagementProtocol 协议：</strong>应用程序的 ApplicationMaster 通过该 RPC 协议向 NodeManager 发起针对 Container 的相关操作，包括启动 Container、杀死 Container、获取 Container 执行状态等。在该协议中，ApplicationMaster 扮演 RPC Client 的角色，而 NodeManager 扮演 RPC Server 的角色（由内部组件 ContainerManager 实现），换句话说，NodeManager 与 ApplicationMaster 之间采用了 “push 模型”，ApplicationMaster 可以将 Container 相关操作的第一时间告诉 NodeManager，相比于 “pull 模型”，可以大大降低时间延迟。</p>
<h2 id="2-NodeManager-内部结构"><a href="#2-NodeManager-内部结构" class="headerlink" title="2. NodeManager 内部结构"></a>2. NodeManager 内部结构</h2><p>这部分主要深入介绍 NodeManager 内部组织结构和主要模块，如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/07/3cb960ba925a2316cbadd27f46fa7edf-1372882-20200902212423701-381786426-f639fd.png" alt="img"></p>
<center>NodeManager 内部结构</center>

<p><strong>NodeStatusUpdate：</strong>NodeStatusUpdater 是 NodeManager 与 ResourceManager 通信的唯一通道。当 NodeManager 启动是，该组件负责向 ResourceManager 注册，并汇报节点上总的可用资源。之后，该组件周期性与 ResourceManager 通信，汇报各个 Container 的状态更新，包括节点上正在运行的 Container、已经完成的 Container 等信息，同时 ResourceManager 会为之返回待清理的 Container 列表、待清理的应用程序列表、诊断信息、各种 Token 等信息。</p>
<p><strong>ContainerManager：</strong>ContainerManager 是 NodeManager 中最核心的组件之一，它由多个子组件组成，每个子组件负责一部分功能，协同管理运行在该节点上的所有 Container，各个子组件如下：</p>
<ul>
<li><strong>RPC Server：</strong>该 RPC Server 实现了 ContainerManagementProtocol 协议，是 ApplicationMaster 与 NodeManager 通信的唯一通道。ContainerManager 从各个 ApplicationMaster 上接收 RPC 请求以启动新的 Container 或者挺直正在运行的 Container。需要注意的是，任何 Container 操作均会经 ContainerTokenSecretManager 合法性验证，以防止 ApplicationMaster 伪造启动或停止 Container 的命令。</li>
<li><strong>ResourceLocalizationService：</strong>负责 Container 所需资源的本地化，它能够按照描述从 HDFS 上下载 Container 所需的文件资源，并尽量将它们分摊到各个磁盘上以防止出现热点访问。此外，它会为下载的文件添加访问控制限制，并为之施加合适的磁盘空间使用份额。</li>
<li><strong>ContianersLauncher：</strong>维护了一个线程池以并行完成 Container 相关操作，比如启动或者杀死 Container，其中启动 Container 请求是由 ApplicationMaster 发起的，而杀死 Container 请求则可能来自 ApplicationMaster 或者 ResourceManager。</li>
<li><strong>AuxService：</strong>NodeManager 允许用户通过配置附属服务的方式扩展自己的功能，这使得每个节点可以定制一些特定框架的服务。附属服务需要在 NodeManager 启动之前配置好，并由 NodeManager 统一启动与关闭。</li>
<li><strong>ContainersMonitor：</strong>ContainersMonitor 负责监控 Container 的资源使用量，为了实现资源隔离和公平共享，ResourceManager 为每个 Container 分配了一定量的资源。而 ContainersMonitor 周期性探测它在运行过程中的资源利用量，一旦发生 Container 超出了它的允许使用份额上线，就向 Container 发送信号将其杀掉，这可以避免资源密集型的 Container 影响同节点上其他正在运行的 Container。</li>
<li><strong>LogHandler：</strong>一个可插拔组件，用户可通过它控制 Container 日志的保存方式，即是写到本地磁盘上还是将其打包后上传到一个文件系统中。</li>
<li><strong>ContainerEventDispatcher：</strong>Container 事件调度器，负责将 ContainerEvent 类型的事件调度给对应 Container 的状态机 ContainerImpl。</li>
<li><strong>ApplicationEventDispatcher：</strong>Application 事件调度器，负责将 ApplicationEvent 类型的事件调度给对应 Application 的状态机 ApplicationImpl。</li>
</ul>
<p><strong>ContainerExecutor：</strong>ContainerExecutor 可与底层操作系统交互，安全存放 Container 需要的文件和目录，进而以一种安全的方式启动和清除 Container 对应的进程。目前 YARN 提供了 DefaultContainerExecutor、LinuxContainerExecutor 和 DockerContainerExecutor 三种实现。</p>
<p><strong>NodeHealthCheckerService：</strong>NodeHealthCheckerService 通过周期性地运行一个自定义脚本（由组件 NodeHealthScriptRunner 完成）和向磁盘写文件（由服务 LocalDirsHandlerService 完成）检查节点的健康状况，并将之通过 NodeStatusUpdater 传递给 ResourceManager。一旦 ResourceManager 发现一个节点处于不健康状态，则会将它加入黑名单，此后不再为它分配资源，直到再次转为健康状态。需要注意的是，节点被加入黑名单时，正在运行的 Container 仍会正常运行，不会被杀死。</p>
<p><strong>DeletionService：</strong>NodeManager 将文件删除功能服务化，即提供一个专门的文件删除服务，异步删除失效文件，这样可避免删除文件带来的性能开销。</p>
<p><strong>Security：</strong>安全模块是 NodeManager 中的一个重要模块，它包含两部分，分别是 ApplicationACLsManager 和 ContainerTokenSecretManager，ApplicationACLsManager 确保访问 NodeManager 的用户是合法的，ContainerTokenSecretManager 确保用户请求的资源被 ResourceManager 授权过。具体如下：</p>
<ul>
<li><strong>ApplicationACLsManager：</strong>NodeManager 需要为所有面向用户的 API 提供安全检查，如在 Web UI 上只能将 Container 日志显示给授权用户。该组件为每个应用程序维护了一个 ACL 列表，一旦收到类似请求后会利用该列表对其进行验证。</li>
<li><strong>ContainerTokenSecretManager：</strong>检查收到的各种访问请求的合法性，确保这些请求操作已被 ResourceManager 授权。</li>
</ul>
<p><strong>WebServer：</strong>通过 Web 界面向用户展示该节点上所有应用程序运行状态、Container 列表、节点健康状况和 Container 产生的日志等信息。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>董西成.《Hadoop技术内幕：深入解析YARN架构设计与实现原理》</li>
</ul>
]]></content>
      <categories>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title>YARN ResourceManager总体架构</title>
    <url>/2021/11/06/YARN-ResourceManager%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<p>ResourceManager(简称 RM) 是 YARN 组件的核心服务，主要负责 Hadoop 集群中资源的集中管理和统一调度。</p>
<h2 id="1-ResourceManager基本职能"><a href="#1-ResourceManager基本职能" class="headerlink" title="1. ResourceManager基本职能"></a>1. ResourceManager基本职能</h2><p>在YARN中，ResourceManager负责集群中所有资源的统一管理和分配，它接收来自各个节点（NodeManager）的资源汇报信息，并把这些信息按照一定的策略分配给各个应用程序（实际上是ApplicationMaster）。整体上讲，ResourceManager需通过两个RPC协议与NodeManager和（各个应用程序的）ApplicationMaster交互，如图所示，具体如下。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/08/06c906d90643b0318e2737d79bcc34b2-1636344037002-d29d9c88-e285-4acd-8db0-63d036b7f8ef-40200f.png" alt="RM rpc调用.png"></p>
<center>ResourceManager相关RPC协议</center>

<p><strong>ResourceTrackerProtocol：</strong>NodeManager通过该RPC协议向ResourceManager注册、汇报节点健康状况和Container运行状态，并领取ResourceManager下达的命令，这些命令包括重新初始化、清理Container等，在该RPC协议中，ResourceManager扮演RPC Server的角色（由内部组件ResourceTrackerService实现），而NodeManager扮演RPC Client的角色，换句话说，NodeManager与ResourceManager之间采用了“pull模型”（与MRv1类似），NodeManager 总是周期性地主动向ResourceManager发起请求，并通过领取下达给自己的命令。 </p>
<p><strong>ApplicationMasterProtocol：</strong>应用程序的ApplicationMaster通过该RPC协议向ResourceManager注册、申请资源和释放资源。在该协议中，ApplicationMaster扮演RPCClient的角色，而ResourceManager扮演RPC Server的角色，换句话说，ResourceManager与ApplicationMaster之间也采用了“pull模型”。</p>
<p><strong>ApplicationClientProtocol：</strong>应用程序的客户端通过该RPC协议向ResourceManager提交应用程序、查询应用程序状态和控制应用程序（比如杀死应用程序）等。在该协议中，应用工程序客户端扮演RPC Client的角色，而ResourceManager扮演RPC Server的角色。</p>
<p>概括起来，ResourceManager主要完成以下几个功能：</p>
<ul>
<li>与客户端交互，处理来自客户端的请求；</li>
<li>启动和管理ApplicationMaster，并在它运行失败时重新启动它；</li>
<li>管理NodeManager，接收来自NodeManager的资源汇报信息，并向NodeManager下达管理指令（比如杀死Container等）；</li>
<li>资源管理与调度，接收来自ApplicationMaster的资源申请请求，并为之分配资源。</li>
</ul>
<h2 id="2-ResourceManager内部架构"><a href="#2-ResourceManager内部架构" class="headerlink" title="2. ResourceManager内部架构"></a>2. ResourceManager内部架构</h2><p>这部分主要介绍ResourceManager的内部组织结构和主要模块，如图所示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/08/8fe952fed3089589fb0d146b0e01ca29-1636344037139-e1372801-a1af-4c3f-a04b-989a1388280a-1963ce.png" alt="img"></p>
<center>ResourceManager内部架构图</center>

<p>ResourceManager主要由以下几个部分组成：</p>
<h3 id="2-1-用户交互模块"><a href="#2-1-用户交互模块" class="headerlink" title="2.1 用户交互模块"></a>2.1 用户交互模块</h3><p>ResourceManager分别针对普通用户、管理员和Web提供了三种对外服务，具体实现分别对应ClientRMService、AdminService和WebApp。</p>
<p><strong>ClientRMService。</strong>ClientRMService是为普通用户提供的服务，它处理来自客户端各种RPC请求，比如提交应用程序、终止应用程序、获取应用程序运行状态等。</p>
<p><strong>AdminService。</strong>ResourceManager为管理员提供了一套独立的服务接口，以防止大量的普通用户请求使管理员发送的管理命令饿死，管理员可通过这些接口管理集群，比如动态更新节点列表、更新ACL列表、更新队列信息等。　</p>
<p><strong>WebApp。</strong>为了更加友好地展示集群资源使用情况和应用程序运行状态等信息，YARN对外提供了一个Web 界面，这一部分是YARN仿照Haml。</p>
<h3 id="2-2-NM管理模块"><a href="#2-2-NM管理模块" class="headerlink" title="2.2 NM管理模块"></a>2.2 NM管理模块</h3><p>该模块主要涉及以下组件：　</p>
<p><strong>NMLivelinessMonitor。</strong>监控NM是否活着，如果一个NodeManager在一定时间（默认为10min）内未汇报心跳信息，则认为它死掉了，需将其从集群中移除。　</p>
<p><strong>NodesListManager。</strong>维护正常节点和异常节点列表，管理exclude（类似于黑名单）和include（类似于白名单）节点列表，这两个列表均是在配置文件中设置的，可以动态加载。</p>
<p><strong>ResourceTrackerService。</strong>处理来自NodeManager的请求，主要包括注册和心跳两种请求，其中，注册是NodeManager启动时发生的行为，请求包中包含节点ID、可用的资源上限等信息；而心跳是周期性行为，包含各个Container运行状态，运行的Application列表、节点健康状况（可通过一个脚本设置）等信息，作为请求的应答，ResourceTrackerService可为NodeManager返回待释放的Container列表、Application列表等信息。</p>
<h3 id="2-3-AM管理模块"><a href="#2-3-AM管理模块" class="headerlink" title="2.3 AM管理模块"></a>2.3 AM管理模块</h3><p>该模块主要涉及以下组件： </p>
<p><strong>AMLivelinessMonitor</strong>。监控AM是否活着，如果一个ApplicationMaster在一定时间（默认为10min）内未汇报心跳信息，则认为它死掉了，它上面所有正在运行的Container将被置为失败状态，而AM本身会被重新分配到另外一个节点上（用户可指定每个ApplicationMaster的尝试次数，默认是2）执行。　</p>
<p><strong>ApplicationMasterLauncher。</strong>与某个NodeManager通信，要求它为某个应用程序启动ApplicationMaster。　</p>
<p><strong>ApplicationMasterService（AMS）。</strong>处理来自ApplicationMaster的请求，主要包括注册和心跳两种请求，其中，注册是ApplicationMaster启动时发生的行为，注册请求包中包含ApplicationMaster启动节点；对外RPC端口号和tracking URL等信息；而心跳则是周期性行为，汇报信息包含所需资源描述、待释放的Container列表、黑名单列表等，而AMS则为之返回新分配的Container、失败的Container、待抢占的Container列表等信息。</p>
<h3 id="2-4-Application管理模块"><a href="#2-4-Application管理模块" class="headerlink" title="2.4 Application管理模块"></a>2.4 Application管理模块</h3><p>该模块主要涉及以下组件：　 </p>
<p><strong>ApplicationACLsManage。</strong>管理应用程序访问权限，包含两部分权限：查看权限和修改权限。查看权限主要用于查看应用程序基本信息，而修改权限则主要用于修改应用程序优先级、杀死应用程序等。　</p>
<p><strong>RMAppManager。</strong>管理应用程序的启动和关闭。　</p>
<p><strong>ContainerAllocationExpirer。</strong>当AM收到RM新分配的一个Container后，必须在一定的时间（默认为10min）内在对应的NM上启动该Container，否则RM将强制回收该Container，而一个已经分配的Container是否该被回收则是由ContainerAllocationExpirer决定和执行的。</p>
<h3 id="2-5-状态机管理模块"><a href="#2-5-状态机管理模块" class="headerlink" title="2.5 状态机管理模块"></a>2.5 状态机管理模块</h3><p>ResourceManager使用有限状态机维护有状态对象的生命周期，状态机的引入使得YARN设计架构更加清晰。ResourceManager共维护了4类状态机，分别是RMApp、RMAppAttempt、RMContainer和RMNode（这几个均是接口，具体实现类为对应接口名加上”Impl”后缀）。　</p>
<p><strong>RMApp。</strong>RMApp维护了一个应用程序（Application）的整个运行周期，包括从启动到运行结束整个过程。由于一个Application的生命周期可能会启动多个Application运行实例（Application Attempt），因此可认为，RMApp维护的是同一个Application启动的所有运行实例的生命周期。　</p>
<p><strong>RMAppAttempt。</strong>一个应用程序可能启动多个实例，即一个实例运行失败后，可能再次启动一个重新运行，而每次启动称为一次运行尝试（或者“运行实例”），用”RMAppAttempt”描述，RMAppAttempt维护了一次运行尝试的整个生命周期。　</p>
<p><strong>RMContainer。</strong>RMContainer维护了一个Container的运行周期，包括从创建到运行结束整个过程。ResourceManager将资源封装成Container发送给应用程序的ApplicationMaster，而ApplicationMaster则会在Container描述的运行环境中启动任务，因此，从这个层面上讲，Container和任务的生命周期是一致的（目前YARN尚不支持Container重用，一个Container用完后会立刻释放，将来可能会增加Container重用机制）。　</p>
<p><strong>RMNode。</strong>RMNode维护了一个NodeManager的生命周期，包括启动到运行结束整个过程。</p>
<h3 id="2-6-安全管理模块"><a href="#2-6-安全管理模块" class="headerlink" title="2.6 安全管理模块"></a>2.6 安全管理模块</h3><p>ResourceManage自带了非常全面的权限管理机制，主要由ClientTo-AMSecretManager、ContainerTokenSecretManager、ApplicationTokenSecretManager等模块完成。</p>
<h3 id="2-7-资源分配模块"><a href="#2-7-资源分配模块" class="headerlink" title="2.7 资源分配模块"></a>2.7 资源分配模块</h3><p>该模块主要涉及一个组件—ResourceScheduler。ResourceScheduler是资源调度器，它按照一定的约束条件（比如队列容量限制等）将集群中的资源分配给各个应用程序，当前主要考虑内存和CPU资源。ResourceScheduler是一个插拔式模块，YARN自带了一个批处理资源调度器—FIFO（First In First Out）和两个多用户调度器—Fair Scheduler和CapacityScheduler。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>董西成.《Hadoop技术内幕：深入解析YARN架构设计与实现原理》</li>
</ul>
]]></content>
      <categories>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title>始于日记，忠于写作</title>
    <url>/2021/11/10/%E5%A7%8B%E4%BA%8E%E6%97%A5%E8%AE%B0%EF%BC%8C%E5%BF%A0%E4%BA%8E%E5%86%99%E4%BD%9C/</url>
    <content><![CDATA[<p>一直想写点东西，但迟迟没有动身，一是不知道写什么，二是担心写不好，便一拖再拖，可明日复明日，明日何其多，结果就是永远不会有进展。</p>
<p>想做一件事，不去做始终是一项遗憾，正如非洲经济学家 Dambisa Moyo（丹比萨·莫约）在书籍 《dead aid》结束语中所说，只要你兴趣还在，可以一直做，什么时候都不会晚，种一棵树最好的时间是十年前，其次是现在。</p>
<p>确实，一件事，只要开始行动，终究是有回馈的，不管是积极的还是消极的，一旦有变化就是好的征兆，因为行动之后的连锁效应是无法预估的。</p>
<p>因为对写作充满畏惧，一直没有行动，于是选择从最简单的方式开始——写日记。日记是一种特殊的写作方式，因为读者通常只有你自己，这就给了作者很大的发挥空间，你不用顾及自己写什么，也不用担心写不好或者被别人嘲笑。自己坚持写日记有一段时间，也就有了现在在平台写作的勇气，写得好与坏姑且不做评判，但一定是自己想写的内容，也一定是最真实的想法。</p>
<p>从自己写日记的经验来看，日记内容主要包括三个方面，分别是：工作日记、读书心得和思想感悟。</p>
<ul>
<li><p>记好工作日记，既要记录工作的过程也要记录工作的结果，记录过程是为了让自己同样的错误不犯两遍，也是为了积累更多成功的经验，而记录结果是工作的产出，更是努力收获的果实。</p>
</li>
<li><p>阅读，通常是为了获得收获，包括对全书轮廓的了解、书中细节和知识的了解，以及作者在写作技巧和手法的了解等方面，阅读过的书，把书中精彩的段落记下来，把自己的收获和想法写下来，过一遍脑子，才能更好的变成自己的东西。</p>
</li>
<li><p>记录思想感悟，既可以是思维认知上的感知，也可以是技巧方法上的体会，既可以是现实世界的亲身目睹，也可以是网络世界的“百家争鸣”，既可以是成功的经历，也可以是失败的教训，能让你产生思考和想法的事物，都是可以记录的。</p>
</li>
</ul>
<p>而对于写日记的形式，也是有一些经验技巧。</p>
<p>首先，每次只写一件事。为了达到练习写作的目的，最好努力做到表达清晰、行文流畅、用词准确，尽量避免文章中有太多的基本错误。</p>
<p>其次，带着目的去写。比如你是为了理清某一个问题的思路，还是总结工作内容的得失，亦或是总结时间分配是否合理等等，目的性要很聚焦。</p>
<p>然后，写作要真实。不用在意想法多么单纯不成熟，也不用在意以后想法是否会改变，记录当下最真实的想法，它像是一面镜子，需要时间回顾复盘，不断校正和优化自己的想法。</p>
<p>最后，坚持每天写。写便是行动，行动终将会前进。</p>
]]></content>
      <categories>
        <category>flomo</category>
      </categories>
      <tags>
        <tag>随想录</tag>
      </tags>
  </entry>
  <entry>
    <title>YARN FairScheduler调度原理与源码分析</title>
    <url>/2021/11/06/YARN-FairScheduler%E8%B0%83%E5%BA%A6%E5%8E%9F%E7%90%86%E4%B8%8E%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>本文主要介绍 YARN 组件中的核心功能——资源调度，YARN 调度策略包括 FifoScheduler、FairSheduler 和 CapacityScheduler 三种，本文主要介绍 FairScheduler 的调度逻辑，看看集群的 NodeManager 是如何将资源上报给 ResourceManager（简称 RM），RM 是如何集中管理和分配这些资源，作业启动后 ApplicationMaster 又是如何定期向 RM 申请资源，接下来我们详细分析。</p>
<h1 id="1-YARN-资源调度方式"><a href="#1-YARN-资源调度方式" class="headerlink" title="1. YARN 资源调度方式"></a>1. YARN 资源调度方式</h1><p>资源调度方式确定了当任务提交到集群，如何为其分配资源执行任务。在 FairScheduler 中提供了两种资源调度方式：心跳调度和连续调度。</p>
<ul>
<li><strong>心跳调度方式：</strong>NodeManager 向 ResourceManager 汇报了自身资源情况（比如，当前可用资源，正在使用的资源，已经释放的资源)，这个 RPC 会触发 ResourceManager 调用 nodeUpdate() 方法，这个方法为这个节点进行一次资源调度，即，从维护的 Queue 中取出合适的应用的资源请求(合适 ，指的是这个资源请求既不违背队列的最大资源使用限制，也不违背这个 NodeManager 的剩余资源量限制)放到这个NodeManager上运行。这种调度方式一个主要缺点就是调度缓慢，当一个NodeManager即使已经有了剩余资源，调度也只能在心跳发送以后才会进行，不够及时。</li>
<li><strong>连续调度方式：</strong>由一个独立的线程 ContinuousSchedulingThread 负责进行持续的资源调度，与 NodeManager 的心跳是异步进行的。即不需要等到 NodeManager 发来心跳才开始资源调度。 </li>
</ul>
<p>无论是 NodeManager 心跳时触发调度，还是通过 ContinuousSchedulingThread 进行实时、持续触发，他们对某个节点进行一次调度的算法和原理是公用的，都是通过 synchronized void attemptScheduling(FSSchedulerNode node) 来在某个节点上进行一次调度，方法的的参数代表了准备进行资源分配的节点。两种触发机制不同的地方只有两个：</p>
<ul>
<li>调度时机：心跳调度仅仅发生在收到了某个 NodeManager 的心跳信息的情况下，持续调度则不依赖与NodeManager的心跳通信，是连续发生的，当心跳到来，会将调度结果直接返回给 NodeManager；</li>
<li>调度范围：心跳调度机制下，当收到某个节点的心跳，就对这个节点且仅仅对这个节点进行一次调度，即谁的心跳到来就触发对谁的调度，而持续调度的每一轮，是会遍历当前集群的所有节点，每个节点依次进行一次调度，保证一轮下来每一个节点都被公平的调度一次；</li>
</ul>
<p>在集群环境中，连续调度默认不开启，只有设置 YARN.scheduler.fair.continuous-scheduling-enabled 参数为 true，才会启动该线程。连续调度现在已经不推荐了，因为它会因为锁的问题，而导致资源调度变得缓慢。可以使用 YARN.scheduler.assignmultiple 参数启动批量分配功能，作为连续调度的替代。</p>
<h1 id="2-YARN-调度流程"><a href="#2-YARN-调度流程" class="headerlink" title="2. YARN 调度流程"></a>2. YARN 调度流程</h1><p>本文的调度流程主要介绍心跳调度的方式，下图是 YARN 心跳调度的主要流程。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/07/b7e1bab2bb03ef9579f700d67eba79f2-1372882-20200826170350663-775889568-dd9425.png" alt="img"></p>
<center>YARN 调度流程图</center>

<h2 id="2-1-名词解释"><a href="#2-1-名词解释" class="headerlink" title="2.1 名词解释"></a>2.1 名词解释</h2><ul>
<li>ResrouceSacheduler 是YARN 的调度器，负责 Container 的分配。下面主要是以 FairScheduler 调度器为例。</li>
<li>AsyncDispatcher 是单线程的事件分发器，负责向调度器发送调度事件。</li>
<li>ResourceTrackerService 是资源跟踪器，主要负责接收处理 NodeManager 的心跳信息。</li>
<li>ApplicationMasterService 是作业的 RPC 服务，主要负责接收处理作业的心跳信息。</li>
<li>AppMaster 是作业的程序控制器，负责跟 YARN 交互获取/释放资源。</li>
</ul>
<h2 id="2-2-调度流程"><a href="#2-2-调度流程" class="headerlink" title="2.2 调度流程"></a>2.2 调度流程</h2><p>YARN 的资源调度是异步进行的，NM 心跳发生时，调度器 ResourceScheduler 根据作业需求将 Container 资源分配给作业后，不会立即通知 AM，而是等待 AM 注册后通过心跳方式来主动获取。YARN 的整个调度流程可以概括为以下几个步骤：</p>
<ol>
<li>NM 节点通过心跳方式向 RM 汇报节点资源信息（包括当前可用资源、正在使用的资源、已经释放的资源)。</li>
<li>ResourceTrackerService 服务收到 NM 的心跳事件，将 NODE_UPDATE 事件交给 中央调度器 AsyncDispatcher 处理；</li>
<li>AsyncDispatcher 根据事件类型将请求转发给 ResourceScheduler 处理，ResourceScheduler 则按照一定的调度策略（队列层级调度）将 NM 的资源分配到 Container，并将 Container 保存在数据结构中；</li>
<li>ResourceScheduler 针对作业分配的第一个 Container 用于启动作业的 AM 进程；</li>
<li>AM 启动后，通过 ApplicationMasterService 定期向 RM 发生资源请求心跳，领取之前记录在 RM 中分配给自己的 Container 资源；</li>
<li>AM 向 NM 发送启动 Container 的命令，将收到的 Container 在 NM 上启动执行。</li>
</ol>
<p>其中，YARN 分配策略确定了在 NM 发生心跳时，如何在所有队列中选择合适的 APP 资源请求以为其分配资源。从上图队列层级结构可以看出一次 Container 的分配流程：每次分配从 root 节点开始，先从队列中选择合适的叶子队列，然后从队列的 APP 中选择合适的 APP，最后选择出该 APP 中合适的 Container 为其分配资源执行任务，选择过程如下：</p>
<ul>
<li><strong>选择队列 （排序）。</strong>从根队列开始，使用深度优先遍历算法，从根队列开始，依次遍历子队列找出资源占用率最小的子队列。若子队列为叶子队列，则选择该队列；若子队列为非叶子队列，则以该子队列为根队列重复前面的过程直到找到一个资源使用率最小的叶子队列为止。</li>
<li><strong>选择应用</strong> <strong>（排序）。</strong>在Step1中选好了叶子队列后，取该队列中排序最靠前的应用程序（排序逻辑可以根据应用程序的资源请求量、提交时间、作业名）。</li>
<li><strong>选择 Container （排序）。</strong>在 Step2中选好应用程序之后，选择该应用程序中优先级最高的 Container。对于优先级相同的 Containers，优选选择满足本地性的 Container，会依次选择 node local、rack local、no local。</li>
</ul>
<h1 id="3-FairScheduler-资源分配"><a href="#3-FairScheduler-资源分配" class="headerlink" title="3. FairScheduler 资源分配"></a>3. FairScheduler 资源分配</h1><h2 id="3-1-NM-心跳上报"><a href="#3-1-NM-心跳上报" class="headerlink" title="3.1 NM 心跳上报"></a>3.1 NM 心跳上报</h2><p>NM 中负责心跳的类是 NodeStatusUpdater 类型的成员变量 nodeStatusUpdater，在 NM 调用 serviceInit() 方法时被创建：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//代码：org/apache/hadoop/yarn/server/nodemanager/NodeManager.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    ...  <span class="comment">// 省略</span></span><br><span class="line">    nodeStatusUpdater =</span><br><span class="line">        createNodeStatusUpdater(context, dispatcher, nodeHealthChecker);</span><br><span class="line">    ...  <span class="comment">// 省略</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> NodeStatusUpdater <span class="title">createNodeStatusUpdater</span><span class="params">(Context context,</span></span></span><br><span class="line"><span class="params"><span class="function">      Dispatcher dispatcher, NodeHealthCheckerService healthChecker)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> NodeStatusUpdaterImpl(context, dispatcher, healthChecker,</span><br><span class="line">      metrics);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>nodeStatusUpdater 在创建时初始化实例 NodeStatusUpdaterImpl，它是真正负责与 RM 通讯的类，其中 serviceStart() 方法中会进行 NM 注册和心跳。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceStart</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// NodeManager is the last service to start, so NodeId is available.</span></span><br><span class="line">    <span class="keyword">this</span>.nodeId = <span class="keyword">this</span>.context.getNodeId();</span><br><span class="line">    <span class="keyword">this</span>.httpPort = <span class="keyword">this</span>.context.getHttpPort();</span><br><span class="line">    <span class="keyword">this</span>.nodeManagerVersionId = YARNVersionInfo.getVersion();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// Registration has to be in start so that ContainerManager can get the</span></span><br><span class="line">      <span class="comment">// perNM tokens needed to authenticate ContainerTokens.</span></span><br><span class="line">      <span class="keyword">this</span>.resourceTracker = getRMClient();</span><br><span class="line">      registerWithRM();         <span class="comment">// NM向RM注册</span></span><br><span class="line">      <span class="keyword">super</span>.serviceStart();</span><br><span class="line">      startStatusUpdater();     <span class="comment">// 独立线程进行NM心跳上报</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      String errorMessage = <span class="string">&quot;Unexpected error starting NodeStatusUpdater&quot;</span>;</span><br><span class="line">      LOG.error(errorMessage, e);</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> YARNRuntimeException(e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>NM 向 RM 注册逻辑直接跳过，重点看一下心跳逻辑，首先启动心跳线程：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">startStatusUpdater</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    statusUpdaterRunnable = <span class="keyword">new</span> Runnable() &#123; ... &#125;;</span><br><span class="line">    statusUpdater =</span><br><span class="line">        <span class="keyword">new</span> Thread(statusUpdaterRunnable, <span class="string">&quot;Node Status Updater&quot;</span>);</span><br><span class="line">    statusUpdater.start();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>接着来看看 statusUpdaterRunnable 线程如何进行心跳上报：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java</span></span><br><span class="line">    statusUpdaterRunnable = <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> lastHeartBeatID = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (!isStopped) &#123;    <span class="comment">// 在被终止前死循环的跑</span></span><br><span class="line">          <span class="comment">// Send heartbeat</span></span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            NodeHeartbeatResponse response = <span class="keyword">null</span>;</span><br><span class="line">            NodeStatus nodeStatus = getNodeStatus(lastHeartBeatID);</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 构建 request 请求信息</span></span><br><span class="line">            NodeHeartbeatRequest request =</span><br><span class="line">                NodeHeartbeatRequest.newInstance(nodeStatus,</span><br><span class="line">                  NodeStatusUpdaterImpl.<span class="keyword">this</span>.context</span><br><span class="line">                    .getContainerTokenSecretManager().getCurrentKey(),</span><br><span class="line">                  NodeStatusUpdaterImpl.<span class="keyword">this</span>.context.getNMTokenSecretManager()</span><br><span class="line">                    .getCurrentKey());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 重点：这里向 RM 发送心跳 RPC 请求，并得到返回结果 response</span></span><br><span class="line">            response = resourceTracker.nodeHeartbeat(request);</span><br><span class="line">            <span class="comment">//get next heartbeat interval from response</span></span><br><span class="line">            nextHeartBeatInterval = response.getNextHeartBeatInterval();</span><br><span class="line">            updateMasterKeys(response);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (response.getNodeAction() == NodeAction.SHUTDOWN) &#123;</span><br><span class="line">              <span class="comment">// 处理 RM 返回的结果，包括停止运行和重新注册</span></span><br><span class="line">            &#125;</span><br><span class="line">            ...  <span class="comment">// 省略</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br></pre></td></tr></table></figure>

<p>至此，NM 已经通过 NodeStatusUpdaterImpl 类向 RM 发送了心跳请求，那 RM 又如何处理该心跳请求呢？我们接着分析。</p>
<h2 id="3-2-RM-接收心跳"><a href="#3-2-RM-接收心跳" class="headerlink" title="3.2 RM 接收心跳"></a>3.2 RM 接收心跳</h2><p>NM 与 RM 通讯的接口是通过 ResourceTrackerService 服务来实现。直接来看看 NM 调用 nodeHeartbeat() 方法发送过来的请求。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> NodeHeartbeatResponse <span class="title">nodeHeartbeat</span><span class="params">(NodeHeartbeatRequest request)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> YARNException, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    NodeStatus remoteNodeStatus = request.getNodeStatus();</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 处理心跳的流程：</span></span><br><span class="line"><span class="comment">     * 1. 判断是否是合法的 node（即是否被拉黑过）</span></span><br><span class="line"><span class="comment">     * 2. 判断是否是一个注册过的 node</span></span><br><span class="line"><span class="comment">     * 3. 判断这个心跳是否是重复的心跳</span></span><br><span class="line"><span class="comment">     * 4. 发送 NM 的状态给 RMNodeStatusEvent 事件处理器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    ... <span class="comment">// 1-3 步跳过</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. Send status to RMNode, saving the latest response.</span></span><br><span class="line">    RMNodeStatusEvent nodeStatusEvent =</span><br><span class="line">        <span class="keyword">new</span> RMNodeStatusEvent(nodeId, remoteNodeStatus.getNodeHealthStatus(),</span><br><span class="line">          remoteNodeStatus.getContainersStatuses(),</span><br><span class="line">          remoteNodeStatus.getKeepAliveApplications(), nodeHeartBeatResponse);</span><br><span class="line">    <span class="keyword">if</span> (request.getLogAggregationReportsForApps() != <span class="keyword">null</span></span><br><span class="line">        &amp;&amp; !request.getLogAggregationReportsForApps().isEmpty()) &#123;</span><br><span class="line">      nodeStatusEvent.setLogAggregationReportsForApps(request</span><br><span class="line">        .getLogAggregationReportsForApps());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.rmContext.getDispatcher().getEventHandler().handle(nodeStatusEvent);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nodeHeartBeatResponse;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>心跳处理过程的关键是第 4 步，它会通过中央调度器 AsyncDispatcher 向 RM 发送 RMNodeStatusEvent 事件，那这个事件是由谁来处理的呢？在 YARN 这种事件处理逻辑很常见，关键点是要看事件对应的处理器是如何注册的。上面的 RMNodeStatusEvent 事件处理器继承自 RMNodeEvent，在 RM 的注册处理器代码如下。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">      <span class="comment">// Register event handler for RmNodes</span></span><br><span class="line">      rmDispatcher.register(</span><br><span class="line">          RMNodeEventType.class, <span class="keyword">new</span> NodeEventDispatcher(rmContext));</span><br></pre></td></tr></table></figure>

<p>其中 RMNodeStatusEvent 事件是交由 NodeEventDispatcher 调度器处理，处理的 handle() 方法如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(RMNodeEvent event)</span> </span>&#123;</span><br><span class="line">      NodeId nodeId = event.getNodeId();</span><br><span class="line">      RMNode node = <span class="keyword">this</span>.rmContext.getRMNodes().get(nodeId);</span><br><span class="line">      <span class="keyword">if</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          ((EventHandler&lt;RMNodeEvent&gt;) node).handle(event);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">          LOG.error(<span class="string">&quot;Error in handling event type &quot;</span> + event.getType()</span><br><span class="line">              + <span class="string">&quot; for node &quot;</span> + nodeId, t);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>这里会调用 RMNode 的handle() 方法，RMNode 是一个接口类，实现类为 RMNodeImpl，对应 handle() 方法如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(RMNodeEvent event)</span> </span>&#123;</span><br><span class="line">  LOG.debug(<span class="string">&quot;Processing &quot;</span> + event.getNodeId() + <span class="string">&quot; of type &quot;</span> + event.getType());</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    writeLock.lock();</span><br><span class="line">    NodeState oldState = getState();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">       stateMachine.doTransition(event.getType(), event);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InvalidStateTransitonException e) &#123;</span><br><span class="line">      LOG.error(<span class="string">&quot;Can&#x27;t handle this event at current state&quot;</span>, e);</span><br><span class="line">      LOG.error(<span class="string">&quot;Invalid event &quot;</span> + event.getType() + </span><br><span class="line">          <span class="string">&quot; on Node  &quot;</span> + <span class="keyword">this</span>.nodeId);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (oldState != getState()) &#123;</span><br><span class="line">      LOG.info(nodeId + <span class="string">&quot; Node Transitioned from &quot;</span> + oldState + <span class="string">&quot; to &quot;</span></span><br><span class="line">               + getState());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">finally</span> &#123;</span><br><span class="line">    writeLock.unlock();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里就涉及到 RMNodeImpl 的状态机，由于 RMNodeStatusEvent 事件类型是 RMNodeEventType.STATUS_UPDATE，状态机中对这个事件的处理有三种情况：</p>
<ul>
<li>从 RUNNING 到 RUNNING、UNHEALTHY，调用 StatusUpdateWhenHealthyTransition；</li>
<li>从 DECOMMISSIONING 到 DECOMMISSIONING、DECOMMISSIONED，调用 StatusUpdateWhenHealthyTransition；</li>
<li>从 UNHEALTHY 到 UNHEALTHY、RUNNING，调用 StatusUpdateWhenUnHealthyTransition；</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java</span></span><br><span class="line">      <span class="comment">//Transitions from RUNNING state</span></span><br><span class="line">      .addTransition(NodeState.RUNNING,</span><br><span class="line">          EnumSet.of(NodeState.RUNNING, NodeState.UNHEALTHY),</span><br><span class="line">          RMNodeEventType.STATUS_UPDATE,</span><br><span class="line">          <span class="keyword">new</span> StatusUpdateWhenHealthyTransition())</span><br><span class="line">      <span class="comment">//Transitions from DECOMMISSIONING state</span></span><br><span class="line">      .addTransition(NodeState.DECOMMISSIONING,</span><br><span class="line">          EnumSet.of(NodeState.DECOMMISSIONING, NodeState.DECOMMISSIONED),</span><br><span class="line">          RMNodeEventType.STATUS_UPDATE,</span><br><span class="line">          <span class="keyword">new</span> StatusUpdateWhenHealthyTransition())</span><br><span class="line">      <span class="comment">//Transitions from UNHEALTHY state</span></span><br><span class="line">      .addTransition(NodeState.UNHEALTHY,</span><br><span class="line">          EnumSet.of(NodeState.UNHEALTHY, NodeState.RUNNING),</span><br><span class="line">          RMNodeEventType.STATUS_UPDATE,</span><br><span class="line">          <span class="keyword">new</span> StatusUpdateWhenUnHealthyTransition())</span><br></pre></td></tr></table></figure>

<p>这里选择最常见的状态转换，从 RUNNING 到 RUNNING，查看被调用的 StatusUpdateWhenHealthyTransition 状态机的 transition() 方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NodeState <span class="title">transition</span><span class="params">(RMNodeImpl rmNode, RMNodeEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">      RMNodeStatusEvent statusEvent = (RMNodeStatusEvent) event;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Switch the last heartbeatresponse.</span></span><br><span class="line">      rmNode.latestNodeHeartBeatResponse = statusEvent.getLatestResponse();</span><br><span class="line"></span><br><span class="line">      NodeHealthStatus remoteNodeHealthStatus =</span><br><span class="line">          statusEvent.getNodeHealthStatus();</span><br><span class="line">      rmNode.setHealthReport(remoteNodeHealthStatus.getHealthReport());</span><br><span class="line">      rmNode.setLastHealthReportTime(</span><br><span class="line">          remoteNodeHealthStatus.getLastHealthReportTime());</span><br><span class="line">      NodeState initialState = rmNode.getState();</span><br><span class="line">      <span class="keyword">boolean</span> isNodeDecommissioning =</span><br><span class="line">          initialState.equals(NodeState.DECOMMISSIONING);</span><br><span class="line">      </span><br><span class="line">      ... <span class="comment">// 跳过 unhealthy 和 decommsission 的判断逻辑</span></span><br><span class="line"></span><br><span class="line">      rmNode.handleContainerStatus(statusEvent.getContainers());</span><br><span class="line"></span><br><span class="line">      List&lt;LogAggregationReport&gt; logAggregationReportsForApps =</span><br><span class="line">          statusEvent.getLogAggregationReportsForApps();</span><br><span class="line">      <span class="keyword">if</span> (logAggregationReportsForApps != <span class="keyword">null</span></span><br><span class="line">          &amp;&amp; !logAggregationReportsForApps.isEmpty()) &#123;</span><br><span class="line">        rmNode.handleLogAggregationStatus(logAggregationReportsForApps);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(rmNode.nextHeartBeat) &#123;</span><br><span class="line">        rmNode.nextHeartBeat = <span class="keyword">false</span>;</span><br><span class="line">        <span class="comment">// 重点：向 RM 发送一个 NodeUpdateSchedulerEvent 事件</span></span><br><span class="line">        rmNode.context.getDispatcher().getEventHandler().handle(</span><br><span class="line">            <span class="keyword">new</span> NodeUpdateSchedulerEvent(rmNode));</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Update DTRenewer in secure mode to keep these apps alive. Today this is</span></span><br><span class="line">      <span class="comment">// needed for log-aggregation to finish long after the apps are gone.</span></span><br><span class="line">      <span class="keyword">if</span> (UserGroupInformation.isSecurityEnabled()) &#123;</span><br><span class="line">        rmNode.context.getDelegationTokenRenewer().updateKeepAliveApplications(</span><br><span class="line">          statusEvent.getKeepAliveAppIds());</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> initialState;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里关键的逻辑是向 RM 发送了一个 NodeUpdateSchedulerEvent 事件，那这个事件又是谁处理的呢？NodeUpdateSchedulerEvent 继承自 SchedulerEvent，SchedulerEvent在RM中注册的处理器如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">  <span class="comment">// 注册 SchedulerEventDispatcher</span></span><br><span class="line">  schedulerDispatcher = createSchedulerEventDispatcher();</span><br><span class="line">  addIfService(schedulerDispatcher);</span><br><span class="line">  rmDispatcher.register(SchedulerEventType.class, schedulerDispatcher);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 注册方法</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> EventHandler&lt;SchedulerEvent&gt; <span class="title">createSchedulerEventDispatcher</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> SchedulerEventDispatcher(<span class="keyword">this</span>.scheduler);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>其中 scheduler 对象是根据配置 YARN.resourcemanager.YARN.resourcemanager 指定的类生成的对象，这里使用 org.apache.hadoop.YARN.server.resourcemanager.scheduler.fair.FairScheduler。那就进入到 FairScheduler 的 handle() 方法，这里只看 NODE_UPDATE 事件的处理逻辑，其他的先省略。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(SchedulerEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> start = getClock().getTime();</span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">    <span class="keyword">case</span> NODE_ADDED:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> NODE_REMOVED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> NODE_UPDATE:</span><br><span class="line">      <span class="keyword">if</span> (!(event <span class="keyword">instanceof</span> NodeUpdateSchedulerEvent)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Unexpected event type: &quot;</span> + event);</span><br><span class="line">      &#125;</span><br><span class="line">      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;</span><br><span class="line">      nodeUpdate(nodeUpdatedEvent.getRMNode());</span><br><span class="line">      fsOpDurations.addHandleNodeUpdateEventDuration(getClock().getTime() - start);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> APP_ADDED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> APP_REMOVED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> NODE_RESOURCE_UPDATE: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> APP_ATTEMPT_ADDED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> APP_ATTEMPT_REMOVED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CONTAINER_EXPIRED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CONTAINER_RESCHEDULED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">      LOG.error(<span class="string">&quot;Unknown event arrived at FairScheduler: &quot;</span> + event.toString());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>由于 NM 的心跳事件是 RMNodeEventType.STATUS_UPDATE，可以得知这里处理的事件类型为 SchedulerEventType.NODE_UPDATE，进入NODE_UPDATE处理逻辑。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">nodeUpdate</span><span class="params">(RMNode nm)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      writeLock.lock();</span><br><span class="line">      <span class="keyword">long</span> start = getClock().getTime();</span><br><span class="line">      <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">        LOG.debug(<span class="string">&quot;nodeUpdate: &quot;</span> + nm +</span><br><span class="line">            <span class="string">&quot; cluster capacity: &quot;</span> + getClusterResource());</span><br><span class="line">      &#125;</span><br><span class="line">      eventLog.log(<span class="string">&quot;HEARTBEAT&quot;</span>, nm.getHostName());</span><br><span class="line">      FSSchedulerNode node = getFSSchedulerNode(nm.getNodeID());</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Containe 状态更新：处理新运行或者运行完成的 Container</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 判断 NM 是否是 DECOMMISSIONING 状态</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 核心调度入口，无论是否开启连续调度入口都是 attemptScheduling(node) 方法</span></span><br><span class="line">      <span class="keyword">if</span> (continuousSchedulingEnabled) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!completedContainers.isEmpty()) &#123;</span><br><span class="line">          attemptScheduling(node);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        attemptScheduling(node);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">long</span> duration = getClock().getTime() - start;</span><br><span class="line">      fsOpDurations.addNodeUpdateDuration(duration);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      writeLock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>看看核心调度入口，这里获取了一个 FSSchedulerNode 实例，并尝试进行调度。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">attemptScheduling</span><span class="params">(FSSchedulerNode node)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 检查是否是有效的 node</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Assign new containers...</span></span><br><span class="line">    <span class="comment">// 1. 检查是否有资源预留的应用</span></span><br><span class="line">    <span class="comment">// 2. 没有预留则进行调度分配新的 Container</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> validReservation = <span class="keyword">false</span>;</span><br><span class="line">    FSAppAttempt reservedAppSchedulable = node.getReservedAppSchedulable();</span><br><span class="line">    <span class="keyword">if</span> (reservedAppSchedulable != <span class="keyword">null</span>) &#123;</span><br><span class="line">      validReservation = reservedAppSchedulable.assignReservedContainer(node);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!validReservation) &#123;</span><br><span class="line">      <span class="comment">// No reservation, schedule at queue which is farthest below fair share</span></span><br><span class="line">      <span class="keyword">int</span> assignedContainers = <span class="number">0</span>;</span><br><span class="line">      Resource assignedResource = Resources.clone(Resources.none());</span><br><span class="line">      Resource maxResourcesToAssign =</span><br><span class="line">          Resources.multiply(node.getAvailableResource(), <span class="number">0.5f</span>);</span><br><span class="line">      <span class="keyword">while</span> (node.getReservedContainer() == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">boolean</span> assignedContainer = <span class="keyword">false</span>;</span><br><span class="line">        <span class="comment">// 重点：核心分配逻辑开始，从 ROOT 队列开始调度</span></span><br><span class="line">        Resource assignment = queueMgr.getRootQueue().assignContainer(node);</span><br><span class="line">        <span class="keyword">if</span> (!assignment.equals(Resources.none())) &#123;</span><br><span class="line">          assignedContainers++;</span><br><span class="line">          assignedContainer = <span class="keyword">true</span>;</span><br><span class="line">          Resources.addTo(assignedResource, assignment);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!assignedContainer) &#123; <span class="keyword">break</span>; &#125;</span><br><span class="line">        <span class="keyword">if</span> (!shouldContinueAssigning(assignedContainers,</span><br><span class="line">            maxResourcesToAssign, assignedResource)) &#123;</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    updateRootQueueMetrics();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>分配 Container 是从 ROOT 队列开始，这里调用 queueMgr.getRootQueue() 方法找到 ROOT 队列，然后调用 assignContainer(node) 方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSParentQueue.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Resource <span class="title">assignContainer</span><span class="params">(FSSchedulerNode node)</span> </span>&#123;</span><br><span class="line">    Resource assigned = Resources.none();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果超过了队列的 maxShare 则直接返回</span></span><br><span class="line">    <span class="keyword">if</span> (!assignContainerPreCheck(node)) &#123;</span><br><span class="line">      <span class="keyword">return</span> assigned;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    TreeSet&lt;FSQueue&gt; sortedChildQueues = <span class="keyword">new</span> TreeSet&lt;&gt;(policy.getComparator());</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 这里对所有叶子队列进行排序，有两个情况需要考虑下：</span></span><br><span class="line"><span class="comment">     * 1. 新增加一个 queue，不影响结果的正确性，下次会处理新 queue</span></span><br><span class="line"><span class="comment">     * 2. 删除一个 queue，最好处理一下以不对该 queue 进行分配，不过目前没有处理，也没有影响</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    readLock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        sortedChildQueues.addAll(childQueues);</span><br><span class="line">        <span class="keyword">for</span> (FSQueue child : sortedChildQueues) &#123;</span><br><span class="line">        assigned = child.assignContainer(node);</span><br><span class="line">        <span class="keyword">if</span> (!Resources.equals(assigned, Resources.none())) &#123;</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      readLock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> assigned;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里是 FSParentQueue 父队列的 assignContainer() 逻辑，对所有孩子节点进行遍历，递归调用该该方法，调用过程有两种情况：</p>
<ul>
<li>如果孩子节点是 FSParentQueue 父队列，则递归进入 FSParentQueue 类相同的逻辑中。</li>
<li>如果孩子节点是 FSLeafQueue 叶子队列，则进入到下一步的调用逻辑。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSLeafQueue.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Resource <span class="title">assignContainer</span><span class="params">(FSSchedulerNode node)</span> </span>&#123;</span><br><span class="line">    Resource assigned = none();</span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Node &quot;</span> + node.getNodeName() + <span class="string">&quot; offered to queue: &quot;</span> +</span><br><span class="line">          getName() + <span class="string">&quot; fairShare: &quot;</span> + getFairShare());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检查是否超过队列的 maxShare 限制</span></span><br><span class="line">    <span class="keyword">if</span> (!assignContainerPreCheck(node)) &#123;</span><br><span class="line">      <span class="keyword">return</span> assigned;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历叶子节点所有有资源需求的 APP，并对其尝试分配 Container</span></span><br><span class="line">    <span class="keyword">for</span> (FSAppAttempt sched : fetchAppsWithDemand(<span class="keyword">true</span>)) &#123;</span><br><span class="line">      <span class="keyword">if</span> (SchedulerAppUtils.isBlacklisted(sched, node, LOG)) &#123;</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      assigned = sched.assignContainer(node);</span><br><span class="line">      <span class="keyword">if</span> (!assigned.equals(none())) &#123;</span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">          LOG.debug(<span class="string">&quot;Assigned container in queue:&quot;</span> + getName() + <span class="string">&quot; &quot;</span> +</span><br><span class="line">              <span class="string">&quot;container:&quot;</span> + assigned);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> assigned;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里获取叶子节点的 APP 调用了 fetchAppsWithDemand() 方法，该方法主要是对该队列所有 APP 进行遍历，找到真正有资源需求的 APP，过滤掉没有资源的 APP。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSLeafQueue.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> TreeSet&lt;FSAppAttempt&gt; <span class="title">fetchAppsWithDemand</span><span class="params">(<span class="keyword">boolean</span> assignment)</span> </span>&#123;</span><br><span class="line">    TreeSet&lt;FSAppAttempt&gt; pendingForResourceApps =</span><br><span class="line">        <span class="keyword">new</span> TreeSet&lt;&gt;(policy.getComparator());</span><br><span class="line">    readLock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">for</span> (FSAppAttempt app : runnableApps) &#123;</span><br><span class="line">        <span class="comment">// 判断 APP 是否有资源需求，即有资源还没有得到满足</span></span><br><span class="line">        <span class="keyword">if</span> (!Resources.isNone(app.getPendingDemand()) &amp;&amp;</span><br><span class="line">            (assignment || app.shouldCheckForStarvation())) &#123;</span><br><span class="line">          pendingForResourceApps.add(app);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      readLock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pendingForResourceApps;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>获取到叶子节点有资源需求的 APP 后，调用 FSAppAttempt 类的实例 assignContainer(node) 方法，进行接下来的分配逻辑。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Resource <span class="title">assignContainer</span><span class="params">(FSSchedulerNode node)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 这里主要检查队列已使用资源是否达到了用于运行 AM 的资源限制</span></span><br><span class="line">    <span class="keyword">if</span> (isOverAMShareLimit()) &#123;</span><br><span class="line">      List&lt;ResourceRequest&gt; ask = appSchedulingInfo.getAllResourceRequests();</span><br><span class="line">      Resource amResourceRequest = Resources.none();</span><br><span class="line">      <span class="keyword">if</span> (!ask.isEmpty()) &#123;</span><br><span class="line">        amResourceRequest = ask.get(<span class="number">0</span>).getCapability();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">        LOG.debug(<span class="string">&quot;AM resource request: &quot;</span> + amResourceRequest</span><br><span class="line">            + <span class="string">&quot; exceeds maximum AM resource allowed, &quot;</span></span><br><span class="line">            + getQueue().dumpState());</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> Resources.none();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> assignContainer(node, <span class="keyword">false</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里主要检查队列已使用资源是否达到了用于运行 AM 的资源限制，如果没有的话，则继续调度。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> Resource <span class="title">assignContainer</span><span class="params">(FSSchedulerNode node, <span class="keyword">boolean</span> reserved)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (LOG.isTraceEnabled()) &#123;</span><br><span class="line">      LOG.trace(<span class="string">&quot;Node offered to app: &quot;</span> + getName() + <span class="string">&quot; reserved: &quot;</span> + reserved);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对 APP 的所有 ResourceRequest 按照 priority 排序</span></span><br><span class="line">    Collection&lt;Priority&gt; prioritiesToTry = (reserved) ?</span><br><span class="line">        Arrays.asList(node.getReservedContainer().getReservedPriority()) :</span><br><span class="line">        getPriorities();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// For each priority, see if we can schedule a node local, rack local</span></span><br><span class="line">    <span class="comment">// or off-switch request. Rack of off-switch requests may be delayed</span></span><br><span class="line">    <span class="comment">// (not scheduled) in order to promote better locality.</span></span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">      <span class="comment">// 按照 priority 从高到低遍历所有 ResourceRequest</span></span><br><span class="line">      <span class="keyword">for</span> (Priority priority : prioritiesToTry) &#123;</span><br><span class="line">        <span class="comment">// 判断该 Container 是否有预留</span></span><br><span class="line">        <span class="comment">// hasContainerForNode() 会分 node、rack、any 三种情况考虑该节点是否有合适的 Container</span></span><br><span class="line">        <span class="keyword">if</span> (!reserved &amp;&amp; !hasContainerForNode(priority, node)) &#123;</span><br><span class="line">          <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 调度机会计数加 1</span></span><br><span class="line">        addSchedulingOpportunity(priority);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 下面的逻辑主要根据 NODE_LOCAL、RACK_LOCAL、OFF_SWITCH 三种情况判断该 ResourceRequest 满足哪一种调度方式</span></span><br><span class="line">        ResourceRequest rackLocalRequest = getResourceRequest(priority,</span><br><span class="line">            node.getRackName());</span><br><span class="line">        ResourceRequest localRequest = getResourceRequest(priority,</span><br><span class="line">            node.getNodeName());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (localRequest != <span class="keyword">null</span> &amp;&amp; !localRequest.getRelaxLocality()) &#123;</span><br><span class="line">          LOG.warn(<span class="string">&quot;Relax locality off is not supported on local request: &quot;</span></span><br><span class="line">              + localRequest);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 省略三种情况的具体选择逻辑</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> Resources.none();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>上面这段代码，主要是按照 priority 从高到低的顺序遍历所有的 ResourceRequest，针对每个 ResourceRequest，在待分配的 node 节点上，根据 NODE_LOCAL、RACK_LOCAL、OFF_SWITCH 三种情况判断该 ResourceRequest 满足哪一种调度方式，这里以 NODE_LOCAL 参数为例进入到下一步的调度逻辑。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> Resource <span class="title">assignContainer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      FSSchedulerNode node, ResourceRequest request, NodeType type,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">boolean</span> reserved)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当前 ResoureRequest 需要多少资源</span></span><br><span class="line">    Resource capability = request.getCapability();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当前 node 还剩多少资源可分配</span></span><br><span class="line">    Resource available = node.getAvailableResource();</span><br><span class="line"></span><br><span class="line">    Container reservedContainer = <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">// 判断是否有预留，有预留在直接从该 node 获取对应资源。这里不考虑预留的情况</span></span><br><span class="line">    <span class="keyword">if</span> (reserved) &#123;</span><br><span class="line">      reservedContainer = node.getReservedContainer().getContainer();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断该 ResourRequest 的资源需求是否能够在该 node 上得到满足</span></span><br><span class="line">    <span class="keyword">if</span> (Resources.fitsIn(capability, available)) &#123;</span><br><span class="line">      <span class="comment">// 重点：node 资源足够的话，这里会分配出一个 Container</span></span><br><span class="line">      RMContainer allocatedContainer =</span><br><span class="line">          allocate(type, node, request.getPriority(), request,</span><br><span class="line">              reservedContainer);</span><br><span class="line">      <span class="keyword">if</span> (allocatedContainer == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Did the application need this resource?</span></span><br><span class="line">        <span class="keyword">if</span> (reserved) &#123;</span><br><span class="line">          unreserve(request.getPriority(), node);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Resources.none();</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// If we had previously made a reservation, delete it</span></span><br><span class="line">      <span class="keyword">if</span> (reserved) &#123;</span><br><span class="line">        unreserve(request.getPriority(), node);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 通知 node 记录该分配出来的 Container</span></span><br><span class="line">      node.allocateContainer(allocatedContainer);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// If not running unmanaged, the first container we allocate is always</span></span><br><span class="line">      <span class="comment">// the AM. Set the amResource for this app and update the leaf queue&#x27;s AM</span></span><br><span class="line">      <span class="comment">// usage</span></span><br><span class="line">      <span class="keyword">if</span> (!isAmRunning() &amp;&amp; !getUnmanagedAM()) &#123;</span><br><span class="line">        setAMResource(capability);</span><br><span class="line">        getQueue().addAMResourceUsage(capability);</span><br><span class="line">        setAmRunning(<span class="keyword">true</span>);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> capability;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...  <span class="comment">// 省略</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>重点看看分配逻辑 allocate() 方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">public</span> RMContainer <span class="title">allocate</span><span class="params">(NodeType type, FSSchedulerNode node,</span></span></span><br><span class="line"><span class="params"><span class="function">      Priority priority, ResourceRequest request,</span></span></span><br><span class="line"><span class="params"><span class="function">      Container reservedContainer)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 更新 locality 级别，忽略</span></span><br><span class="line">    NodeType allowed = allowedLocalityLevel.get(priority);</span><br><span class="line">    <span class="keyword">if</span> (allowed != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (allowed.equals(NodeType.OFF_SWITCH) &amp;&amp;</span><br><span class="line">          (type.equals(NodeType.NODE_LOCAL) ||</span><br><span class="line">              type.equals(NodeType.RACK_LOCAL))) &#123;</span><br><span class="line">        <span class="keyword">this</span>.resetAllowedLocalityLevel(priority, type);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (allowed.equals(NodeType.RACK_LOCAL) &amp;&amp;</span><br><span class="line">          type.equals(NodeType.NODE_LOCAL)) &#123;</span><br><span class="line">        <span class="keyword">this</span>.resetAllowedLocalityLevel(priority, type);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Required sanity check - AM can call &#x27;allocate&#x27; to update resource</span></span><br><span class="line">    <span class="comment">// request without locking the scheduler, hence we need to check</span></span><br><span class="line">    <span class="keyword">if</span> (getTotalRequiredResources(priority) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Container container = reservedContainer;</span><br><span class="line">    <span class="keyword">if</span> (container == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// 重点：这里会具体创建一个 Container 实例</span></span><br><span class="line">      container =</span><br><span class="line">          createContainer(node, request.getCapability(), request.getPriority());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用 RMContainer 记录新创建出来的 Container 实例</span></span><br><span class="line">    RMContainer rmContainer = <span class="keyword">new</span> RMContainerImpl(container,</span><br><span class="line">        getApplicationAttemptId(), node.getNodeID(),</span><br><span class="line">        appSchedulingInfo.getUser(), rmContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重点：记录 rmContainer，等待下次 AM 心跳发生时，会从这里把分配出来的 Container 带走</span></span><br><span class="line">    newlyAllocatedContainers.add(rmContainer);</span><br><span class="line">    liveContainers.put(container.getId(), rmContainer);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Update consumption and track allocations</span></span><br><span class="line">    List&lt;ResourceRequest&gt; resourceRequestList = appSchedulingInfo.allocate(</span><br><span class="line">        type, node, priority, request, container);</span><br><span class="line">    Resources.addTo(currentConsumption, container.getResource());</span><br><span class="line">    getQueue().incUsedResource(container.getResource());</span><br><span class="line">    <span class="comment">// Update resource requests related to &quot;request&quot; and store in RMContainer</span></span><br><span class="line">    ((RMContainerImpl) rmContainer).setResourceRequests(resourceRequestList);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里发送 Container 的 START 事件，更新 Container 状态</span></span><br><span class="line">    rmContainer.handle(</span><br><span class="line">        <span class="keyword">new</span> RMContainerEvent(container.getId(), RMContainerEventType.START));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;allocate: applicationAttemptId=&quot;</span></span><br><span class="line">          + container.getId().getApplicationAttemptId()</span><br><span class="line">          + <span class="string">&quot; container=&quot;</span> + container.getId() + <span class="string">&quot; host=&quot;</span></span><br><span class="line">          + container.getNodeId().getHost() + <span class="string">&quot; type=&quot;</span> + type);</span><br><span class="line">    &#125;</span><br><span class="line">    RMAuditLogger.logSuccess(getUser(),</span><br><span class="line">        AuditConstants.ALLOC_CONTAINER, <span class="string">&quot;SchedulerApp&quot;</span>,</span><br><span class="line">        getApplicationId(), container.getId());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rmContainer;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，一个全新的 Container 已经分配出来了，并保存在 RM 的内存数据结构中，那分配出来的 Container 是如何被用到的呢？我们接着后续的逻辑。</p>
<h2 id="3-3-AM-认领资源"><a href="#3-3-AM-认领资源" class="headerlink" title="3.3 AM 认领资源"></a><strong>3.3 AM 认领资源</strong></h2><p>上面知道，分配的 Container 已经保存在 RM 的内存数据结构中了，接下来就是 AM 的心跳上报定时领取给自己分配的资源。</p>
<h3 id="3-3-1-AM-启动并发起资源请求"><a href="#3-3-1-AM-启动并发起资源请求" class="headerlink" title="3.3.1 AM 启动并发起资源请求"></a><strong>3.3.1 AM 启动并发起资源请求</strong></h3><p>作业在启动时，会首先启动 ApplicationMaster 进程，启动入口如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/YARN/applications/distributedshell/ApplicationMaster.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> result = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      ApplicationMaster appMaster = <span class="keyword">new</span> ApplicationMaster();</span><br><span class="line">      LOG.info(<span class="string">&quot;Initializing ApplicationMaster&quot;</span>);</span><br><span class="line">      <span class="keyword">boolean</span> doRun = appMaster.init(args);</span><br><span class="line">      <span class="keyword">if</span> (!doRun) &#123;</span><br><span class="line">        System.exit(<span class="number">0</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// ApplicationMaster 启动 run() 方法</span></span><br><span class="line">      appMaster.run();</span><br><span class="line">      result = appMaster.finish();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">      LOG.fatal(<span class="string">&quot;Error running ApplicationMaster&quot;</span>, t);</span><br><span class="line">      LogManager.shutdown();</span><br><span class="line">      ExitUtil.terminate(<span class="number">1</span>, t);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (result) &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Application Master completed successfully. exiting&quot;</span>);</span><br><span class="line">      System.exit(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Application Master failed. exiting&quot;</span>);</span><br><span class="line">      System.exit(<span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>run() 方法做了什么事呢？主要是进行 ApplicationMaster 的注册和心跳。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/YARN/applications/distributedshell/ApplicationMaster.java</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> YARNException, IOException </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">&quot;Starting ApplicationMaster&quot;</span>);</span><br><span class="line"></span><br><span class="line">    ... <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化 AMRMClient 实例，用于向 RM 发送 RPC 请求，这里采用异步方式，每个 AMRMClient 都是单独的线程</span></span><br><span class="line">    AMRMClientAsync.CallbackHandler allocListener = <span class="keyword">new</span> RMCallbackHandler();</span><br><span class="line">    amRMClient = AMRMClientAsync.createAMRMClientAsync(<span class="number">1000</span>, allocListener);</span><br><span class="line">    amRMClient.init(conf);</span><br><span class="line">    amRMClient.start();</span><br><span class="line"></span><br><span class="line">    containerListener = createNMCallbackHandler();</span><br><span class="line">    nmClientAsync = <span class="keyword">new</span> NMClientAsyncImpl(containerListener);</span><br><span class="line">    nmClientAsync.init(conf);</span><br><span class="line">    nmClientAsync.start();</span><br><span class="line"></span><br><span class="line">    appMasterHostname = NetUtils.getHostname();</span><br><span class="line">    <span class="comment">// 重要：AM 通过 RPC 请求向 RM 注册，心跳线程在注册逻辑里启动</span></span><br><span class="line">    RegisterApplicationMasterResponse response = amRMClient</span><br><span class="line">        .registerApplicationMaster(appMasterHostname, appMasterRpcPort,</span><br><span class="line">            appMasterTrackingUrl);</span><br><span class="line">    ... <span class="comment">// 省略</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>AM 向 RM 发送 RPC 请求是通过 ApplicationMasterService 服务实现的，这里的 AM 注册和心跳都需要通过该服务与 RM 进行通讯。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/YARN/client/api/async/impl/AMRMClientAsyncImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> RegisterApplicationMasterResponse <span class="title">registerApplicationMaster</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      String appHostName, <span class="keyword">int</span> appHostPort, String appTrackingUrl)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> YARNException, IOException </span>&#123;</span><br><span class="line">    RegisterApplicationMasterResponse response = client</span><br><span class="line">        .registerApplicationMaster(appHostName, appHostPort, appTrackingUrl);</span><br><span class="line">    <span class="comment">// 启动 AM 心跳线程</span></span><br><span class="line">    heartbeatThread.start();</span><br><span class="line">    <span class="keyword">return</span> response;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里忽略 registerApplicationMaster() 注册的逻辑，主要是心跳线程做了些什么，即 heartbeatThread 线程的工作。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/YARN/client/api/async/impl/AMRMClientAsyncImpl.java</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        AllocateResponse response = <span class="keyword">null</span>;</span><br><span class="line">        <span class="comment">// synchronization ensures we don&#x27;t send heartbeats after unregistering</span></span><br><span class="line">        <span class="keyword">synchronized</span> (unregisterHeartbeatLock) &#123;</span><br><span class="line">          <span class="keyword">if</span> (!keepRunning) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 重要：AM 通过 AMRMClient 客户端向 RM 发送请求，进行资源的 allocate() 操作</span></span><br><span class="line">            response = client.allocate(progress);</span><br><span class="line">          &#125; <span class="keyword">catch</span> (ApplicationAttemptNotFoundException e) &#123;</span><br><span class="line">            handler.onShutdownRequest();</span><br><span class="line">            LOG.info(<span class="string">&quot;Shutdown requested. Stopping callback.&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Throwable ex) &#123;</span><br><span class="line">            LOG.error(<span class="string">&quot;Exception on heartbeat&quot;</span>, ex);</span><br><span class="line">            savedException = ex;</span><br><span class="line">            <span class="comment">// interrupt handler thread in case it waiting on the queue</span></span><br><span class="line">            handlerThread.interrupt();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (response != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                responseQueue.put(response);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">              &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;</span><br><span class="line">                LOG.debug(<span class="string">&quot;Interrupted while waiting to put on response queue&quot;</span>, ex);</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          Thread.sleep(heartbeatIntervalMs.get());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;</span><br><span class="line">          LOG.debug(<span class="string">&quot;Heartbeater interrupted&quot;</span>, ex);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>至此，AM 已经向 RM 发送资源请求，接下来看看 RM 是如何处理这个 RPC 请求的。</p>
<h3 id="3-3-2-RM-处理-AM-请求"><a href="#3-3-2-RM-处理-AM-请求" class="headerlink" title="3.3.2 RM 处理 AM 请求"></a><strong>3.3.2 RM 处理 AM 请求</strong></h3><p>RM 中负责处理 AM 心跳请求是通过 ApplicationMasterService 服务，其内部的 allocate() 负责处理 AM 的 RPC 资源分配请求，具体逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> AllocateResponse <span class="title">allocate</span><span class="params">(AllocateRequest request)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> YARNException, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    AMRMTokenIdentifier amrmTokenIdentifier = authorizeRequest();</span><br><span class="line"></span><br><span class="line">    ApplicationAttemptId appAttemptId =</span><br><span class="line">        amrmTokenIdentifier.getApplicationAttemptId();</span><br><span class="line">    ApplicationId applicationId = appAttemptId.getApplicationId();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.amLivelinessMonitor.receivedPing(appAttemptId);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 针对每个 appAttempt，会有一个独立的锁对象</span></span><br><span class="line">    AllocateResponseLock lock = responseMap.get(appAttemptId);</span><br><span class="line">    <span class="keyword">if</span> (lock == <span class="keyword">null</span>) &#123;</span><br><span class="line">      String message =</span><br><span class="line">          <span class="string">&quot;Application attempt &quot;</span> + appAttemptId</span><br><span class="line">              + <span class="string">&quot; doesn&#x27;t exist in ApplicationMasterService cache.&quot;</span>;</span><br><span class="line">      LOG.error(message);</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> ApplicationAttemptNotFoundException(message);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">      AllocateResponse lastResponse = lock.getAllocateResponse();</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 省略一些神圣的检查工作</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 重点：AM 资源请求的心跳函数，发送新请求和接收之前的分配都需要进行</span></span><br><span class="line">      Allocation allocation =</span><br><span class="line">          <span class="keyword">this</span>.rScheduler.allocate(appAttemptId, ask, release, </span><br><span class="line">              blacklistAdditions, blacklistRemovals);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 省略一些状态更新操作</span></span><br><span class="line">      </span><br><span class="line">      lock.setAllocateResponse(allocateResponse);</span><br><span class="line">      <span class="keyword">return</span> allocateResponse;</span><br><span class="line">    &#125;    </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>继续跟进 this.rScheduler.allocate() 方法，这里的 scheduler 配置的是 FairScheduler，来看看它的 allocate 方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Allocation <span class="title">allocate</span><span class="params">(ApplicationAttemptId appAttemptId,</span></span></span><br><span class="line"><span class="params"><span class="function">      List&lt;ResourceRequest&gt; ask, List&lt;ContainerId&gt; release,</span></span></span><br><span class="line"><span class="params"><span class="function">      List&lt;String&gt; blacklistAdditions, List&lt;String&gt; blacklistRemovals)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 跳过一些检查工作</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 记录 Container 分配的开始开始时间</span></span><br><span class="line">    application.recordContainerRequestTime(getClock().getTime());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放 AM 认为要释放的 Container</span></span><br><span class="line">    releaseContainers(release, application);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">synchronized</span> (application) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!ask.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">          LOG.debug(<span class="string">&quot;allocate: pre-update&quot;</span> +</span><br><span class="line">              <span class="string">&quot; applicationAttemptId=&quot;</span> + appAttemptId +</span><br><span class="line">              <span class="string">&quot; application=&quot;</span> + application.getApplicationId());</span><br><span class="line">        &#125;</span><br><span class="line">        application.showRequests();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 更新应用的资源请求</span></span><br><span class="line">        application.updateResourceRequests(ask);</span><br><span class="line"></span><br><span class="line">        application.showRequests();</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      Set&lt;ContainerId&gt; preemptionContainerIds =</span><br><span class="line">          application.getPreemptionContainerIds();</span><br><span class="line">      <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">        LOG.debug(</span><br><span class="line">            <span class="string">&quot;allocate: post-update&quot;</span> + <span class="string">&quot; applicationAttemptId=&quot;</span> + appAttemptId</span><br><span class="line">                + <span class="string">&quot; #ask=&quot;</span> + ask.size() + <span class="string">&quot; reservation= &quot;</span> + application</span><br><span class="line">                .getCurrentReservation());</span><br><span class="line"></span><br><span class="line">        LOG.debug(<span class="string">&quot;Preempting &quot;</span> + preemptionContainerIds.size()</span><br><span class="line">            + <span class="string">&quot; container(s)&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (application.isWaitingForAMContainer(application.getApplicationId())) &#123;</span><br><span class="line">        <span class="comment">// Allocate is for AM and update AM blacklist for this</span></span><br><span class="line">        application.updateAMBlacklist(</span><br><span class="line">            blacklistAdditions, blacklistRemovals);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        application.updateBlacklist(blacklistAdditions, blacklistRemovals);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 重要：这里就是 AM 获取最近分配的 Container。这里获取的其实就是前面保存在 RM 内存数据结构的 Container。</span></span><br><span class="line">      ContainersAndNMTokensAllocation allocation =</span><br><span class="line">          application.pullNewlyAllocatedContainersAndNMTokens();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Record container allocation time</span></span><br><span class="line">      <span class="keyword">if</span> (!(allocation.getContainerList().isEmpty())) &#123;</span><br><span class="line">        application.recordContainerAllocationTime(getClock().getTime());</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> Allocation(allocation.getContainerList(),</span><br><span class="line">        application.getHeadroom(), preemptionContainerIds, <span class="keyword">null</span>, <span class="keyword">null</span>,</span><br><span class="line">        allocation.getNMTokenList());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，AM 已经顺利拿到 RM 分配的 Container，整理 FairScheduler 资源分配流程基本就是这样。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://tech.meituan.com/2019/08/01/hadoop-YARN-scheduling-performance-optimization-practice.html">(美团)Hadoop YARN: 调度性能优化实践</a></li>
<li><a href="https://yoelee.github.io/2018/06/26/YARN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%905-%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/">YARN源码分析-资源调度</a></li>
<li><a href="https://blog.csdn.net/zhanyuanlin/article/details/78799131#">YARN资源请求处理和资源分配原理解析</a></li>
<li><a href="https://blog.csdn.net/iie_libi/article/details/71597753">YARN资源调度策略</a></li>
</ul>
]]></content>
      <categories>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>YARN</tag>
        <tag>YARN源码</tag>
      </tags>
  </entry>
  <entry>
    <title>YARN RM与ZK交互源码分析及优化</title>
    <url>/2021/11/16/YARN-RM%E4%B8%8EZK%E4%BA%A4%E4%BA%92%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8F%8A%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<p>线上集群出现过几次 YARN RM 写 ZK ZNode 的数据量超过 ZNode 限制，导致 RM 服务均进入 Standby 状态，用户无法正常提交任务，整个集群 hang 住，后续排查发现主要是异常任务写 ZNode 数据量太大，超过 ZNode 限制，导致集群其他提交作业的状态信息无法正常写入 ZNode，为避免类似问题再次出现，我们对 RM 写 ZNode 逻辑进行了优化，规避异常任务对整个集群造成的雪崩效应。</p>
<h1 id="1-问题复现"><a href="#1-问题复现" class="headerlink" title="1. 问题复现"></a>1. 问题复现</h1><p>最直接方式是修改 ZK 的 Jute 最大缓冲区为 512 B，重启 ZK 和 Yarn 服务，此时 ZK 和 RM 服务均出现异常，ZK 异常信息表现为数据 java.io.IOException: Len error 614 客户端写入数据超过 512B 无法正常写入 ZK，RM 表现为 ”code:CONNECTIONLOSS“，无法连接到 ZK，两个 RM 均处于 Standy 状态，此时集群处于不可用状态。</p>
<p><strong>leader ZK 异常信息：</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">2020-12-07 16:00:11,869 INFO org.apache.zookeeper.server.ZooKeeperServer: Client attempting to renew session 0x1763c3707800002 at /10.197.1.96:32892</span><br><span class="line">2020-12-07 16:00:11,869 INFO org.apache.zookeeper.server.ZooKeeperServer: Established session 0x1763c3707800002 with negotiated timeout 40000 for client /10.197.1.96:32892</span><br><span class="line">2020-12-07 16:00:11,870 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x1763c3707800002 due to java.io.IOException: Len error 614</span><br><span class="line">2020-12-07 16:00:11,870 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /10.197.1.96:32892 which had sessionid 0x1763c3707800002</span><br><span class="line">2020-12-07 16:00:12,216 INFO org.apache.zookeeper.server.NIOServerCnxnFactory: Accepted socket connection from /10.197.1.141:56492</span><br><span class="line">2020-12-07 16:00:12,216 INFO org.apache.zookeeper.server.ZooKeeperServer: Client attempting to establish new session at /10.197.1.141:56492</span><br><span class="line">2020-12-07 16:00:12,218 INFO org.apache.zookeeper.server.ZooKeeperServer: Established session 0x3763c3707830001 with negotiated timeout 40000 for client /10.197.1.141:56492</span><br><span class="line">2020-12-07 16:00:12,219 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x3763c3707830001 due to java.io.IOException: Len error 614</span><br><span class="line">2020-12-07 16:00:12,220 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /10.197.1.141:56492 which had sessionid 0x3763c3707830001</span><br><span class="line">2020-12-07 16:00:14,275 INFO org.apache.zookeeper.server.NIOServerCnxnFactory: Accepted socket connection from /10.197.1.141:56510</span><br><span class="line">2020-12-07 16:00:14,275 INFO org.apache.zookeeper.server.ZooKeeperServer: Client attempting to renew session 0x3763c3707830001 at /10.197.1.141:56510</span><br><span class="line">2020-12-07 16:00:14,276 INFO org.apache.zookeeper.server.ZooKeeperServer: Established session 0x3763c3707830001 with negotiated timeout 40000 for client /10.197.1.141:56510</span><br><span class="line">2020-12-07 16:00:14,276 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x3763c3707830001 due to java.io.IOException: Len error 614</span><br><span class="line">2020-12-07 16:00:14,276 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /10.197.1.141:56510 which had sessionid 0x3763c3707830001</span><br><span class="line">2020-12-07 16:00:16,000 INFO org.apache.zookeeper.server.ZooKeeperServer: Expiring session 0x1763c3707800000, timeout of 5000ms exceeded</span><br></pre></td></tr></table></figure>

<p><strong>YARN RM 日志：</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">2020-12-07 16:00:10,938 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.</span><br><span class="line">2020-12-07 16:00:10,938 INFO org.apache.hadoop.ha.ActiveStandbyElector: Ignore duplicate monitor lock-node request.</span><br><span class="line">2020-12-07 16:00:11,038 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...</span><br><span class="line">2020-12-07 16:00:11,647 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server slave-prd-10-197-1-236.v-bj-5.kwang.lan/10.197.1.236:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">2020-12-07 16:00:11,647 INFO org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /10.197.1.141:56854, server: slave-prd-10-197-1-236.v-bj-5.kwang.lan/10.197.1.236:2181</span><br><span class="line">2020-12-07 16:00:11,649 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server slave-prd-10-197-1-236.v-bj-5.kwang.lan/10.197.1.236:2181, sessionid = 0x1763c3707800001, negotiated timeout = 40000</span><br><span class="line">2020-12-07 16:00:11,649 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.</span><br><span class="line">2020-12-07 16:00:11,650 INFO org.apache.hadoop.ha.ActiveStandbyElector: Ignore duplicate monitor lock-node request.</span><br><span class="line">2020-12-07 16:00:11,650 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1763c3707800001, likely server has closed socket, closing socket connection and attempting reconnect</span><br><span class="line">2020-12-07 16:00:11,750 FATAL org.apache.hadoop.ha.ActiveStandbyElector: Received create error from Zookeeper. code:CONNECTIONLOSS for path /yarn-leader-election/yarnRM/ActiveStandbyElectorLock. Not retrying further znode create connection errors.</span><br><span class="line">2020-12-07 16:00:12,210 INFO org.apache.zookeeper.ZooKeeper: Session: 0x1763c3707800001 closed</span><br><span class="line">2020-12-07 16:00:12,212 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x1763c3707800001</span><br><span class="line">2020-12-07 16:00:12,212 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x1763c3707800001</span><br><span class="line">2020-12-07 16:00:12,212 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down</span><br><span class="line">2020-12-07 16:00:12,213 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received RMFatalEvent of type EMBEDDED<span class="built_in">_</span>ELECTOR<span class="built_in">_</span>FAILED, caused by Received create error from Zookeeper. code:CONNECTIONLOSS for path /yarn-leader-election/yarnRM/ActiveStandbyElectorLock. Not retrying further znode create connection errors.</span><br><span class="line">2020-12-07 16:00:12,213 WARN org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning the resource manager to standby.</span><br><span class="line">2020-12-07 16:00:12,214 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning RM to Standby mode</span><br><span class="line">2020-12-07 16:00:12,214 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Already in standby state</span><br><span class="line">2020-12-07 16:00:12,214 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from electionÏ</span><br><span class="line">2020-12-07 16:00:12,214 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=slave-prd-10-197-1-236.v-bj-5.kwang.lan:2181,slave-prd-10-197-1-96.v-bj-5.kwang.lan:2181,slave-prd-10-197-1-141.v-bj-5.kwang.lan:2181 sessionTimeout=60000 watcher=org.apache.hadoop.ha.ActiveStandbyElector<span class="built_in">$</span>WatcherWithClientRef@67b6359c</span><br><span class="line">2020-12-07 16:00:12,215 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server slave-prd-10-197-1-141.v-bj-5.kwang.lan/10.197.1.141:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">2020-12-07 16:00:12,216 INFO org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /10.197.1.141:56492, server: slave-prd-10-197-1-141.v-bj-5.kwang.lan/10.197.1.141:2181</span><br><span class="line">2020-12-07 16:00:12,218 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server slave-prd-10-197-1-141.v-bj-5.kwang.lan/10.197.1.141:2181, sessionid = 0x3763c3707830001, negotiated timeout = 40000</span><br><span class="line">2020-12-07 16:00:12,219 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.</span><br><span class="line">2020-12-07 16:00:12,220 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x3763c3707830001, likely server has closed socket, closing socket connection and attempting reconnect</span><br><span class="line">2020-12-07 16:00:12,320 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...</span><br><span class="line">2020-12-07 16:00:12,320 WARN org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService: Lost contact with Zookeeper. Transitioning to standby in 60000 ms if connection is not reestablished.</span><br></pre></td></tr></table></figure>

<h1 id="2-RM-与-ZNode-交互原理"><a href="#2-RM-与-ZNode-交互原理" class="headerlink" title="2. RM 与 ZNode 交互原理"></a>2. RM 与 ZNode 交互原理</h1><h2 id="2-1-RM-状态在-ZK-中的存储"><a href="#2-1-RM-状态在-ZK-中的存储" class="headerlink" title="2.1 RM 状态在 ZK 中的存储"></a>2.1 RM 状态在 ZK 中的存储</h2><p>不管 RM 是否启用了高可用，RM 作为 Yarn 的核心服务组件，不仅要与各个节点上的 ApplicationMaster 进行通信，还要与 NodeManager 进行心跳包的传输，自然在 RM 上会注册进来很多应用，每个应用由一个 ApplicationMaster 负责掌管整个应用周期，既然 RM 角色如此重要，就有必要保存一下 RM 的信息状态，以免 RM 进程异常退出后导致应用状态信息全部丢失，RM 重启无法重跑之前的任务。</p>
<p>既然应用状态信息要保存的目标易经明确了，那保存方式和保存的数据信息是什么呢。</p>
<p>在 Yarn 中 RM 应用状态信息保存的方式有四种：</p>
<ul>
<li><p>MemoryRMStateStore——信息状态保存在内存中的实现类。</p>
</li>
<li><p>FileSystemRMStateStore——信息状态保存在 HDFS 文件系统中，这个是做了持久化的。</p>
</li>
<li><p>NullRMStateStore——什么都不做，就是不保存应用状态信息。</p>
</li>
<li><p>ZKRMStateStore——信息状态保存在 Zookeeper 中。</p>
</li>
</ul>
<p>由于 YARN 启用了 RM HA，以上四种方式只能支持 ZKRMStateStore。</p>
<p>那 RM 在 ZK 中到底是存储了哪些信息状态呢？如下所示，是 ZK 中存储 RM 信息状态的目录格式，可以看出，ZK 中主要存储 Application（作业的状态信息）和 SECRET_MANAGER（作业的 TOKEN 信息）等。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">ROOT<span class="built_in">_</span>DIR<span class="built_in">_</span>PATH</span><br><span class="line">  |--- VERSION<span class="built_in">_</span>INFO</span><br><span class="line">  |--- EPOCH<span class="built_in">_</span>NODE</span><br><span class="line">  |--- RM<span class="built_in">_</span>ZK<span class="built_in">_</span>FENCING<span class="built_in">_</span>LOCK</span><br><span class="line">  |--- RM<span class="built_in">_</span>APP<span class="built_in">_</span>ROOT</span><br><span class="line">  |     |----- (<span class="params">#</span>ApplicationId1)</span><br><span class="line">  |     |        |----- (<span class="params">#</span>ApplicationAttemptIds)</span><br><span class="line">  |     |</span><br><span class="line">  |     |----- (<span class="params">#</span>ApplicationId2)</span><br><span class="line">  |     |       |----- (<span class="params">#</span>ApplicationAttemptIds)</span><br><span class="line">  |     ....</span><br><span class="line">  |</span><br><span class="line">  |--- RM<span class="built_in">_</span>DT<span class="built_in">_</span>SECRET<span class="built_in">_</span>MANAGER<span class="built_in">_</span>ROOT</span><br><span class="line">  |----- RM<span class="built_in">_</span>DT<span class="built_in">_</span>SEQUENTIAL<span class="built_in">_</span>NUMBER<span class="built_in">_</span>ZNODE<span class="built_in">_</span>NAME</span><br><span class="line">  |----- RM<span class="built_in">_</span>DELEGATION<span class="built_in">_</span>TOKENS<span class="built_in">_</span>ROOT<span class="built_in">_</span>ZNODE<span class="built_in">_</span>NAME</span><br><span class="line">  |       |----- Token<span class="built_in">_</span>1</span><br><span class="line">  |       |----- Token<span class="built_in">_</span>2</span><br><span class="line">  |       ....</span><br><span class="line">  |</span><br><span class="line">  |----- RM<span class="built_in">_</span>DT<span class="built_in">_</span>MASTER<span class="built_in">_</span>KEYS<span class="built_in">_</span>ROOT<span class="built_in">_</span>ZNODE<span class="built_in">_</span>NAME</span><br><span class="line">  |      |----- Key<span class="built_in">_</span>1</span><br><span class="line">  |      |----- Key<span class="built_in">_</span>2</span><br><span class="line">  ....</span><br><span class="line">  |--- AMRMTOKEN<span class="built_in">_</span>SECRET<span class="built_in">_</span>MANAGER<span class="built_in">_</span>ROOT</span><br><span class="line">  |----- currentMasterKey</span><br><span class="line">  |----- nextMasterKey</span><br></pre></td></tr></table></figure>

<h2 id="2-2-ZK-存储-amp-更新-RM-信息状态逻辑"><a href="#2-2-ZK-存储-amp-更新-RM-信息状态逻辑" class="headerlink" title="2.2 ZK 存储&amp;更新 RM 信息状态逻辑"></a>2.2 ZK 存储&amp;更新 RM 信息状态逻辑</h2><p>作业提交到 YARN 上的入口，都是通过 YarnClient 这个接口 api 提交的，具体提交方法为 submitApplication()。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/YarnClient.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> ApplicationId <span class="title">submitApplication</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationSubmissionContext appContext)</span> <span class="keyword">throws</span> YarnException,</span></span><br><span class="line"><span class="function">      IOException</span>;</span><br></pre></td></tr></table></figure>



<p>作业提交后，会经过一些列的事件转换，请求到不同的状态机进行处理，而保存作业的状态机 StoreAppTransition 会对 APP 的状态进行保存，将其元数据存储到 ZK 中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">storeNewApplication</span><span class="params">(RMApp app)</span> </span>&#123;</span><br><span class="line">    ApplicationSubmissionContext context = app</span><br><span class="line">                                            .getApplicationSubmissionContext();</span><br><span class="line">    <span class="keyword">assert</span> context <span class="keyword">instanceof</span> ApplicationSubmissionContextPBImpl;</span><br><span class="line">    ApplicationStateData appState =</span><br><span class="line">        ApplicationStateData.newInstance(</span><br><span class="line">            app.getSubmitTime(), app.getStartTime(), context, app.getUser());</span><br><span class="line">    <span class="comment">// 向调度器发送 RMStateStoreEventType.STORE_APP 事件</span></span><br><span class="line">    dispatcher.getEventHandler().handle(<span class="keyword">new</span> RMStateStoreAppEvent(appState));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>这里向调度器发送 RMStateStoreEventType.STORE_APP 事件，并注册了 StoreAppTransition 状态机。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java</span></span><br><span class="line">    .addTransition(RMStateStoreState.ACTIVE,</span><br><span class="line">          EnumSet.of(RMStateStoreState.ACTIVE, RMStateStoreState.FENCED),</span><br><span class="line">          RMStateStoreEventType.STORE_APP, <span class="keyword">new</span> StoreAppTransition())</span><br></pre></td></tr></table></figure>



<p>StoreAppTransition 状态机最终会调用 ZKRMStateStore#storeApplicationStateInternal() 方法，对 RM 的元数据在 ZK 中进行保存。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java </span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">storeApplicationStateInternal</span><span class="params">(ApplicationId appId,</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationStateData appStateDataPB)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    String nodeCreatePath = getNodePath(rmAppRoot, appId.toString());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Storing info for app: &quot;</span> + appId + <span class="string">&quot; at: &quot;</span> + nodeCreatePath);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">byte</span>[] appStateData = appStateDataPB.getProto().toByteArray();</span><br><span class="line">	createWithRetries(nodeCreatePath, appStateData, zkAcl,</span><br><span class="line">              CreateMode.PERSISTENT);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>RM Application 的状态保存到 ZK 后，APP 状态最终会转化为 ACCETPED 状态 ，此时，会触发 StartAppAttemptTransition 状态机，对 AppAttemp 状态进行保存。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java </span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">storeApplicationAttemptStateInternal</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationAttemptId appAttemptId,</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationAttemptStateData attemptStateDataPB)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    String appDirPath = getNodePath(rmAppRoot,</span><br><span class="line">        appAttemptId.getApplicationId().toString());</span><br><span class="line">    String nodeCreatePath = getNodePath(appDirPath, appAttemptId.toString());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Storing info for attempt: &quot;</span> + appAttemptId + <span class="string">&quot; at: &quot;</span></span><br><span class="line">          + nodeCreatePath);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">byte</span>[] attemptStateData = attemptStateDataPB.getProto().toByteArray();</span><br><span class="line">	createWithRetries(nodeCreatePath, attemptStateData, zkAcl,</span><br><span class="line">					CreateMode.PERSISTENT);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>而在任务运行结束时，会对 Application 和 AppAttemp 的状态进行更新。而更新操作也是容易出现异常的地方，这两段代码主要是执行更新或添加任务重试状态信息到 ZK 中的操作，YARN 在调度任务的过程中，可能会对任务进行多次重试，主要受网络、硬件、资源等因素影响，如果任务重试信息保存在 ZK 失败，会调用 ZKRMStateStore.ZKAction 类的 runWithRetries() 方法重试。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java </span></span><br><span class="line">  <span class="comment">// 对 Application 状态进行更新</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">updateApplicationStateInternal</span><span class="params">(ApplicationId appId,</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationStateData appStateDataPB)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    String nodeUpdatePath = getNodePath(rmAppRoot, appId.toString());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Storing final state info for app: &quot;</span> + appId + <span class="string">&quot; at: &quot;</span></span><br><span class="line">          + nodeUpdatePath);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">byte</span>[] appStateData = appStateDataPB.getProto().toByteArray();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (existsWithRetries(nodeUpdatePath, <span class="keyword">false</span>) != <span class="keyword">null</span>) &#123;</span><br><span class="line">      setDataWithRetries(nodeUpdatePath, appStateData, -<span class="number">1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// retry 是会调用到 ZKRMStateStore.ZKAction#runWithRetries() 方法</span></span><br><span class="line">      createWithRetries(nodeUpdatePath, appStateData, zkAcl,</span><br><span class="line">              CreateMode.PERSISTENT);</span><br><span class="line">      LOG.debug(appId + <span class="string">&quot; znode didn&#x27;t exist. Created a new znode to&quot;</span></span><br><span class="line">              + <span class="string">&quot; update the application state.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对 AppAttemp 状态进行更新</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">updateApplicationAttemptStateInternal</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationAttemptId appAttemptId,</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationAttemptStateData attemptStateDataPB)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    String appIdStr = appAttemptId.getApplicationId().toString();</span><br><span class="line">    String appAttemptIdStr = appAttemptId.toString();</span><br><span class="line">    String appDirPath = getNodePath(rmAppRoot, appIdStr);</span><br><span class="line">    String nodeUpdatePath = getNodePath(appDirPath, appAttemptIdStr);</span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Storing final state info for attempt: &quot;</span> + appAttemptIdStr</span><br><span class="line">          + <span class="string">&quot; at: &quot;</span> + nodeUpdatePath);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">byte</span>[] attemptStateData = attemptStateDataPB.getProto().toByteArray();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (existsWithRetries(nodeUpdatePath, <span class="keyword">false</span>) != <span class="keyword">null</span>) &#123;</span><br><span class="line">      setDataWithRetries(nodeUpdatePath, attemptStateData, -<span class="number">1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      createWithRetries(nodeUpdatePath, attemptStateData, zkAcl,</span><br><span class="line">              CreateMode.PERSISTENT);</span><br><span class="line">      LOG.debug(appAttemptId + <span class="string">&quot; znode didn&#x27;t exist. Created a new znode to&quot;</span></span><br><span class="line">              + <span class="string">&quot; update the application attempt state.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>在启用 YARN 高可用情况下，重试间隔机制如下：受 yarn.resourcemanager.zk-timeout-ms（ZK会话超时时间，线上 1 分钟，即 60000ms）和 yarn.resourcemanager.zk-num-retries（操作失败后重试次数，线上环境 1000次）参数控制，计算公式为：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">重试时间间隔（yarn.resourcemanager.zk-retry-interval-ms ）=yarn.resourcemanager.zk-timeout-ms(ZK session超时时间)/yarn.resourcemanager.zk-num-retries（重试次数）</span><br></pre></td></tr></table></figure>

<p>即在生产环境中，重试时间间隔 = 600000ms /1000次 = 60 ms/次，即线上环境在任务不成功的条件下，会重试 1000 次，每次 60 ms，这里也可能会导致 RM 堆内存溢出。参考资料：<a href="https://my.oschina.net/dabird/blog/3089265%E3%80%82">https://my.oschina.net/dabird/blog/3089265。</a></p>
<p>重试间隔确定代码如下，在 ZKRMStateStore 类 serviceInit() 初始化中被设置。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java  </span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">initInternal</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    zkHostPort = conf.get(YarnConfiguration.RM_ZK_ADDRESS);</span><br><span class="line">    <span class="keyword">if</span> (zkHostPort == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(<span class="string">&quot;No server address specified for &quot;</span> +</span><br><span class="line">          <span class="string">&quot;zookeeper state store for Resource Manager recovery. &quot;</span> +</span><br><span class="line">          YarnConfiguration.RM_ZK_ADDRESS + <span class="string">&quot; is not configured.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ZK 连接重试次数</span></span><br><span class="line">    numRetries =</span><br><span class="line">        conf.getInt(YarnConfiguration.RM_ZK_NUM_RETRIES,</span><br><span class="line">            YarnConfiguration.DEFAULT_ZK_RM_NUM_RETRIES);</span><br><span class="line">    znodeWorkingPath =</span><br><span class="line">        conf.get(YarnConfiguration.ZK_RM_STATE_STORE_PARENT_PATH,</span><br><span class="line">            YarnConfiguration.DEFAULT_ZK_RM_STATE_STORE_PARENT_PATH);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ZK session 超时时间</span></span><br><span class="line">    zkSessionTimeout =</span><br><span class="line">        conf.getInt(YarnConfiguration.RM_ZK_TIMEOUT_MS,</span><br><span class="line">            YarnConfiguration.DEFAULT_RM_ZK_TIMEOUT_MS);</span><br><span class="line">    zknodeLimit =</span><br><span class="line">        conf.getInt(YarnConfiguration.RM_ZK_ZNODE_SIZE_LIMIT_BYTES,</span><br><span class="line">            YarnConfiguration.DEFAULT_RM_ZK_ZNODE_SIZE_LIMIT_BYTES);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (HAUtil.isHAEnabled(conf)) &#123;</span><br><span class="line">      zkRetryInterval = zkSessionTimeout / numRetries;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      zkRetryInterval =</span><br><span class="line">          conf.getLong(YarnConfiguration.RM_ZK_RETRY_INTERVAL_MS,</span><br><span class="line">              YarnConfiguration.DEFAULT_RM_ZK_RETRY_INTERVAL_MS);</span><br><span class="line">    &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>至此，我们已经清楚了 RM 中作业的信息状态是如何保存在 ZK 中并如何进行更新的。</p>
<h2 id="2-3-ZK-删除-RM-信息状态逻辑"><a href="#2-3-ZK-删除-RM-信息状态逻辑" class="headerlink" title="2.3 ZK 删除 RM 信息状态逻辑"></a>2.3 ZK 删除 RM 信息状态逻辑</h2><p>在了解了 RM 作业信息状态保存在 ZK 的逻辑后，我们便会产生一个疑问，那 RM 状态保存在 ZK 中后，是否会一直驻留在 ZK 中呢？答案是否定的，ZK 也会对作业的状态进行删除，那删除逻辑是这样的呢？</p>
<p>删除的核心逻辑位于 RMAppManager#checkAppNumCompletedLimit() 方法中调用的 removeApplication() 方法，其逻辑就是判断保存在 ZK StateStore 中或已完成的作业数量超过对应限制，则对 App 状态信息进行删除。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java</span></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * check to see if hit the limit for max # completed apps kept</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">checkAppNumCompletedLimit</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// check apps kept in state store.</span></span><br><span class="line">    <span class="keyword">while</span> (completedAppsInStateStore &gt; <span class="keyword">this</span>.maxCompletedAppsInStateStore) &#123;</span><br><span class="line">      ApplicationId removeId =</span><br><span class="line">          completedApps.get(completedApps.size() - completedAppsInStateStore);</span><br><span class="line">      RMApp removeApp = rmContext.getRMApps().get(removeId);</span><br><span class="line">      LOG.info(<span class="string">&quot;Max number of completed apps kept in state store met:&quot;</span></span><br><span class="line">          + <span class="string">&quot; maxCompletedAppsInStateStore = &quot;</span> + maxCompletedAppsInStateStore</span><br><span class="line">          + <span class="string">&quot;, removing app &quot;</span> + removeApp.getApplicationId()</span><br><span class="line">          + <span class="string">&quot; from state store.&quot;</span>);</span><br><span class="line">      rmContext.getStateStore().removeApplication(removeApp);</span><br><span class="line">      completedAppsInStateStore--;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check apps kept in memorty.</span></span><br><span class="line">    <span class="keyword">while</span> (completedApps.size() &gt; <span class="keyword">this</span>.maxCompletedAppsInMemory) &#123;</span><br><span class="line">      ApplicationId removeId = completedApps.remove();</span><br><span class="line">      LOG.info(<span class="string">&quot;Application should be expired, max number of completed apps&quot;</span></span><br><span class="line">          + <span class="string">&quot; kept in memory met: maxCompletedAppsInMemory = &quot;</span></span><br><span class="line">          + <span class="keyword">this</span>.maxCompletedAppsInMemory + <span class="string">&quot;, removing app &quot;</span> + removeId</span><br><span class="line">          + <span class="string">&quot; from memory: &quot;</span>);</span><br><span class="line">      rmContext.getRMApps().remove(removeId);</span><br><span class="line">      <span class="keyword">this</span>.applicationACLsManager.removeApplication(removeId);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>可以看看相关参数是如何设置的，其中保存在 ZK StateStore 中和保存在 Memory 的 App 最大数量是一致的，默认是 10000（线上环境默认也是 10000），且保存在 ZK StateSotre 中的作业数量不能超过保存在 Memory 中的作业数量。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">RMAppManager</span><span class="params">(RMContext context,</span></span></span><br><span class="line"><span class="params"><span class="function">      YarnScheduler scheduler, ApplicationMasterService masterService,</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationACLsManager applicationACLsManager, Configuration conf)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 保存在 Memory 中的 App 最大数量</span></span><br><span class="line">    <span class="keyword">this</span>.maxCompletedAppsInMemory = conf.getInt(</span><br><span class="line">        YarnConfiguration.RM_MAX_COMPLETED_APPLICATIONS,</span><br><span class="line">        YarnConfiguration.DEFAULT_RM_MAX_COMPLETED_APPLICATIONS);</span><br><span class="line">    <span class="comment">// 保存在 ZK StateStore 中的 App 最大数量，默认和 Memory 中的最大值保存一致</span></span><br><span class="line">    <span class="keyword">this</span>.maxCompletedAppsInStateStore =</span><br><span class="line">        conf.getInt(</span><br><span class="line">          YarnConfiguration.RM_STATE_STORE_MAX_COMPLETED_APPLICATIONS,</span><br><span class="line">          YarnConfiguration.DEFAULT_RM_STATE_STORE_MAX_COMPLETED_APPLICATIONS);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 保存在 ZK StateStore 中的 App 数量不能超过保存在 Memory 中的 App 数量</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.maxCompletedAppsInStateStore &gt; <span class="keyword">this</span>.maxCompletedAppsInMemory) &#123;</span><br><span class="line">      <span class="keyword">this</span>.maxCompletedAppsInStateStore = <span class="keyword">this</span>.maxCompletedAppsInMemory;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/conf/YarnConfiguration.java</span></span><br><span class="line">  <span class="comment">// maxCompletedAppsInMemory 参数定义</span></span><br><span class="line">  <span class="comment">/** The maximum number of completed applications RM keeps. */</span> </span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String RM_MAX_COMPLETED_APPLICATIONS =</span><br><span class="line">    RM_PREFIX + <span class="string">&quot;max-completed-applications&quot;</span>;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_RM_MAX_COMPLETED_APPLICATIONS = <span class="number">10000</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// maxCompletedAppsInStateStore 参数定义，默认和 maxCompletedAppsInMemory 保持一致</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The maximum number of completed applications RM state store keeps, by</span></span><br><span class="line"><span class="comment">   * default equals to DEFAULT_RM_MAX_COMPLETED_APPLICATIONS</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String RM_STATE_STORE_MAX_COMPLETED_APPLICATIONS =</span><br><span class="line">      RM_PREFIX + <span class="string">&quot;state-store.max-completed-applications&quot;</span>;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_RM_STATE_STORE_MAX_COMPLETED_APPLICATIONS =</span><br><span class="line">      DEFAULT_RM_MAX_COMPLETED_APPLICATIONS;</span><br></pre></td></tr></table></figure>



<p>执行真正的删除操作，删除在 ZK 中保存的超出限制的 App 状态信息。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java </span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">removeApplicationStateInternal</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationStateData  appState)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    String appId = appState.getApplicationSubmissionContext().getApplicationId()</span><br><span class="line">        .toString();</span><br><span class="line">    String appIdRemovePath = getNodePath(rmAppRoot, appId);</span><br><span class="line">    ArrayList&lt;Op&gt; opList = <span class="keyword">new</span> ArrayList&lt;Op&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 删除在 ZK 中保存的 AppAttempt 信息</span></span><br><span class="line">    <span class="keyword">for</span> (ApplicationAttemptId attemptId : appState.attempts.keySet()) &#123;</span><br><span class="line">      String attemptRemovePath = getNodePath(appIdRemovePath, attemptId.toString());</span><br><span class="line">      opList.add(Op.delete(attemptRemovePath, -<span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    opList.add(Op.delete(appIdRemovePath, -<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Removing info for app: &quot;</span> + appId + <span class="string">&quot; at: &quot;</span> + appIdRemovePath</span><br><span class="line">          + <span class="string">&quot; and its attempts.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 删除在 ZK 中保存的 Applicaton 信息</span></span><br><span class="line">    doDeleteMultiWithRetries(opList);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="3-技术方案调研及优化"><a href="#3-技术方案调研及优化" class="headerlink" title="3. 技术方案调研及优化"></a>3. 技术方案调研及优化</h1><h2 id="3-1-Hadoop-2-9-0之前修复方法"><a href="#3-1-Hadoop-2-9-0之前修复方法" class="headerlink" title="3.1 Hadoop 2.9.0之前修复方法"></a>3.1 Hadoop 2.9.0之前修复方法</h2><p>RM 状态在 ZK 存储的过程中，RM 作为客户端，ZK 作为服务端，在 Hadoop 2.9.0 版本之前，出现这种异常的处理方式为修改 ZK 端 <code>jute.maxbuffer</code> 参数的值，以增加 RM 作业允许写 ZK 的最大值。但这种处理方式有三种不足：</p>
<ol>
<li><p>ZK 服务端允许写入的 ZNode 数据量太大，会影响 ZK 服务的读写性能和 ZK 内存紧张；</p>
</li>
<li><p>需要重启 ZK 服务端和客户端 RM 服务，运维成本较高。（如果有其他服务依赖此 ZK 则成本更高，可能还需要重启其他服务）</p>
</li>
<li><p>异常任务写 ZNode 数据量不可控，某些情况下还是会发生写入 ZNode 大小超过限制。</p>
</li>
</ol>
<p><strong>Q：为什么要限制 ZK 中 ZNode 大小？</strong></p>
<p>A：ZK 是一套高吞吐量的系统，为了提高系统的读取速度，ZK不允许从文件中读取需要的数据，而是直接从内存中查找。换句话说，ZK 集群中每一台服务器都包含全量的数据，并且这些数据都会加载到内存中，同时 ZNode 的数据不支持 Append 操作，全部都是 Replace 操作。如果 ZNode 数据量过大，那么读写 ZNode 将造成不确定的延时（比如服务端同步数据慢），同时 ZNode 太大会消耗 ZK 服务器的内存，这也是为什么 ZK 不适合存储大量数据的原因。</p>
<h2 id="3-2-Hadoop-2-9-0及后续版本修复方法"><a href="#3-2-Hadoop-2-9-0及后续版本修复方法" class="headerlink" title="3.2 Hadoop 2.9.0及后续版本修复方法"></a>3.2 Hadoop 2.9.0及后续版本修复方法</h2><p>在 Hadoop 2.9.0 及后续版本中，yarn-site.xml 中增加了 <code>yarn.resourcemanager.zk-max-znode-size.bytes</code> 参数，该参数定义了 ZK 的 ZNode 节点所能存储的最大数据量，以字节为单位，默认是 1024*1024 字节，也就是 1MB。使用这种方式，我们就不需要修改 ZK 的服务端的配置，而只需修改 Yarn 服务端的配置并重启 RM 服务，就能限制 RM 往 ZK 中写入的数据量，而且也提高了 ZK 服务的可用性。</p>
<h1 id="4-优化及测试"><a href="#4-优化及测试" class="headerlink" title="4. 优化及测试"></a>4. 优化及测试</h1><h2 id="4-1-优化方案"><a href="#4-1-优化方案" class="headerlink" title="4.1 优化方案"></a>4.1 优化方案</h2><p>优化思路主要是采用 Hadoop 2.9.0 之后的方式，核心主要是在 ZKRMStateStore 类中的 <code>storeApplicationStateInternal()</code>、<code>updateApplicationStateInternal()</code>、<code>storeApplicationAttemptStateInternal()</code>、<code>updateApplicationAttemptStateInternal()</code> 方法逻辑中增加了是否超过写 ZNode 大小限制的判断，避免单个作业写 ZNode 数据量过大导致 RM 和 ZK 服务的不可用。部分代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java </span></span><br><span class="line">  <span class="comment">// Application 写 ZNode 时判断大小限制</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">storeApplicationStateInternal</span><span class="params">(ApplicationId appId,</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationStateData appStateDataPB)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    String nodeCreatePath = getNodePath(rmAppRoot, appId.toString());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Storing info for app: &quot;</span> + appId + <span class="string">&quot; at: &quot;</span> + nodeCreatePath);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">byte</span>[] appStateData = appStateDataPB.getProto().toByteArray();</span><br><span class="line">    <span class="keyword">if</span> (appStateData.length &lt;= zknodeLimit) &#123;</span><br><span class="line">      createWithRetries(nodeCreatePath, appStateData, zkAcl,</span><br><span class="line">              CreateMode.PERSISTENT);</span><br><span class="line">      LOG.debug(<span class="string">&quot;Store application state data size for &quot;</span> + appId + <span class="string">&quot; is &quot;</span> + appStateData.length);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Store application state data size for &quot;</span> + appId + <span class="string">&quot; is &quot;</span> + appStateData.length +</span><br><span class="line">        <span class="string">&quot;. exceeds the maximum allowed size &quot;</span> + zknodeLimit + <span class="string">&quot; for application data.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Application 状态更新时判断写 ZNode 大小</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">updateApplicationStateInternal</span><span class="params">(ApplicationId appId,</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationStateData appStateDataPB)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    String nodeUpdatePath = getNodePath(rmAppRoot, appId.toString());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Storing final state info for app: &quot;</span> + appId + <span class="string">&quot; at: &quot;</span></span><br><span class="line">          + nodeUpdatePath);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">byte</span>[] appStateData = appStateDataPB.getProto().toByteArray();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (appStateData.length &lt;= zknodeLimit) &#123;</span><br><span class="line">      <span class="keyword">if</span> (existsWithRetries(nodeUpdatePath, <span class="keyword">false</span>) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        setDataWithRetries(nodeUpdatePath, appStateData, -<span class="number">1</span>);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        createWithRetries(nodeUpdatePath, appStateData, zkAcl,</span><br><span class="line">                CreateMode.PERSISTENT);</span><br><span class="line">        LOG.debug(appId + <span class="string">&quot; znode didn&#x27;t exist. Created a new znode to&quot;</span></span><br><span class="line">                + <span class="string">&quot; update the application state.&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Update application state data size for &quot;</span> + appId + <span class="string">&quot; is &quot;</span> + appStateData.length);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Update application state data size for &quot;</span> + appId + <span class="string">&quot; is &quot;</span> + appStateData.length +</span><br><span class="line">              <span class="string">&quot;. exceeds the maximum allowed size &quot;</span> + zknodeLimit + <span class="string">&quot; for application data.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="4-2-验证测试"><a href="#4-2-验证测试" class="headerlink" title="4.2 验证测试"></a>4.2 验证测试</h2><p>设置 Yarn app 允许写 ZNode 的最大值（由于是测试环境，上限限制得比较小），重启 active RM。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">参数：yarn-site.xml 的 ResourceManager 高级配置代码段（安全阀）</span><br><span class="line">值：</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.zk-max-znode-size.bytes&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;512&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>测试任务：</p>
<p>hadoop jar /opt/cloudera/parcels/CDH-5.14.4-1.cdh5.14.4.p0.3/jars/hadoop-mapreduce-examples-2.6.0-cdh5.14.4.jar  pi -Dmapred.job.queue.name=root.exquery 20 10</p>
<p>任务失败时 RM 任务日志如下，可以看出作业状态信息保存在 ZK 的数据超过了 ZNode 限制，此时 ZK 不会保存该作业的状态信息，而 ZK 服务和 RM 服务均是正常对外提供服务的，不影响集群的正常使用。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="params">#</span> tailf hadoop-cmf-yarn-RESOURCEMANAGER-slave-prd-10-197-1-141.v-bj-5.kwang.log.out  |grep &quot;the maximum allowed size&quot;</span><br><span class="line">2020-12-10 16:53:37,544 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: Application state data size for application<span class="built_in">_</span>1607589684539<span class="built_in">_</span>0001 is 1515. exceeds the maximum allowed size 512 for application data.</span><br><span class="line">2020-12-10 16:53:48,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: Application state data size for application<span class="built_in">_</span>1607590418121<span class="built_in">_</span>0001 is 1515. exceeds the maximum allowed size 512 for application data.</span><br><span class="line"></span><br><span class="line"><span class="params">#</span> RM 具体 Warn 信息：</span><br><span class="line">2020-12-10 16:53:49,377 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:kwang (auth:SIMPLE) cause:org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id &#x27;application<span class="built_in">_</span>1607590418121<span class="built_in">_</span>0001&#x27; doesn&#x27;t exist in RM.</span><br><span class="line">2020-12-10 16:53:49,377 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8032, call org.apache.hadoop.yarn.api.ApplicationClientProtocolPB.getApplicationReport from 10.197.1.141:56026 Call<span class="params">#6</span>3 Retry<span class="params">#0</span></span><br><span class="line">org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id &#x27;application<span class="built_in">_</span>1607590418121<span class="built_in">_</span>0001&#x27; doesn&#x27;t exist in RM.</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:324)</span><br><span class="line">        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:170)</span><br><span class="line">        at org.apache.hadoop.yarn.proto.ApplicationClientProtocol<span class="built_in">$</span>ApplicationClientProtocolService<span class="built_in">$</span>2.callBlockingMethod(ApplicationClientProtocol.java:401)</span><br><span class="line">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class="built_in">$</span>Server<span class="built_in">$</span>ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)</span><br><span class="line">        at org.apache.hadoop.ipc.RPC<span class="built_in">$</span>Server.call(RPC.java:1073)</span><br><span class="line">        at org.apache.hadoop.ipc.Server<span class="built_in">$</span>Handler<span class="built_in">$</span>1.run(Server.java:2281)</span><br><span class="line">        at org.apache.hadoop.ipc.Server<span class="built_in">$</span>Handler<span class="built_in">$</span>1.run(Server.java:2277)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)</span><br><span class="line">        at org.apache.hadoop.ipc.Server<span class="built_in">$</span>Handler.run(Server.java:2275)</span><br></pre></td></tr></table></figure>

<h2 id="4-3-线上参数配置"><a href="#4-3-线上参数配置" class="headerlink" title="4.3 线上参数配置"></a>4.3 线上参数配置</h2><p>设置 YARN app 允许写 ZNode 的最大值（4<em>1024</em>1024 B，即 4M），重启 RM。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">参数：yarn-site.xml 的 ResourceManager 高级配置代码段（安全阀）</span><br><span class="line">值：</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.zk-max-znode-size.bytes&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">4194304</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>前面在 2.2 小节中分析了作业在更新 Application 或 AppAttemp 状态时，会通过重试的方式向 ZK 的 ZNode 中写入数据，线上环境默认的重试次数为 1000 次，重试间隔为 60ms，而一旦任务出现异常时，这种高频次的写入会对 ZK 或 RM 服务造成一定的压力，因此可以调小作业的重试次数，减少重试时对服务的压力。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">参数：yarn-site.xml 的 ResourceManager 高级配置代码段（安全阀）</span><br><span class="line">值：</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.zk-num-retries&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">100</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><p><a href="https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java">Hadoop官方源码</a></p>
</li>
<li><p><a href="https://issues.apache.org/jira/browse/YARN-2368">issue YARN-2368</a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/1629687">ZooKeeper节点数据量限制引起的Hadoop YARN ResourceManager崩溃原因分析</a></p>
</li>
<li><p><a href="https://blog.csdn.net/Androidlushangderen/article/details/48224707">YARN源码分析(三)—–ResourceManager HA之应用状态存储与恢复</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>YARN</tag>
        <tag>YARN源码</tag>
        <tag>bugfix</tag>
      </tags>
  </entry>
  <entry>
    <title>YARN聚合日志源码分析及优化</title>
    <url>/2021/11/16/YARN%E8%81%9A%E5%90%88%E6%97%A5%E5%BF%97%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8F%8A%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<p>线上集群 Container 日志上报的事务集群 namenode rpc 持续飙高，影响到了 YARN 分配 Container 的性能，任务提交数下降，导致整个集群的吞吐量下降。</p>
<p>这是由于作业提交到 YARN 集群时，每个 NM 节点都会对每个 app 作业进行日志聚合操作，该操作包括初始化日志聚合服务、检测和创建日志聚合的 HDFS 目录、创建日志聚合线程执行本地日志的上传。其中，初始化日志聚合服务就是简单的对象创建，不和 HDFS 交互，基本无压力；检测和创建日志聚合的 HDFS 目录会执行 HDFS 读和写请求，并且是同步阻塞执行，依赖写 /tmp/logs/ 目录所在集群的 HDFS 服务；创建日志聚合线程上传本地日志，代码中该线程是通过线程池异步创建，不存在阻塞，但固定大小的线程池可能会出现线程创建阻塞。</p>
<p>根据以上分析，针对日志聚合依赖读写 HDFS 数据反向影响作业的提交问题，主要有两种解决方案：</p>
<ul>
<li>作业提交不依赖日志聚合对 HDFS 服务的读/写。（本文主要解决这一问题）</li>
<li>日志聚合写 HDFS 进行分流，写到多个 HDFS 集群。</li>
</ul>
<h1 id="1-聚合日志介绍"><a href="#1-聚合日志介绍" class="headerlink" title="1. 聚合日志介绍"></a>1. 聚合日志介绍</h1><p>日志聚集是 YARN 提供的日志中央化管理功能，它能将运行完成的 Container 任务日志上传到 HDFS 上，从而减轻 NodeManager 负载，且提供一个中央化存储和分析机制。默认情况下，Container 任务日志存在在各个 NodeManager 的本地磁盘上，保存在 <code>yarn.nodemanager.log-dirs</code>参数配置的目录下，保存的时间由 <code>yarn.nodemanager.log.retain-seconds</code> 参数决定（默认时3小时）。若启用日志聚集功能，会将作业完成的日志上传到 HDFS 的 <code>$&#123;yarn.nodemanager.remote-app-log-dir&#125;/$&#123;user&#125;/$&#123;yarn.nodemanager.remote-app-log-dir-suffix&#125; </code>下，要实现日志聚合功能，需要额外的配置。</p>
<p>这里的日志存储的就是具体 Mapreduce 和 Spark 任务的日志，包括框架的和应用程序里自己打印的。这日志聚合是用来看日志的，而 job history server 则是用来看某个application 的大致统计信息的，包括作业启停时间，map 任务数，reduce 任务数以及各种计数器的值等等。job history server 是抽象概要性的统计信息，而聚合日志是该application 所有任务节点的详细日志集合。</p>
<h1 id="2-聚合日志生命周期"><a href="#2-聚合日志生命周期" class="headerlink" title="2. 聚合日志生命周期"></a>2. 聚合日志生命周期</h1><p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/16/6d487d41f9fb51ce956ff7f2877ff802-1637065327727-58de5ef3-1b51-4f9d-809d-95a468c82d44-678b84.png" alt="img"></p>
<center>YARN 聚合日志上传流程</center>

<p>YARN 作业在运行过程中，聚合日志的生命周期如上图，大致分为以下四个步骤：</p>
<ol>
<li><p>作业运行过程中，日志将暂存于 <code>yarn.nodemanager.log-dirs</code> 配置项指定的本地路径下，默认为 <code>/var/log/hadoop-yarn/container/</code>。</p>
</li>
<li><p>作业运行结束后（无论正常结束与否），将持久化日志到 <code>yarn.nodemanager.remote-app-log-dir</code> 和 <code>yarn.nodemanager.remote-app-log-dir-suffix</code> 配置项指定的 HDFS 路径下，前者默认为 <code>/tmp/logs/</code>，后者默认为 <code>logs</code>。对应 HDFS 的实际路径为 <code>$&#123;yarn.nodemanager.remote-app-log-dir&#125;/$&#123;user&#125;/$&#123;yarn.nodemanager.remote-app-log-dir-suffix&#125;/$&#123;application_id&#125;/</code>，即 <code>/tmp/logs/&lt;user&gt;/logs/</code>。控制日志聚合操作的服务为 LogAggregationService，具体上传日志到 HDFS 的行为由 LogAggregationService 服务创建的 AppLogAggregator 线程执行。</p>
</li>
<li><p>日志持久化聚合到 HDFS 后，会删除本地的暂存日志。</p>
</li>
<li><p>聚合上传到 HDFS 的日志也是有保留周期的，保存周期由 <code>yarn.log-aggregation.retain-seconds</code> 参数控制，集群可配置。</p>
</li>
</ol>
<h1 id="3-聚合日志参数"><a href="#3-聚合日志参数" class="headerlink" title="3. 聚合日志参数"></a>3. 聚合日志参数</h1><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">参数：yarn.nodemanager.log-dirs</span><br><span class="line">参数解释：日志存放地址（可配置多个目录）。</span><br><span class="line">默认值：<span class="built_in">$</span>&#123;yarn.log.dir&#125;/userlogs</span><br><span class="line"></span><br><span class="line">参数：yarn.log-aggregation-enable</span><br><span class="line">参数解释：是否启用日志聚集功能。</span><br><span class="line">默认值：false</span><br><span class="line"></span><br><span class="line">参数：yarn.log-aggregation.retain-seconds</span><br><span class="line">参数解释：在HDFS上聚集的日志最多保存多长时间。</span><br><span class="line">默认值：-1</span><br><span class="line"></span><br><span class="line">参数：yarn.log-aggregation.retain-check-interval-seconds</span><br><span class="line">参数解释：多长时间检查一次日志，并将满足条件的删除，如果是0或者负数，则为上一个值的1/10。</span><br><span class="line">默认值：-1</span><br><span class="line"></span><br><span class="line">参数：yarn.nodemanager.remote-app-log-dir</span><br><span class="line">参数解释：当应用程序运行结束后，日志被转移到的HDFS目录（启用日志聚集功能时有效）。</span><br><span class="line">默认值：/tmp/logs</span><br><span class="line"></span><br><span class="line">参数：yarn.nodemanager.remote-app-log-dir-suffix</span><br><span class="line">参数解释：远程日志目录子目录名称（启用日志聚集功能时有效）。</span><br><span class="line">默认值：logs 日志将被转移到目录<span class="built_in">$</span>&#123;yarn.nodemanager.remote-app-log-dir&#125;/<span class="built_in">$</span>&#123;user&#125;/<span class="built_in">$</span>&#123;thisParam&#125;下</span><br><span class="line"></span><br><span class="line">参数：yarn.nodemanager.log.retain-seconds</span><br><span class="line">参数解释：NodeManager上日志最多存放时间（不启用日志聚集功能时有效）。</span><br><span class="line">默认值：10800（3小时）</span><br></pre></td></tr></table></figure>

<h1 id="4-技术方案调研"><a href="#4-技术方案调研" class="headerlink" title="4. 技术方案调研"></a>4. 技术方案调研</h1><p><a href="https://mp.weixin.qq.com/s/0ffysIzzJIFLcyg2bXfwSQ">YARN 在字节跳动的优化与实践</a></p>
<blockquote>
<p>将 HDFS 做成弱依赖 对于一般的离线批处理来说，如果 HDFS 服务不可用了，那么 YARN 也没必要继续运行了。但是在字节跳动内部由于 YARN 还同时承载流式作业和模型训练，因此不能容忍 HDFS 故障影响到 YARN。为此，我们通过将 NodeLabel 存储到 ZK 中，将 Container Log 在 HDFS 的目录初始化和上传都改为异步的方式，摆脱了对 HDFS 的强依赖。</p>
</blockquote>
<p><a href="https://mp.weixin.qq.com/s/z5HzYSqc2zHmd-DDBVcG4w">YARN 在快手的应用实践与技术演进之路</a></p>
<blockquote>
<p>HDFS是yarn非常底层的基础设施，ResourceManager事件处理逻辑中有一些HDFS操作，HDFS卡一下，会造成整个事件处理逻辑卡住，最终整个集群卡住。分析发现RM对HDFS的操作主要集中在失败APP的处理，不是非常核心的逻辑，解决方案也比较简单粗暴，把HDFS的操作从同步改成异步。我们还对整个yarn事件处理逻辑进行排查，发现有一些像DNS的操作，在某些情况下也会比较卡，我们就把这种比较重IO的操作进行相应的优化，确保事件处理逻辑中都是快速的CPU操作，保证事件处理的高效和稳定。</p>
</blockquote>
<p><a href="https://www.infoq.cn/article/pmlsDrWfcjZBxj2P97S1">基于 Hadoop 的 58 同城离线计算平台设计与实践</a></p>
<blockquote>
<p>虽然有 Fedoration 机制来均衡各个 NN 的压力，但是对于单个 NN 压力仍然非常大，各种问题时刻在挑战 HDFS 稳定性，比如：NN RPC 爆炸，我们线上最大的 NS 有 15 亿的 RPC 调用，4000+ 并发连接请求，如此高的连接请求对业务稳定影响很大。针对这个问题，我们使用”拆解+优化”的两种手段相结合的方式来改进。拆解就是说我们把一些大的访问，能不能拆解到不同的集群上，或者我们能不能做些控制，具体案例如下： 1.Hive Scratch：我们经过分析 Hive Scratch 的临时目录在 RPC 调用占比中达到 20%，对于 Hive Scratch 实际上每个业务不需要集中到一个 NS 上，我们把它均衡到多个 NS 上。 2.Yarn 日志聚合：Yarn 的日志聚合主要是给业务查看一些日志，实际上他没有必要那个聚合到 HDFS 上，只需要访问本地就可以了。ResourceLocalize：同样把它均衡到各个 NS 上。</p>
</blockquote>
<p><a href="https://tech.meituan.com/2017/04/14/hdfs-federation.html">HDFS Federation在美团点评的应用与改进</a></p>
<blockquote>
<p>计算引擎（包括MapReduce和Spark）在提交作业时，会向NameNode发送RPC，获取HDFS Token。在ViewFileSystem中，会向所有namespace串行的申请Token，如果某个namespace的NameNode负载很高，或者发生故障，则任务无法提交，YARN的ResourceManager在renew Token时，也会受此影响。随着美团点评的发展YARN作业并发量也在逐渐提高，保存在HDFS上的YARN log由于QPS过高，被拆分为独立的namespace。但由于其并发和YARN container并发相同，NameNode读写压力还是非常大，经常导致其RPC队列打满，请求超时，进而影响了作业的提交。针对此问题，我们做出了一下改进： 1.container日志由NodeManager通过impersonate写入HDFS，这样客户端在提交Job时，就不需要YARN log所在namespace的Token。 2.ViewFileSystem在获取Token时，增加了参数，用于指定不获取哪些namespace的Token。 3.由于作业并不总是需要所有namespace中的数据，因此当单个namespace故障时，不应当影响其他namespace数据的读写，否则会降低整个集群的分区容忍性和可用性，ViewFileSystem在获取Token时，即使失败，也不影响作业提交，而是在真正访问数据时作业失败，这样在不需要的Token获取失败时，不影响作业的运行。 另外，客户端获取到的Token会以namespace为key，保存在一个自定义数据结构中（Credentials）。ResourceManager renew时，遍历这个数据结构。而NodeManager在拉取JAR包时，根据本地配置中的namespace名去该数据结构中获取对应Token。因此需要注意的是，虽然namespace配置和服务端不同不影响普通HDFS读写，但提交作业所使用的namespace配置需要与NodeManager相同，至少会用到的namespace配置需要是一致的。</p>
</blockquote>
<p>本文主要针对字节跳动的思路对日志聚合逻辑进行优化，将日志聚合读写 HDFS 集群改为弱依赖。</p>
<h1 id="5-YARN日志聚合源码分析"><a href="#5-YARN日志聚合源码分析" class="headerlink" title="5. YARN日志聚合源码分析"></a>5. YARN日志聚合源码分析</h1><p>要弄清楚聚合日志如何工作的，就需要了解 YARN 中处理聚合日志的服务在哪里创建的，根据 <a href="https://benkoons.github.io/2021/11/07/YARN-ApplicationMaster%E5%90%AF%E5%8A%A8%E5%8E%9F%E7%90%86%E4%B8%8E%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">YARN ApplicationMaster启动原理与源码分析</a> 文章分析，我们知道 YARN 的第一个 Container 启动是用于 AppAttmpt 角色，也就是我们通常在 YARN UI 界面看到的 ApplicationMaster 服务。所以我们来看看一个作业的第一个 Container 是如何启动以及如何创建日志记录组件 LogHandler 的。</p>
<p>ApplicationMaster 通过调用 RPC 函数 ContainerManagementProtocol#startContainers() 开始启动 Container，即 startContainerInternal() 方法，这部分逻辑做了两件事：</p>
<ul>
<li>发送 ApplicationEventType.INIT_APPLICATION 事件，对应用程序资源的初始化，主要是初始化各类必需的服务组件（如日志记录组件 LogHandler、资源状态追踪组件 LocalResourcesTrackerImpl等），供后续 Container 启动，通常来自 ApplicationMaster 的第一个 Container 完成，这里的 if 逻辑针对一个 NM 节点上运行作业的所有 Containers 只调用一次，后续的 Container 跳过这段 Application 初始化过程。</li>
<li>发送 ApplicationEventType.INIT_CONTAINER 事件，对 Container 进行初始化操作。（这部分事件留在 Container 启动环节介绍）</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startContainerInternal</span><span class="params">(NMTokenIdentifier nmTokenIdentifier,</span></span></span><br><span class="line"><span class="params"><span class="function">      ContainerTokenIdentifier containerTokenIdentifier,</span></span></span><br><span class="line"><span class="params"><span class="function">      StartContainerRequest request)</span> <span class="keyword">throws</span> YarnException, IOException </span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 省略Token认证及ContainerLaunchContext上下文初始化</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">this</span>.readLock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (!serviceStopped) &#123;</span><br><span class="line">        <span class="comment">// Create the application</span></span><br><span class="line">        Application application =</span><br><span class="line">            <span class="keyword">new</span> ApplicationImpl(dispatcher, user, applicationID, credentials, context);</span><br><span class="line">         </span><br><span class="line">        <span class="comment">// 应用程序的初始化，供后续Container使用，这个逻辑只调用一次，通常由来自ApplicationMaster的第一个Container完成</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == context.getApplications().putIfAbsent(applicationID,</span><br><span class="line">          application)) &#123;</span><br><span class="line">          LOG.info(<span class="string">&quot;Creating a new application reference for app &quot;</span> + applicationID);</span><br><span class="line">          LogAggregationContext logAggregationContext =</span><br><span class="line">              containerTokenIdentifier.getLogAggregationContext();</span><br><span class="line">          Map&lt;ApplicationAccessType, String&gt; appAcls =</span><br><span class="line">              container.getLaunchContext().getApplicationACLs();</span><br><span class="line">          context.getNMStateStore().storeApplication(applicationID,</span><br><span class="line">              buildAppProto(applicationID, user, credentials, appAcls,</span><br><span class="line">                logAggregationContext));</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">          <span class="comment">// 1.向 ApplicationImpl 发送 ApplicationEventType.INIT_APPLICATION 事件</span></span><br><span class="line">          dispatcher.getEventHandler().handle(</span><br><span class="line">            <span class="keyword">new</span> ApplicationInitEvent(applicationID, appAcls,</span><br><span class="line">              logAggregationContext));</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 2.向 ApplicationImpl 发送 ApplicationEventType.INIT_CONTAINER 事件</span></span><br><span class="line">        <span class="keyword">this</span>.context.getNMStateStore().storeContainer(containerId, request);</span><br><span class="line">        dispatcher.getEventHandler().handle(</span><br><span class="line">          <span class="keyword">new</span> ApplicationContainerInitEvent(container));</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">this</span>.context.getContainerTokenSecretManager().startContainerSuccessful(</span><br><span class="line">          containerTokenIdentifier);</span><br><span class="line">        NMAuditLogger.logSuccess(user, AuditConstants.START_CONTAINER,</span><br><span class="line">          <span class="string">&quot;ContainerManageImpl&quot;</span>, applicationID, containerId);</span><br><span class="line">        <span class="comment">// TODO launchedContainer misplaced -&gt; doesn&#x27;t necessarily mean a container</span></span><br><span class="line">        <span class="comment">// launch. A finished Application will not launch containers.</span></span><br><span class="line">        metrics.launchedContainer();</span><br><span class="line">        metrics.allocateContainer(containerTokenIdentifier.getResource());</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> YarnException(</span><br><span class="line">            <span class="string">&quot;Container start failed as the NodeManager is &quot;</span> +</span><br><span class="line">            <span class="string">&quot;in the process of shutting down&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">this</span>.readLock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>这里主要看看第1件事情，即向 ApplicationImpl 发送 ApplicationEventType.INIT_APPLICATION 事件，事件对应的状态机为 AppInitTransition 状态机。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java</span></span><br><span class="line"><span class="comment">// Transitions from NEW state</span></span><br><span class="line">           .addTransition(ApplicationState.NEW, ApplicationState.INITING,</span><br><span class="line">               ApplicationEventType.INIT_APPLICATION, <span class="keyword">new</span> AppInitTransition())</span><br></pre></td></tr></table></figure>



<p>AppInitTransition 状态机会对日志聚合组件服务进行初始化，关键行动是向调度器发送 LogHandlerEventType.APPLICATION_STARTED 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Notify services of new application.</span></span><br><span class="line"><span class="comment">   * </span></span><br><span class="line"><span class="comment">   * In particular, this initializes the &#123;<span class="doctag">@link</span> LogAggregationService&#125;</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AppInitTransition</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">SingleArcTransition</span>&lt;<span class="title">ApplicationImpl</span>, <span class="title">ApplicationEvent</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(ApplicationImpl app, ApplicationEvent event)</span> </span>&#123;</span><br><span class="line">      ApplicationInitEvent initEvent = (ApplicationInitEvent)event;</span><br><span class="line">      app.applicationACLs = initEvent.getApplicationACLs();</span><br><span class="line">      app.aclsManager.addApplication(app.getAppId(), app.applicationACLs);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 初始化日志聚合组件服务</span></span><br><span class="line">      <span class="comment">// Inform the logAggregator</span></span><br><span class="line">      app.logAggregationContext = initEvent.getLogAggregationContext();</span><br><span class="line">      <span class="comment">// 向调度器发送 LogHandlerEventType.APPLICATION_STARTED 事件</span></span><br><span class="line">      app.dispatcher.getEventHandler().handle(</span><br><span class="line">          <span class="keyword">new</span> LogHandlerAppStartedEvent(app.appId, app.user,</span><br><span class="line">              app.credentials, ContainerLogsRetentionPolicy.ALL_CONTAINERS,</span><br><span class="line">              app.applicationACLs, app.logAggregationContext)); </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>想要弄清楚 LogHandlerEventType.APPLICATION_STARTED 事件做了什么，就要知道 LogHandlerEventType 类注册的事件处理器是什么以及事件处理器做了什么事情。这里的 register 方法对 LogHandlerEventType 类进行了注册，对应的 logHandler 事件处理器为 LogAggregationService 服务。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 定义日志处理器</span></span><br><span class="line">    LogHandler logHandler =</span><br><span class="line">      createLogHandler(conf, <span class="keyword">this</span>.context, <span class="keyword">this</span>.deletionService);</span><br><span class="line">    addIfService(logHandler);</span><br><span class="line">    <span class="comment">// 注册 LogHandlerEventType 事件，logHandler 为对应的处理器</span></span><br><span class="line">    dispatcher.register(LogHandlerEventType.class, logHandler);</span><br><span class="line">    </span><br><span class="line">    waitForContainersOnShutdownMillis =</span><br><span class="line">        conf.getLong(YarnConfiguration.NM_SLEEP_DELAY_BEFORE_SIGKILL_MS,</span><br><span class="line">            YarnConfiguration.DEFAULT_NM_SLEEP_DELAY_BEFORE_SIGKILL_MS) +</span><br><span class="line">        conf.getLong(YarnConfiguration.NM_PROCESS_KILL_WAIT_MS,</span><br><span class="line">            YarnConfiguration.DEFAULT_NM_PROCESS_KILL_WAIT_MS) +</span><br><span class="line">        SHUTDOWN_CLEANUP_SLOP_MS;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">super</span>.serviceInit(conf);</span><br><span class="line">    recover();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>具体创建 logHandler 对象的调用，由于集群开启了日志聚合功能（由参数 <code>yarn.log-aggregation-enable</code> 控制），这里返回 LogAggregationService 服务。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> LogHandler <span class="title">createLogHandler</span><span class="params">(Configuration conf, Context context,</span></span></span><br><span class="line"><span class="params"><span class="function">      DeletionService deletionService)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (conf.getBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED,</span><br><span class="line">        YarnConfiguration.DEFAULT_LOG_AGGREGATION_ENABLED)) &#123;</span><br><span class="line">      <span class="comment">// 判断是否启用了日志聚合，由于集群开启了日志聚合，这里初始化 LogAggregationService 服务</span></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> LogAggregationService(<span class="keyword">this</span>.dispatcher, context,</span><br><span class="line">          deletionService, dirsHandler);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> NonAggregatingLogHandler(<span class="keyword">this</span>.dispatcher, deletionService,</span><br><span class="line">                                          dirsHandler,</span><br><span class="line">                                          context.getNMStateStore());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>弄清楚了 LogHandlerEventType 类注册的服务是 LogAggregationService，我们就进入 LogAggregationService 类的 handle() 方法，看看上面的 LogHandlerEventType.APPLICATION_STARTED 事件做了什么事。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(LogHandlerEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">      <span class="comment">// APPLICATION_STARTED 事件处理流程</span></span><br><span class="line">      <span class="keyword">case</span> APPLICATION_STARTED:</span><br><span class="line">        LogHandlerAppStartedEvent appStartEvent =</span><br><span class="line">            (LogHandlerAppStartedEvent) event;</span><br><span class="line">        initApp(appStartEvent.getApplicationId(), appStartEvent.getUser(),</span><br><span class="line">            appStartEvent.getCredentials(),</span><br><span class="line">            appStartEvent.getLogRetentionPolicy(),</span><br><span class="line">            appStartEvent.getApplicationAcls(),</span><br><span class="line">            appStartEvent.getLogAggregationContext());</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> CONTAINER_FINISHED:</span><br><span class="line">        <span class="comment">// 省略</span></span><br><span class="line">      <span class="keyword">case</span> APPLICATION_FINISHED:</span><br><span class="line">        <span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        ; <span class="comment">// Ignore</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>LogHandlerEventType.APPLICATION_STARTED 事件的关键逻辑在 initApp() 方法的调用。这段逻辑主要做了三件事：</p>
<ol>
<li><p>判断 HDFS 上日志聚合的根目录是否存在，即 <code>/tmp/logs/</code> 目录（具体为 <code>hdfs://nameservice/tmp/logs/</code>)，由参数 <code>yarn.nodemanager.remote-app-log-dir</code> 控制。（注意：这里的请求会阻塞读 HDFS）</p>
</li>
<li><p>创建作业日志聚合的 HDFS 目录，并初始化 app 日志聚合实例，采用线程池的方式启动日志聚合进程。（重点，这里会有请求阻塞写 HDFS，并且通过有限大小的线程池异步创建日志聚合线程去做日志的聚合）</p>
</li>
<li><p>根据构建的 ApplicationEvent 事件，向发送 ApplicationEventType.APPLICATION_LOG_HANDLING_INITED 事件，告知处理器日志聚合服务初始化完成。</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initApp</span><span class="params">(<span class="keyword">final</span> ApplicationId appId, String user,</span></span></span><br><span class="line"><span class="params"><span class="function">      Credentials credentials, ContainerLogsRetentionPolicy logRetentionPolicy,</span></span></span><br><span class="line"><span class="params"><span class="function">      Map&lt;ApplicationAccessType, String&gt; appAcls,</span></span></span><br><span class="line"><span class="params"><span class="function">      LogAggregationContext logAggregationContext)</span> </span>&#123;</span><br><span class="line">    ApplicationEvent eventResponse;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、 判断 HDFS 上日志聚合的根目录是否存在，即 `/tmp/logs/` 目录（具体为 `hdfs://nameservice/tmp/logs`)，由参数 `yarn.nodemanager.remote-app-log-dir` 控制</span></span><br><span class="line">      verifyAndCreateRemoteLogDir(getConfig());</span><br><span class="line">      <span class="comment">// 重点：2、创建作业日志聚合的 HDFS 目录，并初始化 app 日志聚合实例，采用线程池的方式启动日志聚合进程</span></span><br><span class="line">      initAppAggregator(appId, user, credentials, logRetentionPolicy, appAcls,</span><br><span class="line">          logAggregationContext);</span><br><span class="line">      <span class="comment">// 构建 ApplicationEvent 事件</span></span><br><span class="line">      eventResponse = <span class="keyword">new</span> ApplicationEvent(appId,</span><br><span class="line">          ApplicationEventType.APPLICATION_LOG_HANDLING_INITED);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (YarnRuntimeException e) &#123;</span><br><span class="line">      LOG.warn(<span class="string">&quot;Application failed to init aggregation&quot;</span>, e);</span><br><span class="line">      eventResponse = <span class="keyword">new</span> ApplicationEvent(appId,</span><br><span class="line">          ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3、根据构建的 ApplicationEvent 事件，向发送 ApplicationEventType.APPLICATION_LOG_HANDLING_INITED 事件，告知处理器日志聚合服务初始化完成</span></span><br><span class="line">    <span class="keyword">this</span>.dispatcher.getEventHandler().handle(eventResponse);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>第一件事比较简单，主要是是判断 HDFS 聚合日志的根目录是否存在，由于目录一般都存在，这一块主要是读 HDFS 请求。我们主要来看看 initApp() 方法做的第二件事，可以看到第三件事是发送 ApplicationEventType.APPLICATION_LOG_HANDLING_INITED 事件，表示日志聚合服务初始化完成，包括创建作业在 HDFS 的日志聚合目录和启动日志聚合线程。所以基本可以知道第2件事的 initAppAggregator() 是会创建作业日志聚合目录，并启动日志聚合线程，具体的我们来看代码。</p>
<p>这段代码其实主要做了两件事：</p>
<ol>
<li>调用 createAppDir() 方法执行 HDFS 写请求为作业创建日志聚合的目录，即 <code>hdfs://nameservice/tmp/logs/&lt;user&gt;/logs/</code> 目录，这里的写逻辑如果成功则只调用一次，一般是由第一个 Container 创建（即作业的 ApplicationMaster Container），其他 Container 只执行 HDFS 读请求判断该目录是否存在即可。</li>
<li>通过 threadPool 线程池创建每个作业在 NM 节点的日志聚合线程，异步处理本地日志的上传，该线程池大小由参数 <code>yarn.nodemanager.logaggregation.threadpool-size-max</code> 控制，默认大小为 100。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initAppAggregator</span><span class="params">(<span class="keyword">final</span> ApplicationId appId, String user,</span></span></span><br><span class="line"><span class="params"><span class="function">      Credentials credentials, ContainerLogsRetentionPolicy logRetentionPolicy,</span></span></span><br><span class="line"><span class="params"><span class="function">      Map&lt;ApplicationAccessType, String&gt; appAcls,</span></span></span><br><span class="line"><span class="params"><span class="function">      LogAggregationContext logAggregationContext)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get user&#x27;s FileSystem credentials</span></span><br><span class="line">    <span class="keyword">final</span> UserGroupInformation userUgi =</span><br><span class="line">        UserGroupInformation.createRemoteUser(user);</span><br><span class="line">    <span class="keyword">if</span> (credentials != <span class="keyword">null</span>) &#123;</span><br><span class="line">      userUgi.addCredentials(credentials);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// New application</span></span><br><span class="line">    <span class="keyword">final</span> AppLogAggregator appLogAggregator =</span><br><span class="line">        <span class="keyword">new</span> AppLogAggregatorImpl(<span class="keyword">this</span>.dispatcher, <span class="keyword">this</span>.deletionService,</span><br><span class="line">            getConfig(), appId, userUgi, <span class="keyword">this</span>.nodeId, dirsHandler,</span><br><span class="line">            getRemoteNodeLogFileForApp(appId, user), logRetentionPolicy,</span><br><span class="line">            appAcls, logAggregationContext, <span class="keyword">this</span>.context,</span><br><span class="line">            getLocalFileContext(getConfig()));</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.appLogAggregators.putIfAbsent(appId, appLogAggregator) != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(<span class="string">&quot;Duplicate initApp for &quot;</span> + appId);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// wait until check for existing aggregator to create dirs</span></span><br><span class="line">    YarnRuntimeException appDirException = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 创建作业日志聚合目录，即 hdfs://nameservice/tmp/logs/&lt;user&gt;/logs/ 目录</span></span><br><span class="line">      <span class="comment">// Create the app dir</span></span><br><span class="line">      createAppDir(user, appId, userUgi);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      appLogAggregator.disableLogAggregation();</span><br><span class="line">      <span class="keyword">if</span> (!(e <span class="keyword">instanceof</span> YarnRuntimeException)) &#123;</span><br><span class="line">        appDirException = <span class="keyword">new</span> YarnRuntimeException(e);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        appDirException = (YarnRuntimeException)e;</span><br><span class="line">      &#125;</span><br><span class="line">      appLogAggregators.remove(appId);</span><br><span class="line">      closeFileSystems(userUgi);</span><br><span class="line">      <span class="keyword">throw</span> appDirException;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建作业的日志聚合线程，并通过线程池启动日志聚合线程，异步上传 NM 节点的日志</span></span><br><span class="line">    <span class="comment">// Schedule the aggregator.</span></span><br><span class="line">    Runnable aggregatorWrapper = <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          appLogAggregator.run();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          appLogAggregators.remove(appId);</span><br><span class="line">          closeFileSystems(userUgi);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">this</span>.threadPool.execute(aggregatorWrapper);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，从日志聚合服务组件的创建，到为作业初始化 HDFS 聚合日志目录，到启动日志聚合线程，整个日志聚合的调用逻辑已介绍完毕，日志的具体上传逻辑在 AppLogAggregatorImpl 类的 run() 方法开始执行，具体上传这里不做详细介绍，感兴趣可以可以去看看上传行为是如何做的。</p>
<h1 id="6-优化方案"><a href="#6-优化方案" class="headerlink" title="6. 优化方案"></a>6. 优化方案</h1><p>在背景介绍中，提到了日志聚合操作存在风险的点主要在读/写 HDFS 请求所在的集群 namenode rpc 压力，和固定大小的线程池创建线程的阻塞，代码的修改逻辑也是结合这两个问题诞生的。</p>
<ul>
<li>针对读/写 HDFS 请求的 rpc 压力，代码将日志聚合逻辑中与 HDFS 交互的方式全部改为异步处理，不依赖日志聚合读写数据的 HDFS 集群。</li>
<li>针对固定大小线程池创建线程可能出现的阻塞情况，代码将这一块修改为生产者-消费者模式，聚合日志线程的产生与线程的处理解耦。</li>
</ul>
<h2 id="6-1-读-写-HDFS-请求异步"><a href="#6-1-读-写-HDFS-请求异步" class="headerlink" title="6.1 读/写 HDFS 请求异步"></a>6.1 读/写 HDFS 请求异步</h2><p>日志聚合服务中与 HDFS 交互有两个地方，一个是读操作，判断 HDFS 上 <code>/tmp/logs/</code> 目录是否存在，一个是写操作，创建作业的聚合日志目录 <code>/tmp/logs/&lt;user&gt;/logs/&lt;appid&gt;/</code>，这写操作每个作业只执行一次，后续都是读操作，判断该目录是否存在即可。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">asyncCreateAppDir</span><span class="params">(<span class="keyword">final</span> String user, <span class="keyword">final</span> ApplicationId appId, <span class="keyword">final</span> UserGroupInformation userUgi)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">new</span> Thread() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	        <span class="keyword">try</span> &#123;</span><br><span class="line">	          <span class="comment">// check dir &#x27;/tmp/logs/&#x27; exists</span></span><br><span class="line">	          verifyAndCreateRemoteLogDir(getConfig());</span><br><span class="line">	          <span class="comment">// create app log dir</span></span><br><span class="line">	          createAppDir(user, appId, userUgi);</span><br><span class="line">	        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">	          e.printStackTrace();</span><br><span class="line">	        &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.start();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>将日志聚合读写 HDFS 请求改为异步后，可能会产生另外一个问题。由于作业日志聚合目录的创建是异步的，而执行日志上传操作也是异步进行的，这里存在着先后顺序，即必须作业的日志聚合目录已经创建完成，上传操作才能正常进行。因此，在具体执行上传操作时，我们对日志聚合目录是否存在添加一层校验，以确保上传前聚合目录必须存在。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doAppLogAggregation</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 判断作业是否完成，直到作业完成后才跳出 while 逻辑</span></span><br><span class="line">    <span class="keyword">while</span> (!<span class="keyword">this</span>.appFinishing.get() &amp;&amp; !<span class="keyword">this</span>.aborted.get()) &#123;</span><br><span class="line">      <span class="keyword">synchronized</span>(<span class="keyword">this</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          waiting.set(<span class="keyword">true</span>);</span><br><span class="line">          <span class="keyword">if</span> (<span class="keyword">this</span>.rollingMonitorInterval &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            wait(<span class="keyword">this</span>.rollingMonitorInterval * <span class="number">1000</span>);</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.appFinishing.get() || <span class="keyword">this</span>.aborted.get()) &#123;</span><br><span class="line">              <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            uploadLogsForContainers(<span class="keyword">false</span>);</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            wait(THREAD_SLEEP_TIME);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          LOG.warn(<span class="string">&quot;PendingContainers queue is interrupted&quot;</span>);</span><br><span class="line">          <span class="keyword">this</span>.appFinishing.set(<span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.aborted.get()) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 改造点：增加日志聚合目录是否存在的校验，如果不存在则创建改目录（具体改造见下面）</span></span><br><span class="line">    <span class="comment">//check remote app dir</span></span><br><span class="line">    checkRemoteDir();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关键：执行真正的日志上传动作</span></span><br><span class="line">    <span class="comment">// App is finished, upload the container logs.</span></span><br><span class="line">    uploadLogsForContainers(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 删除作业在 NM 本地目录保存的日志，由 DeletionService 服务负责。</span></span><br><span class="line">    doAppLogAggregationPostCleanUp();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.dispatcher.getEventHandler().handle(</span><br><span class="line">        <span class="keyword">new</span> ApplicationEvent(<span class="keyword">this</span>.appId,</span><br><span class="line">            ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED));</span><br><span class="line">    <span class="keyword">this</span>.appAggregationFinished.set(<span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p><code>checkRemoteDir()</code> 逻辑和前面的 <code>createAppDir(user, appId, userUgi);</code> 逻辑差不多，无非就是在实际上传时再做一次 check 操作。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java</span></span><br><span class="line">  <span class="comment">// 改造的具体代码如下</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span>  <span class="keyword">void</span> <span class="title">checkRemoteDir</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    userUgi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;Object&gt;() &#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> Object <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        FileSystem remoteFS = remoteNodeLogFileForApp.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (!remoteFS.exists(remoteNodeLogFileForApp.getParent())) &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            FsPermission dirPerm = <span class="keyword">new</span> FsPermission(APP_DIR_PERMISSIONS);</span><br><span class="line">            remoteFS.mkdirs(remoteNodeLogFileForApp.getParent(), dirPerm);</span><br><span class="line">            FsPermission umask = FsPermission.getUMask(remoteFS.getConf());</span><br><span class="line">            <span class="keyword">if</span> (!dirPerm.equals(dirPerm.applyUMask(umask))) &#123;</span><br><span class="line">              remoteFS.setPermission(remoteNodeLogFileForApp.getParent(), dirPerm);</span><br><span class="line">            &#125;</span><br><span class="line">          &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            LOG.error(<span class="string">&quot;Failed to setup application log directory for &quot;</span> + appId, e);</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">      &#125;&#125;);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="6-2-聚合日志线程的创建和处理解耦"><a href="#6-2-聚合日志线程的创建和处理解耦" class="headerlink" title="6.2 聚合日志线程的创建和处理解耦"></a>6.2 聚合日志线程的创建和处理解耦</h2><p>这一块主要是通过生产者-消费者模式，将日志聚合线程的创建和处理解耦，生产的线程由阻塞队列 logAggregatorQueue 维护，具体的线程消费逻辑由独立线程 LauncherLogAggregatorThread 处理，具体代码如下。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initAppAggregator</span><span class="params">(<span class="keyword">final</span> ApplicationId appId, String user,</span></span></span><br><span class="line"><span class="params"><span class="function">      Credentials credentials, ContainerLogsRetentionPolicy logRetentionPolicy,</span></span></span><br><span class="line"><span class="params"><span class="function">      Map&lt;ApplicationAccessType, String&gt; appAcls,</span></span></span><br><span class="line"><span class="params"><span class="function">      LogAggregationContext logAggregationContext)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//省略</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 服务停止时阻塞新请求的接收</span></span><br><span class="line">    <span class="keyword">if</span> (blockNewLogAggr) &#123;</span><br><span class="line">    	<span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">	  processed = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create the aggregator thread.</span></span><br><span class="line">    Runnable aggregatorWrapper = <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          appLogAggregator.run();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          appLogAggregators.remove(appId);</span><br><span class="line">          closeFileSystems(userUgi);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="comment">// 改造点：将线程池直接创建线程改为生产-消费模式，这里负责生产日志聚合线程，添加到阻塞队列中</span></span><br><span class="line">	  logAggregatorQueue.add(aggregatorWrapper);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// logAggregatorQueue 对象的定义</span></span><br><span class="line">  <span class="keyword">private</span> BlockingQueue&lt;Runnable&gt; logAggregatorQueue = <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;();</span><br></pre></td></tr></table></figure>



<p>定义一个消费者线程，专门用来处理 <code>logAggregatorQueue.add(aggregatorWrapper);</code> 队列中添加的线程任务。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java</span></span><br><span class="line">    <span class="comment">// 消费线程停止标志 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> stopped = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 是否阻塞新聚合日志的接收，默认不阻塞 false</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> blockNewLogAggr = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 消费队列中对象是否处理完成</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> processed = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用于线程等待/通知的 syncronized 对象</span></span><br><span class="line">    <span class="keyword">private</span> Object waitForProcess = <span class="keyword">new</span> Object();</span><br><span class="line"> </span><br><span class="line">  <span class="comment">// 改造点：具体的线程消费由单独的线程类控制，实现线程创建和处理的解耦</span></span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">LauncherLogAggregatorThread</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">while</span> (!stopped &amp;&amp; !Thread.currentThread().isInterrupted()) &#123;</span><br><span class="line">        processed = logAggregatorQueue.isEmpty();</span><br><span class="line">          <span class="keyword">if</span> (blockNewLogAggr) &#123;</span><br><span class="line">              <span class="keyword">synchronized</span> (waitForProcess) &#123;</span><br><span class="line">                  <span class="keyword">if</span> (processed) &#123;</span><br><span class="line">                      waitForProcess.notify();</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        Runnable toLaunch;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// Schedule the aggregator.</span></span><br><span class="line">          toLaunch = logAggregatorQueue.take();</span><br><span class="line">          threadPool.execute(toLaunch);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          LOG.warn(<span class="keyword">this</span>.getClass().getName() + <span class="string">&quot; interrupted. Returning.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>服务停止时等待消费队列聚合事件处理完成，然后关闭消费线程和线程池。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceStop</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    LOG.info(<span class="keyword">this</span>.getName() + <span class="string">&quot; waiting for pending aggregation during exit&quot;</span>);</span><br><span class="line">    blockNewLogAggr = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">synchronized</span> (waitForProcess) &#123;</span><br><span class="line">      <span class="keyword">while</span> (!processed &amp;&amp; launcherLogAggregatorThread.isAlive()) &#123;</span><br><span class="line">        waitForProcess.wait(<span class="number">1000</span>);</span><br><span class="line">        LOG.info(<span class="string">&quot;Waiting for launcherLogAggregatorThread to process. Thread state is :&quot;</span> + launcherLogAggregatorThread.getState());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">this</span>.stopped = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">if</span> (launcherLogAggregatorThread != <span class="keyword">null</span>) &#123;</span><br><span class="line">      launcherLogAggregatorThread.interrupt();</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        launcherLogAggregatorThread.join();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException ie) &#123;</span><br><span class="line">        LOG.warn(launcherLogAggregatorThread.getName() + <span class="string">&quot; interrupted during join &quot;</span>, ie);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stopAggregators();</span><br><span class="line">    <span class="keyword">super</span>.serviceStop();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="7-测试分析"><a href="#7-测试分析" class="headerlink" title="7. 测试分析"></a>7. 测试分析</h1><p>测试集群分为 hadoop-up1 集群和 hadoop-up2 集群，采用 viewfs 模式访问 HDFS，作业提交在 hadoop-up1 集群，日志聚合目录 <code>/tmp/logs/</code> 挂载在 hadoop-up2 集群下，即 <code>hdfs://hadoop-up2/tmp/logs/</code> 目录。</p>
<h2 id="7-1-NM日志聚合改造前"><a href="#7-1-NM日志聚合改造前" class="headerlink" title="7.1 NM日志聚合改造前"></a>7.1 NM日志聚合改造前</h2><p>作业提交命令：</p>
<blockquote>
<p>hadoop jar /opt/cloudera/parcels/CDH-5.14.4-1.cdh5.14.4.p0.3/jars/hadoop-mapreduce-examples-2.6.0-cdh5.14.4.jar pi -Dmapred.job.queue.name=root.exquery 50 50</p>
</blockquote>
<h3 id="7-1-1-开启-up2-集群-HDFS-服务"><a href="#7-1-1-开启-up2-集群-HDFS-服务" class="headerlink" title="7.1.1 开启 up2 集群 HDFS 服务"></a>7.1.1 开启 up2 集群 HDFS 服务</h3><p><strong>结论：</strong>作业正常提交，日志正常聚合。</p>
<h3 id="7-1-2-关闭-up2-集群-HDFS-服务"><a href="#7-1-2-关闭-up2-集群-HDFS-服务" class="headerlink" title="7.1.2 关闭 up2 集群 HDFS 服务"></a>7.1.2 关闭 up2 集群 HDFS 服务</h3><p><strong>结论：</strong>作业提交卡住，需等待请求 HDFS 服务超时，作业处于 Accepted 状态卡住，作业的 ApplicationMaster 处于 NEW 状态，该 Container 没有被分配（整个过程卡住大概 3min），直到抛异常触发日志聚合失败（即 ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED）事件，作业的 ApplicationMaster 分配到 Container，作业开始运行，并且日志不聚合。</p>
<p><strong>现象1:</strong></p>
<p>作业提交卡住，YARN UI 作业的状态为 Accepted 状态，Elapsed 时间大概持续了 3min，说明作业在这段时间一直等待运行，并且用于启动 ApplicationMaster 的 Container 状态为 NEW，没有转换到 Submited 状态，表示 Container 没有运行，YARN 认为该作业还未提交。这也是线上集群在日志聚合集群 rpc 压力大时会影响作业的提交数和 Container 分配性能下降的原因。 </p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/16/c3e8a179698af5e4bc0dc4ee1392583b-1637066438185-21b10203-de41-405f-8b01-73be38692032-44b5fa.png" alt="img"><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/16/043368e578589389f5922ef9f798fa7f-1637066438442-12a965c0-2692-452e-8052-c62c424f9c7b-d31c59.png" alt="img"><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/16/63c946d6373eeccbe88257d366bc7f41-1637067044797-16b73c6a-6576-4308-adf3-e7c334c46f5a-9caa8a.png" alt="img"></p>
<p><strong>现象2:</strong></p>
<p>由于 hadoop-up2 集群 HDFS 服务关闭，分析 NodeManager 执行日志，先打印 <code>Application failed to init aggregation</code> 信息，然后打印 <code>LogAggregationService.verifyAndCreateRemoteLogDir() </code>方法执行 HDFS 读请求的调用异常，读请求多次重试后抛出 YarnRuntimeException 异常，堆栈信息的调用栈和执行代码都和这一现象吻合。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// NodeManager 日志：</span><br><span class="line">2021-03-10 09:56:43,444 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService: Application failed to init aggregation</span><br><span class="line">org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to check permissions for dir [/tmp/logs]</span><br><span class="line">        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.verifyAndCreateRemoteLogDir(LogAggregationService.java:205)</span><br><span class="line">        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp(LogAggregationService.java:336)</span><br><span class="line">        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:463)</span><br><span class="line">        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:68)</span><br><span class="line">        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:182)</span><br><span class="line">        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.net.ConnectException: Call From 10-197-1-236/10.197.1.236 to 10-197-1-238:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</span><br><span class="line">        // 省略</span><br><span class="line">        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1261)</span><br><span class="line">        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:432)</span><br><span class="line">        at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.getFileStatus(ChRootedFileSystem.java:226)</span><br><span class="line">        at org.apache.hadoop.fs.viewfs.ViewFileSystem.getFileStatus(ViewFileSystem.java:379)</span><br><span class="line">        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.verifyAndCreateRemoteLogDir(LogAggregationService.java:194)</span><br><span class="line">2021-03-10 09:56:43,445 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Log Aggregation service failed to initialize, there will be no logs for this application</span><br></pre></td></tr></table></figure>

<h2 id="7-2-NM日志聚合改造后"><a href="#7-2-NM日志聚合改造后" class="headerlink" title="7.2 NM日志聚合改造后"></a>7.2 NM日志聚合改造后</h2><p>作业提交命令：</p>
<blockquote>
<p>hadoop jar /opt/cloudera/parcels/CDH-5.14.4-1.cdh5.14.4.p0.3/jars/hadoop-mapreduce-examples-2.6.0-cdh5.14.4.jar pi -Dmapred.job.queue.name=root.exquery 50 50</p>
</blockquote>
<h3 id="（1）开启-hadoop-up2-集群-HDFS-服务"><a href="#（1）开启-hadoop-up2-集群-HDFS-服务" class="headerlink" title="（1）开启 hadoop-up2 集群 HDFS 服务"></a>（1）开启 hadoop-up2 集群 HDFS 服务</h3><p><strong>结论：</strong>作业正常提交，日志正常聚合。</p>
<h3 id="（2）关闭-hadoop-up2-集群-HDFS-服务"><a href="#（2）关闭-hadoop-up2-集群-HDFS-服务" class="headerlink" title="（2）关闭 hadoop-up2 集群 HDFS 服务"></a>（2）关闭 hadoop-up2 集群 HDFS 服务</h3><p><strong>结论：</strong> 作业正常提交，日志聚合失败，不影响作业提交和运行。</p>
<p><strong>现象：</strong></p>
<ul>
<li><p>作业正常执行提交和执行。</p>
</li>
<li><p>日志聚合读请求 HDFS 异常（和 7.1 中 NM 日志一致），但不影响作业执行。</p>
</li>
<li><p>日志聚合失败，HistoryServer 无法查看聚合的日志。 </p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/16/f030a2d6f3aff5590254ba0500e9f117-1637067077718-7e978d06-1909-423f-b610-45e3bc94e209-4fa55a.png" alt="img"></p>
<p>至此，我们已经实现了 YARN 聚合日志的异步初始化和上传，避免了聚合日志集群 HDFS 服务异常对 YARN 集群任务执行的影响。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol>
<li><a href="https://blog.csdn.net/Androidlushangderen/article/details/90115624">YARN的Log Aggregation原理</a></li>
</ol>
]]></content>
      <categories>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>YARN</tag>
        <tag>YARN源码</tag>
        <tag>bugfix</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark RDD算子使用手册</title>
    <url>/2021/11/21/Spark-RDD%E7%AE%97%E5%AD%90%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</url>
    <content><![CDATA[<p>Spark 的 RDD 算子分为 Transformation 算子和 Action 算子，其中Transformation 操作是延迟计算（又叫惰性计算，lazy execution）的，也就是说从一个 RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。Action 算子会触发 SparkContext 提交 Job 作业，并将数据输出 Spark系统。</p>
<h1 id="1-RDD概览"><a href="#1-RDD概览" class="headerlink" title="1. RDD概览"></a>1. RDD概览</h1><p>从大方向来看，Spark RDD 算子氛围 Transformation 算子和 Action 算子，从小方向来看：RDD 算子又可分为以下三类:</p>
<ul>
<li><p>针对 value 数据类型的Transformation算子，这种变换并不触发提交作业，处理的数据项是 value 型的数据。</p>
</li>
<li><p>针对 key-value数据类型的 Transfromation 算子，这种变换并不触发提交作业，处理的数据项是 key-value 型的数据对。</p>
</li>
<li><p>Action算子，这类算子会触发 SparkContext 提交 Job 作业。</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/21/c805d9665b3b320d46be35da36ba5ac9-image-20211121223351191-bd288a.png" alt="image-20211121223351191"></p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/21/a62d5348a80c73b5c4d83851bc068b5c-image-20211121223405459-be7562.png" alt="image-20211121223405459"></p>
<h1 id="2-RDD的创建"><a href="#2-RDD的创建" class="headerlink" title="2. RDD的创建"></a>2. RDD的创建</h1><h2 id="2-1-从集合创建RDD"><a href="#2-1-从集合创建RDD" class="headerlink" title="2.1 从集合创建RDD"></a>2.1 从集合创建RDD</h2><h3 id="parallelize"><a href="#parallelize" class="headerlink" title="parallelize"></a>parallelize</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">例子：</span><br><span class="line">sparkcontext.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)	<span class="comment">// 创建数据为(1,2,3,4,5,6,7,8,9,10)，分区数为3的RDD</span></span><br></pre></td></tr></table></figure>

<h3 id="makeRDD"><a href="#makeRDD" class="headerlink" title="makeRDD"></a>makeRDD</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">例子：</span><br><span class="line">sparkcontext.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)	<span class="comment">// 创建数据为(1,2,3,4,5,6,7,8,9,10)，分区数为3的RDD</span></span><br></pre></td></tr></table></figure>

<h2 id="2-2-从外部存储创建RDD"><a href="#2-2-从外部存储创建RDD" class="headerlink" title="2.2 从外部存储创建RDD"></a>2.2 从外部存储创建RDD</h2><p>从外部存储创建RDD，可以从存储系统文件创建，也可以从Hadoop接口API创建，所有的调用主体均为 <code>SparkContext</code>。</p>
<ul>
<li><p>从HDFS/本地文件格式创建：</p>
</li>
<li><ul>
<li>textFile</li>
<li>hadoopFile</li>
</ul>
</li>
<li><ul>
<li>sequenceFile</li>
<li>objectFile</li>
</ul>
</li>
<li><ul>
<li>newAPIHadoopFile</li>
</ul>
</li>
<li><p>从Hadoop接口API创建：</p>
</li>
<li><ul>
<li>hadoopRDD</li>
<li>newAPIHadoopRDD</li>
</ul>
</li>
</ul>
<p>用法：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sparkcontext.textFile(<span class="string">&quot;/Users/11085245/Documents/code/personal/data/&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="3-Transformation算子"><a href="#3-Transformation算子" class="headerlink" title="3. Transformation算子"></a>3. Transformation算子</h1><h2 id="3-1-单个RDD转换"><a href="#3-1-单个RDD转换" class="headerlink" title="3.1 单个RDD转换"></a>3.1 单个RDD转换</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>将一个RDD中的每个数据项，通过map中的函数映射变为一个新的元素。输入分区与输出分区一对一，即：有多少个输入分区，就有多少个输出分区。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------map-------------------&quot;</span>)</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * map</span></span><br><span class="line"><span class="comment">  * f: Int =&gt; U</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * foreach</span></span><br><span class="line"><span class="comment">  * f: Int =&gt; Unit</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">val</span> array1 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(array1)</span><br><span class="line"><span class="keyword">val</span> res1: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd1.map(x =&gt; x*x)</span><br><span class="line">res1.foreach(x =&gt; println(x))</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------map-------------------</span><br><span class="line">1</span><br><span class="line">4</span><br><span class="line">9</span><br><span class="line">16</span><br><span class="line">25</span><br><span class="line">36</span><br><span class="line">49</span><br><span class="line">64</span><br><span class="line">81</span><br><span class="line">100</span><br></pre></td></tr></table></figure>

<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>第一步和map一样，最后将所有的输出分区合并成一个。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<p>程序例子：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------flatMap-------------------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> array4 = <span class="type">Array</span>(<span class="string">&quot;hello spark&quot;</span>, <span class="string">&quot;hello hadoop&quot;</span>, <span class="string">&quot;hello scala&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> rdd4: <span class="type">RDD</span>[<span class="type">String</span>] = sc.parallelize(array4)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * flatMap</span></span><br><span class="line"><span class="comment">    * f: String =&gt; TraversableOnce[U]</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * flatMap 与 map 的区别</span></span><br><span class="line"><span class="comment">    *   - flatMap 返回 RDD[String] 数组，成员为 String</span></span><br><span class="line"><span class="comment">    *   - map 返回 RDD[List[String]] 数组，成员为 List[String]</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">println(<span class="string">&quot;-------flapMap输出--------&quot;</span>) </span><br><span class="line">  <span class="keyword">val</span> res4: <span class="type">RDD</span>[<span class="type">String</span>] = rdd4.flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>).toList)</span><br><span class="line">  res4.foreach(x =&gt; println(x))</span><br><span class="line"></span><br><span class="line">  println(<span class="string">&quot;-------map输出--------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> res4_1: <span class="type">RDD</span>[<span class="type">List</span>[<span class="type">String</span>]] = rdd4.map(line =&gt; line.split(<span class="string">&quot; &quot;</span>).toList)</span><br><span class="line">  res4_1.foreach(x =&gt; println(x))</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------flatMap-------------------</span><br><span class="line">println(&quot;-------flatMap输出--------&quot;) </span><br><span class="line">hello</span><br><span class="line">spark</span><br><span class="line">hello</span><br><span class="line">hadoop</span><br><span class="line">hello</span><br><span class="line">scala</span><br><span class="line">println(&quot;-------map输出--------&quot;)</span><br><span class="line">List(hello, spark)</span><br><span class="line">List(hello, hadoop)</span><br><span class="line">List(hello, scala)</span><br></pre></td></tr></table></figure>

<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>将 RDD 中的每个数据项，根据 filter 映射函数中的过滤条件过滤数据，返回一个 RDD 对象。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">传入一个映射函数，如果函数返回 <span class="literal">true</span>，则保留元素，返回 <span class="literal">false</span> 则过滤掉元素</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------filter-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> array3 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd3: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(array3)</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * filter</span></span><br><span class="line"><span class="comment">  * f: Int =&gt; Boolean</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">val</span> res3: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd3.filter(x =&gt; <span class="keyword">if</span> (x % <span class="number">2</span> == <span class="number">0</span>) <span class="literal">true</span> <span class="keyword">else</span> <span class="literal">false</span>)</span><br><span class="line">res3.foreach(x =&gt; println(x))</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------filter-------------------</span><br><span class="line">2</span><br><span class="line">4</span><br><span class="line">6</span><br><span class="line">8</span><br><span class="line">10</span><br></pre></td></tr></table></figure>

<h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><p>该函数和map函数类似，只不过映射函数的参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器。如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">      preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>] 表示映射函数</span><br><span class="line">preservesPartitioning 表示是否保留父<span class="type">RDD</span>的partitioner分区信息</span><br></pre></td></tr></table></figure>

<p>程序例子：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------mapPartition-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> array2 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>)</span><br><span class="line"><span class="comment">// 创建具有2个分区的RDD</span></span><br><span class="line"><span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(array2, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * mapPartitons</span></span><br><span class="line"><span class="comment">  * f: Iterator[Int] =&gt; Iterator[U]</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * mapPartitions 与 map 的区别</span></span><br><span class="line"><span class="comment">  *   - mapPartitions 每次处理一个分区的数据</span></span><br><span class="line"><span class="comment">  *   - map 操作每次处理一个元素</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> println(<span class="string">&quot;------mapPartitions输出----&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> res2: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd2.mapPartitions((x: <span class="type">Iterator</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">  println(<span class="string">&quot;执行了一次mapPartitions操作&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> newList: <span class="type">List</span>[<span class="type">Int</span>] = x.toList.map(y =&gt; y*y)</span><br><span class="line">  newList.toIterator</span><br><span class="line">&#125;)</span><br><span class="line">res2.foreach(x =&gt; println(x))</span><br><span class="line"></span><br><span class="line">println(<span class="string">&quot;------map输出----&quot;</span>)</span><br><span class="line">rdd2.map(x =&gt; &#123;</span><br><span class="line">  println(<span class="string">&quot;执行一次map操作&quot;</span>)</span><br><span class="line">  x</span><br><span class="line">&#125;).foreach(println)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------mapPartition-------------------</span><br><span class="line">------mapPartitions输出----</span><br><span class="line">执行了一次mapPartitions操作</span><br><span class="line">1</span><br><span class="line">4</span><br><span class="line">9</span><br><span class="line">16</span><br><span class="line">25</span><br><span class="line">执行了一次mapPartitions操作</span><br><span class="line">36</span><br><span class="line">49</span><br><span class="line">64</span><br><span class="line">81</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line">------map输出----</span><br><span class="line">执行一次map操作</span><br><span class="line">1</span><br><span class="line">执行一次map操作</span><br><span class="line">2</span><br><span class="line">执行一次map操作</span><br><span class="line">3</span><br><span class="line">执行一次map操作</span><br><span class="line">4</span><br><span class="line">执行一次map操作</span><br><span class="line">5</span><br><span class="line">执行一次map操作</span><br><span class="line">6</span><br><span class="line">执行一次map操作</span><br><span class="line">7</span><br><span class="line">执行一次map操作</span><br><span class="line">8</span><br><span class="line">执行一次map操作</span><br><span class="line">9</span><br><span class="line">执行一次map操作</span><br><span class="line">10</span><br></pre></td></tr></table></figure>

<h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h3><p>函数作用同mapPartitions，不过提供了两个参数，第一个参数为分区的索引。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">      preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">和map类似，只是多了一个 <span class="type">Int</span> 表示分区索引</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------mapPartitionsWithIndex-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> array5 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd5: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(array5, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * f: (Int, Iterator[String]) =&gt; Iterator[U]</span></span><br><span class="line"><span class="comment">  * 函数 f 中的 Int 就是分区编号，分区编号从 0 开始</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">val</span> res5: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd5.mapPartitionsWithIndex((index: <span class="type">Int</span>, data: <span class="type">Iterator</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">  println(<span class="string">&quot;当前处理分区编号为: &quot;</span> + index)</span><br><span class="line">  <span class="keyword">val</span> newList: <span class="type">List</span>[<span class="type">Int</span>] = data.map(x =&gt; x * x).toList</span><br><span class="line">  newList.toIterator</span><br><span class="line">&#125;)</span><br><span class="line">res5.foreach(x =&gt; println(x))</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------mapPartitionsWithIndex-------------------</span><br><span class="line">当前处理分区编号为: 0</span><br><span class="line">1</span><br><span class="line">4</span><br><span class="line">9</span><br><span class="line">当前处理分区编号为: 1</span><br><span class="line">16</span><br><span class="line">25</span><br><span class="line">36</span><br><span class="line">当前处理分区编号为: 2</span><br><span class="line">49</span><br><span class="line">64</span><br><span class="line">81</span><br><span class="line">100</span><br></pre></td></tr></table></figure>

<h2 id="3-2-组合其他RDD"><a href="#3-2-组合其他RDD" class="headerlink" title="3.2 组合其他RDD"></a>3.2 组合其他RDD</h2><h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><p>将两个RDD进行合并，不去重。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">other：求和的其他 <span class="type">RDD</span></span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------union-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> array1_1 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> array1_2 = <span class="type">Array</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line"><span class="keyword">val</span> rdd1_1 = sc.parallelize(array1_1)</span><br><span class="line"><span class="keyword">val</span> rdd1_2 = sc.parallelize(array1_2)</span><br><span class="line"><span class="keyword">val</span> res1: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd1_1.union(rdd1_2)</span><br><span class="line">res1.foreach(println)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------union-------------------</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td></tr></table></figure>

<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>join相当于SQL中的内关联join，根据key关联两个 RDD 的结果，只返回两个RDD key可以关联上的结果，关联不上的不输出，join只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="number">1.</span> <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br><span class="line"><span class="number">2.</span> <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br><span class="line"><span class="number">3.</span> <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br><span class="line"></span><br><span class="line">参数说明</span><br><span class="line">other		表示关联的其他 <span class="type">RDD</span></span><br><span class="line">numPartitions 表示关联后的分区数</span><br><span class="line">partitioner	 	表示关联的分区函数</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------join-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> list2_1 = <span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>),(<span class="string">&quot;d&quot;</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> list2_2 = <span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="string">&quot;hello&quot;</span>),(<span class="string">&quot;b&quot;</span>,<span class="string">&quot;hadoop&quot;</span>),(<span class="string">&quot;c&quot;</span>,<span class="string">&quot;spark&quot;</span>),(<span class="string">&quot;e&quot;</span>,<span class="string">&quot;flink&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> rdd2_1 = sc.parallelize(list2_1)</span><br><span class="line"><span class="keyword">val</span> rdd2_2 = sc.parallelize(list2_2)</span><br><span class="line"><span class="keyword">val</span> res2: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">String</span>))] = rdd2_1.join(rdd2_2)</span><br><span class="line">res2.foreach(x =&gt; println(x))</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------join-------------------</span><br><span class="line">(a,(1,hello))</span><br><span class="line">(b,(2,hadoop))</span><br><span class="line">(c,(3,spark))</span><br></pre></td></tr></table></figure>

<h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><p>cogroup相当于SQL中的全外关联full outer join，返回左右RDD中的记录，关联不上的用 null 表示。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"># 参数为一个<span class="type">RDD</span></span><br><span class="line"><span class="number">1.</span> <span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))]</span><br><span class="line"><span class="number">2.</span> <span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], partitioner: <span class="type">Partitioner</span>)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))]</span><br><span class="line"></span><br><span class="line"># 参数为两个<span class="type">RDD</span></span><br><span class="line"><span class="number">1.</span> <span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W1</span>, <span class="type">W2</span>](other1: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W1</span>)], other2: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W2</span>)])</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W1</span>], <span class="type">Iterable</span>[<span class="type">W2</span>]))]</span><br><span class="line"><span class="number">2.</span> <span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W1</span>, <span class="type">W2</span>](other1: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W1</span>)], other2: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W2</span>)], numPartitions: <span class="type">Int</span>)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W1</span>], <span class="type">Iterable</span>[<span class="type">W2</span>]))]</span><br><span class="line"><span class="number">3.</span> <span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W1</span>, <span class="type">W2</span>](other1: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W1</span>)], other2: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W2</span>)], partitioner: <span class="type">Partitioner</span>)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W1</span>], <span class="type">Iterable</span>[<span class="type">W2</span>]))]</span><br><span class="line"></span><br><span class="line"># 参数为三个<span class="type">RDD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W1</span>, <span class="type">W2</span>, <span class="type">W3</span>](other1: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W1</span>)], other2: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W2</span>)], other3: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W3</span>)])</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W1</span>], <span class="type">Iterable</span>[<span class="type">W2</span>], <span class="type">Iterable</span>[<span class="type">W3</span>]))]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W1</span>, <span class="type">W2</span>, <span class="type">W3</span>](other1: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W1</span>)],</span><br><span class="line">      other2: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W2</span>)],</span><br><span class="line">      other3: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W3</span>)],</span><br><span class="line">      numPartitions: <span class="type">Int</span>)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W1</span>], <span class="type">Iterable</span>[<span class="type">W2</span>], <span class="type">Iterable</span>[<span class="type">W3</span>]))]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W1</span>, <span class="type">W2</span>, <span class="type">W3</span>](other1: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W1</span>)],</span><br><span class="line">      other2: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W2</span>)],</span><br><span class="line">      other3: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W3</span>)],</span><br><span class="line">      partitioner: <span class="type">Partitioner</span>)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W1</span>], <span class="type">Iterable</span>[<span class="type">W2</span>], <span class="type">Iterable</span>[<span class="type">W3</span>]))]</span><br><span class="line"></span><br><span class="line">参数说明:</span><br><span class="line"><span class="type">RDD</span>[<span class="type">K</span>, <span class="type">W</span>] 	表示cogroup的<span class="type">RDD</span>对象</span><br><span class="line">numPartitions		用于指定结果的分区数</span><br><span class="line">partitioner		用于指定分区函数</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------cogroup-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> list3_1 = <span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>),(<span class="string">&quot;d&quot;</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> list3_2 = <span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="string">&quot;hello&quot;</span>),(<span class="string">&quot;b&quot;</span>,<span class="string">&quot;hadoop&quot;</span>),(<span class="string">&quot;c&quot;</span>,<span class="string">&quot;spark&quot;</span>),(<span class="string">&quot;e&quot;</span>,<span class="string">&quot;flink&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> rdd3_1 = sc.parallelize(list3_1)</span><br><span class="line"><span class="keyword">val</span> rdd3_2 = sc.parallelize(list3_2)</span><br><span class="line"><span class="keyword">val</span> res3: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">String</span>]))] = rdd3_1.cogroup(rdd3_2)</span><br><span class="line">res3.foreach(x =&gt; println(<span class="string">&quot;(&quot;</span> + x._1 + <span class="string">&quot;,(&quot;</span> + x._2._1.toList + <span class="string">&quot;,&quot;</span> + x._2._2.toList + <span class="string">&quot;))&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------cogroup-------------------</span><br><span class="line">(d,(List(4),List()))</span><br><span class="line">(e,(List(),List(flink)))</span><br><span class="line">(a,(List(1),List(hello)))</span><br><span class="line">(b,(List(2),List(hadoop)))</span><br><span class="line">(c,(List(3),List(spark)))</span><br></pre></td></tr></table></figure>

<h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><p>该函数返回两个RDD的交集，并去重。通过函数签名的方法1可以看到，intersection 底层调用 cogroup，按 key 进行关联，关联得上的返回 key，关联不上返回空的过滤掉。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：方法<span class="number">1</span>底层调用cogroup，方法<span class="number">2</span>底层调用方法<span class="number">3</span>，方法<span class="number">3</span>底层也是调用cogroup</span><br><span class="line"><span class="number">1.</span> <span class="function"><span class="keyword">def</span> <span class="title">intersection</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.map(v =&gt; (v, <span class="literal">null</span>)).cogroup(other.map(v =&gt; (v, <span class="literal">null</span>)))</span><br><span class="line">        .filter &#123; <span class="keyword">case</span> (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty &#125;</span><br><span class="line">        .keys</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">2.</span> <span class="function"><span class="keyword">def</span> <span class="title">intersection</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>], numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="number">3.</span> <span class="function"><span class="keyword">def</span> <span class="title">intersection</span></span>(</span><br><span class="line">      other: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      partitioner: <span class="type">Partitioner</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">other 表示求交集的目标<span class="type">RDD</span></span><br><span class="line">numPartitions		用于指定结果的分区数</span><br><span class="line">partitioner		用于指定分区函数</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------intersection-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> array4_1 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">8</span>)</span><br><span class="line"><span class="keyword">val</span> array4_2 = <span class="type">Array</span>(<span class="number">5</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>)</span><br><span class="line"><span class="keyword">val</span> rdd4_1 = sc.parallelize(array4_1)</span><br><span class="line"><span class="keyword">val</span> rdd4_2 = sc.parallelize(array4_2)</span><br><span class="line"><span class="keyword">val</span> res4: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd4_1.intersection(rdd4_2)</span><br><span class="line">res4.foreach(println)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------intersection-------------------</span><br><span class="line">6</span><br><span class="line">8</span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<h3 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h3><p>求RDD的笛卡尔积。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cartesian</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](other: <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">other 表示求笛卡尔积的目标<span class="type">RDD</span></span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------cartesian-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> array5_1 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> array5_2 = <span class="type">Array</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line"><span class="keyword">val</span> rdd5_1 = sc.parallelize(array5_1)</span><br><span class="line"><span class="keyword">val</span> rdd5_2 = sc.parallelize(array5_2)</span><br><span class="line"><span class="keyword">val</span> res5: <span class="type">RDD</span>[(<span class="type">Int</span>,<span class="type">Int</span>)] = rdd5_1.cartesian(rdd5_2)</span><br><span class="line">res5.foreach(println)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------cartesian-------------------</span><br><span class="line">(1,5)</span><br><span class="line">(1,6)</span><br><span class="line">(1,7)</span><br><span class="line">(1,8)</span><br><span class="line">(2,5)</span><br><span class="line">(2,6)</span><br><span class="line">(2,7)</span><br><span class="line">(2,8)</span><br><span class="line">(3,5)</span><br><span class="line">(3,6)</span><br><span class="line">(3,7)</span><br><span class="line">(3,8)</span><br><span class="line">(4,5)</span><br><span class="line">(4,6)</span><br><span class="line">(4,7)</span><br><span class="line">(4,8)</span><br></pre></td></tr></table></figure>

<h2 id="3-3-数据采样"><a href="#3-3-数据采样" class="headerlink" title="3.3 数据采样"></a>3.3 数据采样</h2><h3 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h3><p>对 RDD 中的数据进行采样。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">    withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">    fraction: <span class="type">Double</span>,</span><br><span class="line">    seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">withReplacement: <span class="type">Boolean</span>,  表示之前采样的样本是否放回</span><br><span class="line">fraction: <span class="type">Double</span>,   表示采样样本的数据占总数据的比例</span><br><span class="line">seed: <span class="type">Long</span>   表示采样规则的种子值</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------sample-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> list1 = (<span class="number">1</span> to <span class="number">100</span>).toList</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(list1)</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * withReplacement: Boolean,  表示之前采样的样本是否放回</span></span><br><span class="line"><span class="comment">  * fraction: Double,   表示采样样本的数据占总数据的比例</span></span><br><span class="line"><span class="comment">  * seed: Long   表示采样规则的种子值</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">val</span> res1: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd1.sample(<span class="literal">true</span>, <span class="number">0.05</span>, <span class="number">0</span>)</span><br><span class="line">res1.foreach(println)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------sample-------------------</span><br><span class="line">20</span><br><span class="line">45</span><br><span class="line">48</span><br><span class="line">67</span><br><span class="line">96</span><br></pre></td></tr></table></figure>

<h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>对RDD中的元素进行去重操作。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：方法<span class="number">1</span>底层调用方法<span class="number">2</span></span><br><span class="line"><span class="number">1.</span> <span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="number">2.</span> <span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">numPartitions 表示去重后的分区数。</span><br><span class="line"><span class="keyword">implicit</span> ord 表示隐式排序器，在 sortByKey 算子中也有这样的隐式排序器，可自定义</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------distinct-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> array6 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> rdd6: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(array6, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> res6: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd6.distinct(<span class="number">1</span>)</span><br><span class="line">println(<span class="string">&quot;初始分区数 = &quot;</span> + rdd6.partitions.size + <span class="string">&quot;; distince后分区数 = &quot;</span> + res6.partitions.size)</span><br><span class="line">res6.foreach(println)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------distinct-------------------</span><br><span class="line">初始分区数 = 2; distince后分区数 = 1</span><br><span class="line">4</span><br><span class="line">1</span><br><span class="line">3</span><br><span class="line">5</span><br><span class="line">2</span><br></pre></td></tr></table></figure>

<h2 id="3-4-数据重分布"><a href="#3-4-数据重分布" class="headerlink" title="3.4 数据重分布"></a>3.4 数据重分布</h2><h3 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h3><p>该函数用于将RDD进行重分区，使用HashPartitioner。coalesce 通常用来缩小分区，但也可以通过是否触发 shuffle 来扩大分区。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">               partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">              (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">numPartitions 表示 coalesce 后的分区数</span><br><span class="line">shuffle 表示是否触发 shuffle</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------coalesce-------------------&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> array1 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.parallelize(array1, <span class="number">4</span>)</span><br><span class="line">    <span class="keyword">val</span> res1_1 = rdd1.coalesce(<span class="number">2</span>).partitions.size</span><br><span class="line">    <span class="keyword">val</span> res1_2 = rdd1.coalesce(<span class="number">8</span>).partitions.size</span><br><span class="line">    <span class="keyword">val</span> res1_3 = rdd1.coalesce(<span class="number">8</span>, <span class="literal">true</span>).partitions.size</span><br><span class="line">    println(<span class="string">&quot;原始RDD分区:&quot;</span> + rdd1.partitions.size +</span><br><span class="line">      <span class="string">&quot;\n1. coalesce 不触发shuffle缩小分区: &quot;</span> + res1_1 +</span><br><span class="line">      <span class="string">&quot;\n2. coalesce 不触发shuffle扩大分区: &quot;</span> + res1_2 + <span class="string">&quot;(不shuffle无法扩大分区)&quot;</span> +</span><br><span class="line">      <span class="string">&quot;\n3. coalesce 触发shuffle扩大分区:&quot;</span> + res1_3)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------coalesce-------------------</span><br><span class="line">原始RDD分区:4</span><br><span class="line">1. coalesce 不触发shuffle缩小分区: 2</span><br><span class="line">2. coalesce 不触发shuffle扩大分区: 4(不shuffle无法扩大分区)</span><br><span class="line">3. coalesce 触发shuffle扩大分区:8</span><br></pre></td></tr></table></figure>

<h3 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h3><p>该函数其实就是coalesce函数第二个参数shuffle为true的实现。由于 repartition 默认会触发 shuffle 操作，所以既可以用来缩小分区，又可以用来扩大分区。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：底层调用 coalesce 的 shuffle 模式</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    coalesce(numPartitions, shuffle = <span class="literal">true</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">numPartitions 表示 repartition 后的分区数</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------repartition-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> array2 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(array2, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> res2_1 = rdd2.repartition(<span class="number">8</span>).partitions.size</span><br><span class="line"><span class="keyword">val</span> res2_2 = rdd2.repartition(<span class="number">2</span>).partitions.size</span><br><span class="line">println(<span class="string">&quot;原始RDD分区:&quot;</span> + rdd2.partitions.size +</span><br><span class="line">  <span class="string">&quot;\n1. repartition 扩大分区: &quot;</span> + res2_1 +</span><br><span class="line">  <span class="string">&quot;\n2. repartition 缩小分区: &quot;</span> + res2_2)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------repartition-------------------</span><br><span class="line">原始RDD分区:4</span><br><span class="line">1. repartition 扩大分区: 8</span><br><span class="line">2. repartition 缩小分区: 2</span><br></pre></td></tr></table></figure>

<h3 id="repartitionAndSortWithinPartitions"><a href="#repartitionAndSortWithinPartitions" class="headerlink" title="repartitionAndSortWithinPartitions"></a>repartitionAndSortWithinPartitions</h3><p>对 RDD 执行边 repartition 和 边 sort 的操作，这里需要自定义排序规则。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartitionAndSortWithinPartitions</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](self, partitioner).setKeyOrdering(ordering)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">partitioner: <span class="type">Partitioner</span> 表示自定义排序器</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  println(<span class="string">&quot;--------------------repartitionAndSortWithinPartitions-------------------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> list3: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((<span class="string">&quot;math&quot;</span>, <span class="number">90</span>), (<span class="string">&quot;english&quot;</span>, <span class="number">95</span>), (<span class="string">&quot;chinese&quot;</span>, <span class="number">88</span>),</span><br><span class="line">    (<span class="string">&quot;math&quot;</span>, <span class="number">95</span>), (<span class="string">&quot;english&quot;</span>, <span class="number">88</span>), (<span class="string">&quot;chinese&quot;</span>, <span class="number">82</span>))</span><br><span class="line">  <span class="keyword">val</span> rdd3 = sc.parallelize(list3,<span class="number">3</span>)</span><br><span class="line">  println(<span class="string">&quot;------打印原始分区信息------&quot;</span>)</span><br><span class="line">  rdd3.foreachPartition(x =&gt; println(x.toList))</span><br><span class="line"></span><br><span class="line">  println(<span class="string">&quot;------自定义分区器repartitionAndSortWithinPartitions后------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> res3: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd3.repartitionAndSortWithinPartitions(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">  res3.foreachPartition(x =&gt; println(x.toList))</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 自定义分区器</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span>(<span class="params">ptn: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 分区个数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = ptn</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 分区逻辑</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ptn_idx = math.abs(key.hashCode()) % ptn</span><br><span class="line">    ptn_idx</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------repartitionAndSortWithinPartitions-------------------</span><br><span class="line">------打印原始分区信息------</span><br><span class="line">List((math,90), (english,95))</span><br><span class="line">List((chinese,88), (math,95))</span><br><span class="line">List((english,88), (chinese,82))</span><br><span class="line">------自定义分区器repartitionAndSortWithinPartitions后------</span><br><span class="line">List((english,95), (english,88), (math,90), (math,95))</span><br><span class="line">List((chinese,88), (chinese,82))</span><br></pre></td></tr></table></figure>

<h2 id="3-5-单个RDD数据聚合"><a href="#3-5-单个RDD数据聚合" class="headerlink" title="3.5 单个RDD数据聚合"></a>3.5 单个RDD数据聚合</h2><h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><p>该函数用于将RDD[K,V]中每个K对应的V值，合并到一个集合Iterable[V]中。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：函数<span class="number">1</span>和<span class="number">2</span>底层都是调用<span class="number">3</span></span><br><span class="line"><span class="number">1.</span> <span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="number">2.</span> <span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="number">3.</span> <span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">numPartitions 用于指定分区数</span><br><span class="line">partitioner 用于指定分区函数</span><br></pre></td></tr></table></figure>

<p>程序例子：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  println(<span class="string">&quot;--------------------groupByKey-------------------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> list1: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((<span class="string">&quot;math&quot;</span>,<span class="number">90</span>),(<span class="string">&quot;english&quot;</span>, <span class="number">86</span>),(<span class="string">&quot;chinese&quot;</span>, <span class="number">80</span>),</span><br><span class="line">      (<span class="string">&quot;math&quot;</span>,<span class="number">96</span>),(<span class="string">&quot;english&quot;</span>, <span class="number">88</span>), (<span class="string">&quot;chinese&quot;</span>,<span class="number">82</span>))</span><br><span class="line">  <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(list1)</span><br><span class="line"></span><br><span class="line">  println(<span class="string">&quot;before groupby, rdd partition size = &quot;</span> + rdd1.partitions.size)</span><br><span class="line">  <span class="keyword">val</span> res1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = rdd1.groupByKey(<span class="number">2</span>)</span><br><span class="line">  println(<span class="string">&quot;after groupby, rdd partition size = &quot;</span> + res1.partitions.size)</span><br><span class="line">  </span><br><span class="line">println(<span class="string">&quot;----各个成绩分类汇总-----&quot;</span>)</span><br><span class="line">  res1.foreach(x =&gt; &#123;</span><br><span class="line">    println(x._1 + <span class="string">&quot;\t&quot;</span> + x._2.toList.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  println(<span class="string">&quot;----求各科平均分-----&quot;</span>)</span><br><span class="line">  res1.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> avgscore = x._2.sum.toDouble / x._2.toList.size</span><br><span class="line">    (x._1, avgscore)</span><br><span class="line">  &#125;).foreach(x =&gt; println(x._1 + <span class="string">&quot;\t&quot;</span> + x._2))</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--------------------groupByKey-------------------</span><br><span class="line">before groupby, rdd partition size = 1</span><br><span class="line">after groupby, rdd partition size = 2</span><br><span class="line">----各个成绩分类汇总-----</span><br><span class="line">math	90,96</span><br><span class="line">english	86,88</span><br><span class="line">chinese	80,82</span><br><span class="line">----求各科平均分-----</span><br><span class="line">math	93.0</span><br><span class="line">english	87.0</span><br><span class="line">chinese	81.0</span><br></pre></td></tr></table></figure>

<h3 id="reduceBykey"><a href="#reduceBykey" class="headerlink" title="reduceBykey"></a>reduceBykey</h3><p>该函数用于将RDD[K,V]中每个K对应的V值根据映射函数来运算。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：函数<span class="number">1</span>和<span class="number">2</span>底层都是调用<span class="number">3</span>，<span class="number">3</span>最终调用combineByKeyWithClassTag</span><br><span class="line"><span class="number">1.</span> <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="number">2.</span> <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>, numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="number">3.</span> <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span> 用于指定映射函数</span><br><span class="line">numPartitions 用于指定分区数</span><br><span class="line">partitioner 用于指定分区函数</span><br></pre></td></tr></table></figure>

<p>代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------reduceByKey-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> list2: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((<span class="string">&quot;math&quot;</span>,<span class="number">90</span>),(<span class="string">&quot;english&quot;</span>, <span class="number">86</span>),(<span class="string">&quot;chinese&quot;</span>, <span class="number">80</span>),</span><br><span class="line">  (<span class="string">&quot;math&quot;</span>,<span class="number">96</span>),(<span class="string">&quot;english&quot;</span>, <span class="number">88</span>), (<span class="string">&quot;chinese&quot;</span>,<span class="number">82</span>))</span><br><span class="line"><span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(list2)</span><br><span class="line"></span><br><span class="line">println(<span class="string">&quot;----各科成绩总和-----&quot;</span>)</span><br><span class="line">rdd2.reduceByKey((x,y) =&gt; &#123;</span><br><span class="line">  x+y</span><br><span class="line">&#125;).foreach(x =&gt; println(x._1 + <span class="string">&quot;\t&quot;</span> + x._2))</span><br><span class="line"></span><br><span class="line"><span class="comment">//.foreach(x =&gt; println(x._1 + &quot;\t&quot; + x._2))</span></span><br><span class="line">println(<span class="string">&quot;----求各科平均分-----&quot;</span>)</span><br><span class="line"><span class="comment">// reduceByKey对values处理，没法得到各科成绩数量</span></span><br><span class="line">rdd2.map(x =&gt; (x._1,(x._2.toInt, <span class="number">1</span>)))   <span class="comment">// score 添加计数器</span></span><br><span class="line">  .reduceByKey((a,b) =&gt; (a._1 + b._1, a._2 + b._2)) <span class="comment">// 对values(score,count) 分别求和，返回 (sumscore,totalcount)</span></span><br><span class="line">  .map(x =&gt; (x._1, x._2._1.toDouble / x._2._2))   <span class="comment">// avgscore = sumscore / totalcount</span></span><br><span class="line">  .foreach(x =&gt; println(x._1 + <span class="string">&quot;\t&quot;</span> + x._2))</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--------------------reduceByKey-------------------</span><br><span class="line">----各科成绩总和-----</span><br><span class="line">math	186</span><br><span class="line">chinese	162</span><br><span class="line">english	174</span><br><span class="line">----求各科平均分-----</span><br><span class="line">math	93.0</span><br><span class="line">chinese	81.0</span><br><span class="line">english	87.0</span><br></pre></td></tr></table></figure>

<h3 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h3><p>aggregateByKey对PairRDD中相同的Key值进行聚合操作，函数最终返回的类型还是PairRDD，对应的结果是Key和聚合后的值，而aggregate函数直接返回的是非RDD的结果。（<strong>可以先看下文 Action 算子 aggregate 详解，会有助于理解 aggregateByKey</strong>）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line">def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U,</span><br><span class="line">      combOp: (U, U) =&gt; U): RDD[(K, U)]</span><br><span class="line">     </span><br><span class="line">def aggregateByKey[U: ClassTag](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) =&gt; U,</span><br><span class="line">      combOp: (U, U) =&gt; U): RDD[(K, U)]</span><br><span class="line">      </span><br><span class="line">def aggregateByKey[U: ClassTag](zeroValue: U, numPartitions: Int)(seqOp: (U, V) =&gt; U,</span><br><span class="line">      combOp: (U, U) =&gt; U): RDD[(K, U)]</span><br><span class="line">      </span><br><span class="line">参数介绍：（参数详细介绍可参考 aggregate 算子）</span><br><span class="line">zeroValue: U	表示聚合的初始值</span><br><span class="line">seqOp: (U, T) =&gt; U	表示迭代操作，拿RDD中每个元素与初始值进行合并</span><br><span class="line">combOp: (U, U) =&gt; U	表示分区结果数据的最终合并</span><br><span class="line">partitioner: Partitioner)	用于指定分区函数</span><br><span class="line">numPartitions: Int	用于指定分区属</span><br></pre></td></tr></table></figure>

<p>程序例子：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------aggregateByKey-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> list3: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((<span class="string">&quot;math&quot;</span>,<span class="number">90</span>),(<span class="string">&quot;english&quot;</span>, <span class="number">86</span>),(<span class="string">&quot;chinese&quot;</span>, <span class="number">80</span>),</span><br><span class="line">  (<span class="string">&quot;math&quot;</span>,<span class="number">96</span>),(<span class="string">&quot;english&quot;</span>, <span class="number">88</span>), (<span class="string">&quot;chinese&quot;</span>,<span class="number">82</span>))</span><br><span class="line"><span class="keyword">val</span> rdd3 = sc.parallelize(list3)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * aggregate：针对RDD的单个元素进行聚合</span></span><br><span class="line"><span class="comment">  * aggregate = groupByKey + aggregate：对每个 group 的 values 进行聚合。</span></span><br><span class="line"><span class="comment">  *           针对 key-value 类型的 RDD，对每一组的 values 进行 aggreage 操作。</span></span><br><span class="line"><span class="comment">  *           因此，聚合逻辑可以直接 copy aggreate 的传参</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">val</span> res3: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = rdd3.aggregateByKey((<span class="number">0</span>, <span class="number">0</span>))((u: (<span class="type">Int</span>, <span class="type">Int</span>), y) =&gt; (u._1 + y, u._2 + <span class="number">1</span>),</span><br><span class="line">  (x: (<span class="type">Int</span>, <span class="type">Int</span>), y: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (x._1 + y._1, x._2 + y._2))</span><br><span class="line"></span><br><span class="line">println(<span class="string">&quot;----求各科平均分-----&quot;</span>)</span><br><span class="line">res3.map(x =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> avscore = x._2._1.toDouble / x._2._2</span><br><span class="line">  (x._1, avscore)</span><br><span class="line">&#125;).foreach(x =&gt; println(x._1 + <span class="string">&quot;\t&quot;</span> + x._2))</span><br></pre></td></tr></table></figure>

<p>结果输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">----求各科平均分-----</span><br><span class="line">math	93.0</span><br><span class="line">chinese	81.0</span><br><span class="line">english	87.0</span><br></pre></td></tr></table></figure>

<h3 id="sortByKey-sortBy"><a href="#sortByKey-sortBy" class="headerlink" title="sortByKey/sortBy"></a>sortByKey/sortBy</h3><p>sortBy是对标准的RDD进行排序，sortByKey函数作用于Key-Value形式的PairRDD，并对Key进行排序。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line">sortBy</span><br><span class="line">def sortBy[K](</span><br><span class="line">  f: (T) =&gt; K,</span><br><span class="line">  ascending: Boolean = true,</span><br><span class="line">  numPartitions: Int = this.partitions.length)</span><br><span class="line">  (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope &#123;</span><br><span class="line">    this.keyBy[K](f)</span><br><span class="line">	    .sortByKey(ascending, numPartitions)</span><br><span class="line">	    .values</span><br><span class="line">&#125;</span><br><span class="line"> 参数说明：</span><br><span class="line"> f: (T) =&gt; K  表示排序的规则，即自定义排序</span><br><span class="line"></span><br><span class="line">sortByKey</span><br><span class="line">def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)</span><br><span class="line">  : RDD[(K, V)] = self.withScope</span><br><span class="line">&#123;</span><br><span class="line">	val part = new RangePartitioner(numPartitions, self, ascending)</span><br><span class="line">	new ShuffledRDD[K, V, V](self, part)</span><br><span class="line">	  .setKeyOrdering(if (ascending) ordering else ordering.reverse)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>根据参数可以看到 sortBy 可以自定义排序，而 sortByKey 参数默认并没有指定排序规则的参数，是不是就不能进行自定义排序呢？答案是可以的，在 sortByKey 内部实现中有一个 setKeyOrdering 方法，方法就是对 OrderedRDDFunctions 类的变量 ordering 进行设置，它就是默认的排序规则，可以通过重写该排序规则实现 sortByKey 的自定义排序，具体实现我们后面再单独讨论。</p>
<p>程序例子：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  println(<span class="string">&quot;--------------------sortByKey-------------------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> list4: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((<span class="string">&quot;math&quot;</span>,<span class="number">90</span>),(<span class="string">&quot;english&quot;</span>, <span class="number">95</span>),(<span class="string">&quot;chinese&quot;</span>, <span class="number">88</span>),</span><br><span class="line">    (<span class="string">&quot;math&quot;</span>,<span class="number">95</span>),(<span class="string">&quot;english&quot;</span>, <span class="number">88</span>), (<span class="string">&quot;chinese&quot;</span>,<span class="number">82</span>))</span><br><span class="line">  <span class="keyword">val</span> rdd4 = sc.parallelize(list4)</span><br><span class="line"></span><br><span class="line">  println(<span class="string">&quot;----sortBy按成绩降序排序-----&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> res4: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd4.sortBy(x =&gt; x._2, <span class="literal">false</span>)</span><br><span class="line">  res4.foreach(x =&gt; println(x))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 按成绩降序排序（问题：如何先按成绩降序，再按科目生序排序呢？即如何实现二次排序？ sortBy 可以指定多个排序变量）</span></span><br><span class="line">  println(<span class="string">&quot;----sortBy按成绩降序，并按科目降序-----&quot;</span>)</span><br><span class="line">  rdd4.sortBy(x =&gt; (x._2, x._1),<span class="literal">false</span>).foreach(println)</span><br><span class="line"></span><br><span class="line">  println(<span class="string">&quot;----sortByKey按成绩降序排序-----&quot;</span>)</span><br><span class="line">  <span class="comment">// 默认按 Key 排序，所以先要将 &lt;k,v&gt; 位置交换后 sortByKey 后再交换回来</span></span><br><span class="line">  <span class="keyword">val</span> res4_1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd4.map(x =&gt; (x._2, x._1))</span><br><span class="line">                                        .sortByKey(<span class="literal">false</span>)</span><br><span class="line">                                        .map(x =&gt; (x._2, x._1))</span><br><span class="line">  <span class="comment">// 问题：排序方式是先排序分区数据，再对多个分区归并排序？还是直接全局归并排序？</span></span><br><span class="line">  res4_1.foreach(x =&gt; println(x))</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------sortByKey-------------------</span><br><span class="line">----sortBy按成绩降序排序-----</span><br><span class="line">(english,95)</span><br><span class="line">(math,95)</span><br><span class="line">(math,90)</span><br><span class="line">(chinese,88)</span><br><span class="line">(english,88)</span><br><span class="line">(chinese,82)</span><br><span class="line">----sortBy按成绩降序，并按科目降序-----</span><br><span class="line">(math,95)</span><br><span class="line">(english,95)</span><br><span class="line">(math,90)</span><br><span class="line">(english,88)</span><br><span class="line">(chinese,88)</span><br><span class="line">(chinese,82)</span><br><span class="line">----sortByKey按成绩降序排序-----</span><br><span class="line">(english,95)</span><br><span class="line">(math,95)</span><br><span class="line">(math,90)</span><br><span class="line">(chinese,88)</span><br><span class="line">(english,88)</span><br><span class="line">(chinese,82)</span><br></pre></td></tr></table></figure>

<h1 id="4-Action算子"><a href="#4-Action算子" class="headerlink" title="4. Action算子"></a>4. Action算子</h1><h2 id="4-1-收集数据到Driver"><a href="#4-1-收集数据到Driver" class="headerlink" title="4.1 收集数据到Driver"></a>4.1 收集数据到Driver</h2><h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>用于将一个RDD转换成数组。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] </span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------collect-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line">rdd1.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------collect-------------------</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td></tr></table></figure>

<h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><p>返回RDD中的第一个元素，不排序。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：first()底层调用take(<span class="number">1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------first-------------------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line">  <span class="keyword">val</span> res2: <span class="type">Int</span> = rdd2.first()</span><br><span class="line">  println(<span class="string">&quot;first 返回值 = &quot;</span> + res2)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------first-------------------</span><br><span class="line">first 返回值 = 1</span><br></pre></td></tr></table></figure>

<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><p>用于获取RDD中从0到num-1下标的元素，不排序。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------take-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">val</span> res3: <span class="type">Array</span>[<span class="type">Int</span>] = rdd3.take(<span class="number">4</span>)</span><br><span class="line">println(<span class="string">&quot;take(4) 返回值 = &quot;</span> + res3.toList)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------take-------------------</span><br><span class="line">take(4) 返回值 = List(1, 2, 3, 4)</span><br></pre></td></tr></table></figure>

<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>返回RDD中的元素数量。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span></span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------count-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd4 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">val</span> res4: <span class="type">Long</span> = rdd4.count()</span><br><span class="line">println(<span class="string">&quot;count 返回值 = &quot;</span> + res4)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------count-------------------</span><br><span class="line">count 返回值 = 8</span><br></pre></td></tr></table></figure>

<h3 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample"></a>takeSample</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeSample</span></span>(</span><br><span class="line">    withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">    num: <span class="type">Int</span>,</span><br><span class="line">    seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">Array</span>[<span class="type">T</span>] </span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">withReplacement: <span class="type">Boolean</span>,  表示之前采样的样本是否放回</span><br><span class="line">fraction: <span class="type">Double</span>,   表示采样样本的数据占总数据的比例</span><br><span class="line">seed: <span class="type">Long</span>   表示采样规则的种子值</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------takeSample-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd5 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">val</span> res5: <span class="type">Array</span>[<span class="type">Int</span>] = rdd5.takeSample(<span class="literal">true</span>, <span class="number">5</span>, <span class="number">0</span>)</span><br><span class="line">println(<span class="string">&quot;takeSample(5) 返回值 = &quot;</span> + res5.toList)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------takeSample-------------------</span><br><span class="line">takeSample(5) 返回值 = List(1, 3, 8, 7, 2)</span><br></pre></td></tr></table></figure>

<h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><p>和 take 作用类似，只是可以指定 ordering 排序规则。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">num 表示取数的数目</span><br><span class="line">ord 表示隐式排序规则</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">    println(<span class="string">&quot;--------------------takeOrder-------------------&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd6 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line">    <span class="comment">// 指定排序规则，ordering1 和 ordering2 效果一样，都是逆序排序</span></span><br><span class="line"><span class="comment">//    implicit val ordering1 = Ordering[Int].on[Int](x =&gt; -x)</span></span><br><span class="line">    <span class="keyword">implicit</span> <span class="keyword">val</span> ordering2 = implicitly[<span class="type">Ordering</span>[<span class="type">Int</span>]].reverse</span><br><span class="line">    <span class="keyword">val</span> res6: <span class="type">Array</span>[<span class="type">Int</span>] = rdd6.takeOrdered(<span class="number">4</span>)</span><br><span class="line">    println(<span class="string">&quot;takeOrdered(4) 返回值 = &quot;</span> + res6.toList)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------takeOrder-------------------</span><br><span class="line">takeOrdered(4) 返回值 = List(8, 7, 6, 5)</span><br></pre></td></tr></table></figure>

<h2 id="4-2-数据持久化到存储系统"><a href="#4-2-数据持久化到存储系统" class="headerlink" title="4.2 数据持久化到存储系统"></a>4.2 数据持久化到存储系统</h2><h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><p>saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>, codec: <span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>]): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">path 表示要保存的文件系统路径</span><br><span class="line">codec 表示指定的压缩类名</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------saveAsTextFile-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line">rdd1.saveAsTextFile(<span class="string">&quot;/Users/11085245/Documents/code/personal/data/1&quot;</span>,</span><br><span class="line">                    classOf[com.hadoop.compression.lzo.<span class="type">LzopCodec</span>])</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">本地文件系统数据：</span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -l /Users/11085245/Documents/code/personal/data/1</span></span><br><span class="line">total 8</span><br><span class="line">-rw-r--r--  1 11085245  707420648   0 11 21 17:41 _SUCCESS</span><br><span class="line">-rw-r--r--  1 11085245  707420648  66 11 21 17:41 part-00000.lzo</span><br></pre></td></tr></table></figure>

<h3 id="saveAsSequenceFile"><a href="#saveAsSequenceFile" class="headerlink" title="saveAsSequenceFile"></a>saveAsSequenceFile</h3><p>saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到文件系统中。注意，saveAsSequenceFile 针对的对象为 &lt;key, value&gt; 类型的 RDD是，普通 RDD 不能直接调用该 API。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsSequenceFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    codec: <span class="type">Option</span>[<span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>]] = <span class="type">None</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">path 表示要保存的文件系统路径</span><br><span class="line">codec 表示指定的压缩类名</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------saveAsSequenceFile-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="string">&quot;hello&quot;</span>),(<span class="string">&quot;b&quot;</span>,<span class="string">&quot;hadoop&quot;</span>),(<span class="string">&quot;c&quot;</span>,<span class="string">&quot;spark&quot;</span>),(<span class="string">&quot;e&quot;</span>,<span class="string">&quot;flink&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(list2)</span><br><span class="line">rdd2.saveAsSequenceFile(<span class="string">&quot;/Users/11085245/Documents/code/personal/data/2&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">本地文件系统数据：</span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -l /Users/11085245/Documents/code/personal/data/2</span></span><br><span class="line">total 8</span><br><span class="line">-rw-r--r--  1 11085245  707420648    0 11 21 17:41 _SUCCESS</span><br><span class="line">-rw-r--r--  1 11085245  707420648  143 11 21 17:41 part-00000</span><br></pre></td></tr></table></figure>

<h3 id="saveAsObjectFile"><a href="#saveAsObjectFile" class="headerlink" title="saveAsObjectFile"></a>saveAsObjectFile</h3><p>saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：（底层调用 saveAsSequenceFile <span class="type">API</span>）</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">saveAsObjectFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.mapPartitions(iter =&gt; iter.grouped(<span class="number">10</span>).map(_.toArray))</span><br><span class="line">      .map(x =&gt; (<span class="type">NullWritable</span>.get(), <span class="keyword">new</span> <span class="type">BytesWritable</span>(<span class="type">Utils</span>.serialize(x))))</span><br><span class="line">      .saveAsSequenceFile(path)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">path 表示要保存的文件系统路径</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------saveAsObjectFile-------------------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line">  rdd3.saveAsObjectFile(<span class="string">&quot;/Users/11085245/Documents/code/personal/data/3&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">本地文件系统数据：</span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -l /Users/11085245/Documents/code/personal/data/3</span></span><br><span class="line">total 8</span><br><span class="line">-rw-r--r--  1 11085245  707420648    0 11 21 17:41 _SUCCESS</span><br><span class="line">-rw-r--r--  1 11085245  707420648  166 11 21 17:41 part-00000</span><br></pre></td></tr></table></figure>

<h2 id="4-3-聚合操作"><a href="#4-3-聚合操作" class="headerlink" title="4.3 聚合操作"></a>4.3 聚合操作</h2><h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p>将每个分区里面的元素进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line">def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U</span><br><span class="line"></span><br><span class="line">参数介绍：</span><br><span class="line">zeroValue: U	表示聚合的初始值</span><br><span class="line">seqOp: (U, T) =&gt; U	表示迭代操作，拿RDD中每个元素与初始值进行合并</span><br><span class="line">combOp: (U, U) =&gt; U	表示分区结果数据的最终合并</span><br><span class="line"></span><br><span class="line">其中：</span><br><span class="line">1. seqOp操作会聚合各分区中的元素，然后combOp操作把所有分区的聚合结果再次聚合，两个操作的初始值都是zeroValue。</span><br><span class="line">2. seqOp的操作是遍历分区中的所有元素(T)，第一个T跟zeroValue(U)做操作，结果(U)再作为与第二个T做操作的zeroValue，直到遍历完整个分区。</span><br><span class="line">3. combOp操作是把各分区聚合的结果，再聚合。</span><br></pre></td></tr></table></figure>

<p>怎么理解呢？在写代码前先来看一个文字叙述过程。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">假设有三个分区。分区0: (1,2,3,4), 分区1: (5,6,7,8), 分区2: (9,10)</span><br><span class="line">rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8,9,10), 3)</span><br><span class="line">调用：rdd.aggregate((0, 0))((u: (Int, Int), y) =&gt; (u._1 + y, u._2 + 1),</span><br><span class="line">      (x: (Int, Int), y: (Int, Int)) =&gt; (x._1 + y._1, x._2 + y._2))</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> seqOp聚合各个分区中的元素</span></span><br><span class="line">针对分区0:(1,2,3,4)</span><br><span class="line">初始值：(0,0)</span><br><span class="line">分区0第一个元素：(0+1, 0+1) = (1,1)</span><br><span class="line">分区0第二个元素：(1+2, 1+1) = (3,2)</span><br><span class="line">分区0第三个元素：(3+3, 2+1) = (6,3)</span><br><span class="line">分区0第四个元素：(6+4, 3+1) = (10,4)</span><br><span class="line"></span><br><span class="line">针对分区1:(5,6,7,8)</span><br><span class="line">初始值：(0,0)</span><br><span class="line">分区1第一个元素：(0+5, 0+1) = (5,1)</span><br><span class="line">分区1第二个元素：(5+6, 1+1) = (11,2)</span><br><span class="line">分区1第三个元素：(11+7, 2+1) = (18,3)</span><br><span class="line">分区1第四个元素：（18+8, 3+1) = (26,4)</span><br><span class="line"></span><br><span class="line">针对分区2:(9,10)</span><br><span class="line">初始值：(0,0)</span><br><span class="line">分区2第一个元素：(0+9, 0+1) = (9,1)</span><br><span class="line">分区2第二个元素：(9+10, 1+1) = (19,2)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> combOp把所有分区的聚合结果再次聚合</span></span><br><span class="line">第一次聚合：合并分区0和分区1: (10,4) + (26,4) = (36,8)</span><br><span class="line">第二次聚合：合并上次结果和分区2: (36,8) + (19,2) = (55,10)</span><br></pre></td></tr></table></figure>

<p>程序例子：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  println(<span class="string">&quot;--------------------aggregate-------------------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> array1 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>)</span><br><span class="line">  <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(array1, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * (zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U)</span></span><br><span class="line"><span class="comment">    * zeroValue: U   聚合的初始值</span></span><br><span class="line"><span class="comment">    * seqOp: (U, Int) =&gt; U   迭代操作，拿RDD中的每个元素跟初始值进行合并</span></span><br><span class="line"><span class="comment">    * combOp: (U, U) =&gt; U   分区结果数据最终合并</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  println(<span class="string">&quot;---------求和----------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> sum: <span class="type">Int</span> = rdd1.aggregate(<span class="number">0</span>)((x: <span class="type">Int</span>, y: <span class="type">Int</span>) =&gt; x+y,</span><br><span class="line">    (x: <span class="type">Int</span>, y: <span class="type">Int</span>) =&gt; x+y)</span><br><span class="line">  println(<span class="string">&quot;总和:&quot;</span> + sum)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * seqOp: (U, Int) =&gt; U</span></span><br><span class="line"><span class="comment">    *     (10,4) + 5 = (10,4) + (5,1) = (15,5)</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  println(<span class="string">&quot;---------求平均值----------&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> res1: (<span class="type">Int</span>, <span class="type">Int</span>) = rdd1.aggregate((<span class="number">0</span>, <span class="number">0</span>))((u: (<span class="type">Int</span>, <span class="type">Int</span>), y) =&gt; (u._1 + y, u._2 + <span class="number">1</span>),</span><br><span class="line">    (x: (<span class="type">Int</span>, <span class="type">Int</span>), y: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (x._1 + y._1, x._2 + y._2))</span><br><span class="line">  <span class="keyword">val</span> avgscore: <span class="type">Double</span> = res1._1.toDouble / res1._2</span><br><span class="line">  println(<span class="string">&quot;平均分: &quot;</span> + avgscore)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------aggregate-------------------</span><br><span class="line">---------求和----------</span><br><span class="line">总和:55</span><br><span class="line">---------求平均值----------</span><br><span class="line">平均分: 5.5</span><br></pre></td></tr></table></figure>



<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><p>根据映射函数f，对RDD中的元素进行二元计算，返回计算结果。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line">def reduce(f: (T, T) =&gt; T): T</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------reduce-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> array2 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(array2, <span class="number">2</span>)</span><br><span class="line">println(<span class="string">&quot;---------求平均值----------&quot;</span>)</span><br><span class="line"><span class="comment">// 求和前先将用map每个元素的count计为1</span></span><br><span class="line"><span class="keyword">val</span> res2: (<span class="type">Int</span>, <span class="type">Int</span>) = rdd2.map(x =&gt; (x, <span class="number">1</span>)).reduce((x: (<span class="type">Int</span>, <span class="type">Int</span>), y: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; &#123;</span><br><span class="line">  (x._1 + y._1, x._2 + y._2)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">val</span> avgscore2 = res2._1.toDouble / res2._2</span><br><span class="line">println(<span class="string">&quot;平均值：&quot;</span> + avgscore2)</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------reduce-------------------</span><br><span class="line">---------求平均值----------</span><br><span class="line">平均值：5.5</span><br></pre></td></tr></table></figure>

<h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>对 &lt;key, value&gt; 类型的RDD按 key 统计。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByKey</span></span>(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Long</span>]</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">无参，返回<span class="type">Map</span>类型键值对</span><br></pre></td></tr></table></figure>

<p>程序例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;--------------------countByKey-------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> list3 = <span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="string">&quot;hello&quot;</span>),(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;hadoop&quot;</span>),(<span class="string">&quot;c&quot;</span>,<span class="string">&quot;spark1&quot;</span>),(<span class="string">&quot;c&quot;</span>,<span class="string">&quot;spark2&quot;</span>),(<span class="string">&quot;e&quot;</span>,<span class="string">&quot;flink&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> rdd3 = sc.parallelize(list3)</span><br><span class="line"><span class="keyword">val</span> res3: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = rdd3.countByKey()</span><br><span class="line">res3.foreach(x =&gt; println(x))</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--------------------countByKey-------------------</span><br><span class="line">(e,1)</span><br><span class="line">(a,2)</span><br><span class="line">(c,2)</span><br></pre></td></tr></table></figure>

<h2 id="4-4-函数式编程辅助操作"><a href="#4-4-函数式编程辅助操作" class="headerlink" title="4.4 函数式编程辅助操作"></a>4.4 函数式编程辅助操作</h2><h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><p>foreach用于遍历RDD，将映射函数应用于每一个元素，前面算子输出结果用得比较多，这里就不展开介绍。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">函数签名：</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">f: <span class="type">T</span> =&gt; <span class="type">Unit</span>  表示传入的映射函数，返回类型为 <span class="type">Unit</span></span><br></pre></td></tr></table></figure>

<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><p><a href="https://spark.apache.org/docs/2.4.7/rdd-programming-guide.html#transformations">Spark官网</a></p>
</li>
<li><p><a href="http://lxw1234.com/archives/2015/07/363.htm">Spark算子系列文章</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>YARN ApplicationMaster启动原理与源码分析</title>
    <url>/2021/11/07/YARN-ApplicationMaster%E5%90%AF%E5%8A%A8%E5%8E%9F%E7%90%86%E4%B8%8E%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>本文主要介绍 ApplicationMaster(简称 AM) 的运行流程，并从 AM 的启动、注册/心跳、Container 资源申请与分配三个角度分析相关源码。其中花了大量篇幅介绍 AM 的启动过程，包括任务提交流程、App/Attempt 转换过程，到 AM 的启动，这部分主要是方便读者了解从应用程序提交到启动 AM 再到 AM 申请 Container 运行整个过程，对 YARN 的提交流程有更深入的理解。</p>
<h1 id="1-AM整体架构"><a href="#1-AM整体架构" class="headerlink" title="1. AM整体架构"></a>1. AM整体架构</h1><p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/07/2b3f627b3d8e6055d6964830a07d05df-1372882-20200826172820723-885116646-20211107233952482-e969ea.png" alt="img"></p>
<center>ApplicationMaster 生命周期</center>

<p>ApplicationMaster（AM）管理主要由三个服务构成，分别是 ApplicationMasterLauncher、AMLivelinessMonitor 和 ApplicationMasterService，它们共同管理应用程序的 AM 的生命周期。AM 服务从创建到销毁的流程如下：</p>
<ol>
<li>用户向 ResourceManager 提交应用程序，ResourceManager 收到提交请求后，先向资源调度器申请用以启动 ApplicationMaster 的资源，待申请到资源后，再由 ApplicationMasterLauncher 与对应的 NodeManager 通信，从而启动应用程序的 ApplicationMaster。</li>
<li>ApplicationMaster 启动完成后，ApplicationMasterLauncher 会通过事件的形式，将刚刚启动的 ApplicationMaster 注册到 AMLivelinessMonitor，以启动心跳监控。</li>
<li>ApplicationMaster 启动后，先向 ApplicationMasterService 注册，将自己所在 host、端口号等信息汇报给它。</li>
<li>ApplicationMaster 运行过程中，周期性地向 ApplicationMasterService 汇报“心跳”信息（“心跳”信息中包含想要申请的资源描述）。</li>
<li>ApplicationMasterService 每次收到 ApplicationMaster 的心跳信息后，将通知 AMLivelinessMonitor 更新该应用程序的最近汇报心跳的时间。</li>
<li>当应用程序运行完成后，ApplicationMaster 向 ApplicationMasterService 发送请求，注销自己。</li>
<li>ApplicationMasterService 收到注销请求后，标注应用程序运行状态为完成，同时通知 AMLivelinessMonitor 移除对它的心跳监控。</li>
</ol>
<p>结合 ApplicationMaster 的整体生命周期，我们从 ApplicatioMaster 启动、注册/心跳及资源申请三个角度来剖析相关源码。</p>
<h1 id="2-AM-Container启动"><a href="#2-AM-Container启动" class="headerlink" title="2. AM Container启动"></a>2. AM Container启动</h1><p>这部分主要介绍 AM 生命周期的第一步，即 AM 的启动（AM 的启动本质也是 YARN 的一个 Container 容器）。为了方便理解整个任务执行流程，我们不直接分析 AM 的启动类，而是从应用程序提交，到 APP/Attempt 状态转换（AM 启动前应用程序的一些状态转换过程），再到具体的 AM 启动，以对 YARN 的整个任务提交流程有更深的了解。</p>
<h2 id="2-1-APP提交"><a href="#2-1-APP提交" class="headerlink" title="2.1 APP提交"></a>2.1 APP提交</h2><p>不管是什么类型的应用程序，提交到 YARN 上的入口，都是通过 YARNClient 这个接口 api 提交的，具体提交方法为 submitApplication()。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/YARNClient.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> ApplicationId <span class="title">submitApplication</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationSubmissionContext appContext)</span> <span class="keyword">throws</span> YARNException,</span></span><br><span class="line"><span class="function">      IOException</span>;</span><br></pre></td></tr></table></figure>

<p>看看其实现类的提交入口：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/impl/YARNClientImpl.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> ApplicationId</span></span><br><span class="line"><span class="function">      <span class="title">submitApplication</span><span class="params">(ApplicationSubmissionContext appContext)</span></span></span><br><span class="line"><span class="function">          <span class="keyword">throws</span> YARNException, IOException </span>&#123;</span><br><span class="line">    ApplicationId applicationId = appContext.getApplicationId();</span><br><span class="line">    <span class="keyword">if</span> (applicationId == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> ApplicationIdNotProvidedException(</span><br><span class="line">          <span class="string">&quot;ApplicationId is not provided in ApplicationSubmissionContext&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构建应用程序请求的上文文信息</span></span><br><span class="line">    SubmitApplicationRequest request =</span><br><span class="line">        Records.newRecord(SubmitApplicationRequest.class);</span><br><span class="line">    request.setApplicationSubmissionContext(appContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Automatically add the timeline DT into the CLC</span></span><br><span class="line">    <span class="comment">// Only when the security and the timeline service are both enabled</span></span><br><span class="line">    <span class="keyword">if</span> (isSecurityEnabled() &amp;&amp; timelineServiceEnabled) &#123;</span><br><span class="line">      addTimelineDelegationToken(appContext.getAMContainerSpec());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Client 真正提交应用程序</span></span><br><span class="line">    rmClient.submitApplication(request);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;  </span><br><span class="line">      <span class="comment">// 对未能及时提交的应用程序不断重试</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> applicationId;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>YARN Client 与 RM 进行 RPC 通信是通过 ClientRMService 服务实现的，应用程序提交到服务端，会调用 RMAppManager 类的对应方法来处理应用程序。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> SubmitApplicationResponse <span class="title">submitApplication</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      SubmitApplicationRequest request)</span> <span class="keyword">throws</span> YARNException </span>&#123;</span><br><span class="line">    ApplicationSubmissionContext submissionContext = request</span><br><span class="line">        .getApplicationSubmissionContext();</span><br><span class="line">    ApplicationId applicationId = submissionContext.getApplicationId();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 跳过神圣的检查工作</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 重点：调用 RMAppManager 来提交应用程序</span></span><br><span class="line">      rmAppManager.submitApplication(submissionContext,</span><br><span class="line">          System.currentTimeMillis(), user);</span><br><span class="line"></span><br><span class="line">      LOG.info(<span class="string">&quot;Application with id &quot;</span> + applicationId.getId() + </span><br><span class="line">          <span class="string">&quot; submitted by user &quot;</span> + user);</span><br><span class="line">      RMAuditLogger.logSuccess(user, AuditConstants.SUBMIT_APP_REQUEST,</span><br><span class="line">          <span class="string">&quot;ClientRMService&quot;</span>, applicationId);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (YARNException e) &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Exception in submitting application with id &quot;</span> +</span><br><span class="line">          applicationId.getId(), e);</span><br><span class="line">      RMAuditLogger.logFailure(user, AuditConstants.SUBMIT_APP_REQUEST,</span><br><span class="line">          e.getMessage(), <span class="string">&quot;ClientRMService&quot;</span>,</span><br><span class="line">          <span class="string">&quot;Exception in submitting application&quot;</span>, applicationId);</span><br><span class="line">      <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    SubmitApplicationResponse response = recordFactory</span><br><span class="line">        .newRecordInstance(SubmitApplicationResponse.class);</span><br><span class="line">    <span class="keyword">return</span> response;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-2-APP-AppAttempt状态转换"><a href="#2-2-APP-AppAttempt状态转换" class="headerlink" title="2.2 APP/AppAttempt状态转换"></a>2.2 APP/AppAttempt状态转换</h2><p>从 RMAppManager 类的 rmAppManager.submitApplication() 方法，可以看到它向调度器发送 RMAppEventType.START 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">submitApplication</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationSubmissionContext submissionContext, <span class="keyword">long</span> submitTime,</span></span></span><br><span class="line"><span class="params"><span class="function">      String user)</span> <span class="keyword">throws</span> YARNException </span>&#123;</span><br><span class="line">    ApplicationId applicationId = submissionContext.getApplicationId();</span><br><span class="line"></span><br><span class="line">    RMAppImpl application =</span><br><span class="line">        createAndPopulateNewRMApp(submissionContext, submitTime, user, <span class="keyword">false</span>);</span><br><span class="line">    ApplicationId appId = submissionContext.getApplicationId();</span><br><span class="line">    Credentials credentials = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      credentials = parseCredentials(submissionContext);</span><br><span class="line">      <span class="keyword">if</span> (UserGroupInformation.isSecurityEnabled()) &#123;</span><br><span class="line">        <span class="keyword">this</span>.rmContext.getDelegationTokenRenewer().addApplicationAsync(appId,</span><br><span class="line">            credentials, submissionContext.getCancelTokensWhenComplete(),</span><br><span class="line">            application.getUser());</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 重点：向调度器发送 RMAppEventType.START 事件</span></span><br><span class="line">        <span class="keyword">this</span>.rmContext.getDispatcher().getEventHandler()</span><br><span class="line">            .handle(<span class="keyword">new</span> RMAppEvent(applicationId, RMAppEventType.START));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      LOG.warn(<span class="string">&quot;Unable to parse credentials.&quot;</span>, e);</span><br><span class="line">      <span class="comment">// Sending APP_REJECTED is fine, since we assume that the</span></span><br><span class="line">      <span class="comment">// RMApp is in NEW state and thus we haven&#x27;t yet informed the</span></span><br><span class="line">      <span class="comment">// scheduler about the existence of the application</span></span><br><span class="line">      <span class="keyword">assert</span> application.getState() == RMAppState.NEW;</span><br><span class="line">      <span class="keyword">this</span>.rmContext.getDispatcher().getEventHandler()</span><br><span class="line">          .handle(<span class="keyword">new</span> RMAppRejectedEvent(applicationId, e.getMessage()));</span><br><span class="line">      <span class="keyword">throw</span> RPCUtil.getRemoteException(e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>RMAppEventType.START 事件在 RMAppImpl 类中有对应的状态转换，即 APP 状态从 NEW 转换为 NEW_SAVING。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java</span></span><br><span class="line">    <span class="comment">// Transitions from NEW state</span></span><br><span class="line">    .addTransition(RMAppState.NEW, RMAppState.NEW_SAVING,</span><br><span class="line">        RMAppEventType.START, <span class="keyword">new</span> RMAppNewlySavingTransition())</span><br></pre></td></tr></table></figure>

<p>注册的 RMAppNewlySavingTransition 状态机做了什么呢？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">RMAppNewlySavingTransition</span> <span class="keyword">extends</span> <span class="title">RMAppTransition</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(RMAppImpl app, RMAppEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 保存 APP 的状态信息</span></span><br><span class="line">      LOG.info(<span class="string">&quot;Storing application with id &quot;</span> + app.applicationId);</span><br><span class="line">      app.rmContext.getStateStore().storeNewApplication(app);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>状态机会对 APP 的状态进行保存，将其元数据存储到 ZK 中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">storeNewApplication</span><span class="params">(RMApp app)</span> </span>&#123;</span><br><span class="line">    ApplicationSubmissionContext context = app</span><br><span class="line">                                            .getApplicationSubmissionContext();</span><br><span class="line">    <span class="keyword">assert</span> context <span class="keyword">instanceof</span> ApplicationSubmissionContextPBImpl;</span><br><span class="line">    ApplicationStateData appState =</span><br><span class="line">        ApplicationStateData.newInstance(</span><br><span class="line">            app.getSubmitTime(), app.getStartTime(), context, app.getUser());</span><br><span class="line">    <span class="comment">// 向调度器发送 RMStateStoreEventType.STORE_APP 事件</span></span><br><span class="line">    dispatcher.getEventHandler().handle(<span class="keyword">new</span> RMStateStoreAppEvent(appState));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里向调度器发送 RMStateStoreEventType.STORE_APP 事件，并注册了 StoreAppTransition 状态机。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java</span></span><br><span class="line">    .addTransition(RMStateStoreState.ACTIVE,</span><br><span class="line">          EnumSet.of(RMStateStoreState.ACTIVE, RMStateStoreState.FENCED),</span><br><span class="line">          RMStateStoreEventType.STORE_APP, <span class="keyword">new</span> StoreAppTransition())</span><br></pre></td></tr></table></figure>

<p>StoreAppTransition 状态机会向调度器发送 RMAppEventType.APP_NEW_SAVED 事件，触发 APP 状态从 NEW_SAVING 到 SUBMITED 的转换。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">StoreAppTransition</span></span></span><br><span class="line"><span class="class">      <span class="keyword">implements</span> <span class="title">MultipleArcTransition</span>&lt;<span class="title">RMStateStore</span>, <span class="title">RMStateStoreEvent</span>,</span></span><br><span class="line"><span class="class">          <span class="title">RMStateStoreState</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RMStateStoreState <span class="title">transition</span><span class="params">(RMStateStore store,</span></span></span><br><span class="line"><span class="params"><span class="function">        RMStateStoreEvent event)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (!(event <span class="keyword">instanceof</span> RMStateStoreAppEvent)) &#123;</span><br><span class="line">        <span class="comment">// should never happen</span></span><br><span class="line">        LOG.error(<span class="string">&quot;Illegal event type: &quot;</span> + event.getClass());</span><br><span class="line">        <span class="keyword">return</span> RMStateStoreState.ACTIVE;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">boolean</span> isFenced = <span class="keyword">false</span>;</span><br><span class="line">      ApplicationStateData appState =</span><br><span class="line">          ((RMStateStoreAppEvent) event).getAppState();</span><br><span class="line">      ApplicationId appId =</span><br><span class="line">          appState.getApplicationSubmissionContext().getApplicationId();</span><br><span class="line">      LOG.info(<span class="string">&quot;Storing info for app: &quot;</span> + appId);</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        store.storeApplicationStateInternal(appId, appState);</span><br><span class="line">        <span class="comment">// 重点：向调度器发送 RMAppEventType.APP_NEW_SAVED 事件</span></span><br><span class="line">        store.notifyApplication(<span class="keyword">new</span> RMAppEvent(appId,</span><br><span class="line">               RMAppEventType.APP_NEW_SAVED));</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        LOG.error(<span class="string">&quot;Error storing app: &quot;</span> + appId, e);</span><br><span class="line">        isFenced = store.notifyStoreOperationFailedInternal(e);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> finalState(isFenced);</span><br><span class="line">    &#125;;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里会向调度器发送 RMAppEventType.APP_NEW_SAVED 事件，该事件会触发 APP 状态从 NEW_SAVING 到 SUBMITED 的转换，并调用 AddApplicationToSchedulerTransition 状态机。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java</span></span><br><span class="line">    .addTransition(RMAppState.NEW_SAVING, RMAppState.SUBMITTED,</span><br><span class="line">        RMAppEventType.APP_NEW_SAVED, <span class="keyword">new</span> AddApplicationToSchedulerTransition())</span><br></pre></td></tr></table></figure>

<p>AddApplicationToSchedulerTransition 状态机会触发 SchedulerEventType.APP_ADDED 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/event/AppAddedSchedulerEvent.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">AddApplicationToSchedulerTransition</span> <span class="keyword">extends</span></span></span><br><span class="line"><span class="class">      <span class="title">RMAppTransition</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(RMAppImpl app, RMAppEvent event)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// 向调度器发送 SchedulerEventType.APP_ADDED 事件</span></span><br><span class="line">      app.handler.handle(<span class="keyword">new</span> AppAddedSchedulerEvent(app.applicationId,</span><br><span class="line">        app.submissionContext.getQueue(), app.user,</span><br><span class="line">        app.submissionContext.getReservationID()));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>其中AppAddedSchedulerEvent 类继承自 SchedulerEvent 类，事件的处理会进入到 FairScheduler 类，来看看对应的 handle() 方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(SchedulerEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">    <span class="keyword">case</span> NODE_ADDED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> NODE_REMOVED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> NODE_UPDATE: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> APP_ADDED:</span><br><span class="line">      <span class="keyword">if</span> (!(event <span class="keyword">instanceof</span> AppAddedSchedulerEvent)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Unexpected event type: &quot;</span> + event);</span><br><span class="line">      &#125;</span><br><span class="line">      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent) event;</span><br><span class="line">      <span class="comment">// APP_ADDED 事件处理逻辑</span></span><br><span class="line">      addApplication(appAddedEvent.getApplicationId(),</span><br><span class="line">        appAddedEvent.getQueue(), appAddedEvent.getUser(),</span><br><span class="line">        appAddedEvent.getIsAppRecovering());</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> APP_REMOVED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> NODE_RESOURCE_UPDATE: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> APP_ATTEMPT_ADDED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> APP_ATTEMPT_REMOVED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CONTAINER_EXPIRED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CONTAINER_RESCHEDULED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">      LOG.error(<span class="string">&quot;Unknown event arrived at FairScheduler: &quot;</span> + event.toString());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>addApplication() 方法会对应用程序的提交进行一些前期检查工作，比如队列名是否正确、用户是否有队列访问权限等，检查通过后，会向调度器发送 RMAppEventType.APP_ACCEPTED 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">addApplication</span><span class="params">(ApplicationId applicationId,</span></span></span><br><span class="line"><span class="params"><span class="function">      String queueName, String user, <span class="keyword">boolean</span> isAppRecovering)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 提交队列信息判断</span></span><br><span class="line">    <span class="keyword">if</span> (queueName == <span class="keyword">null</span> || queueName.isEmpty()) &#123;</span><br><span class="line">      String message = <span class="string">&quot;Reject application &quot;</span> + applicationId +</span><br><span class="line">              <span class="string">&quot; submitted by user &quot;</span> + user + <span class="string">&quot; with an empty queue name.&quot;</span>;</span><br><span class="line">      LOG.info(message);</span><br><span class="line">      rmContext.getDispatcher().getEventHandler()</span><br><span class="line">          .handle(<span class="keyword">new</span> RMAppRejectedEvent(applicationId, message));</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (queueName.startsWith(<span class="string">&quot;.&quot;</span>) || queueName.endsWith(<span class="string">&quot;.&quot;</span>)) &#123;</span><br><span class="line">      String message = <span class="string">&quot;Reject application &quot;</span> + applicationId</span><br><span class="line">          + <span class="string">&quot; submitted by user &quot;</span> + user + <span class="string">&quot; with an illegal queue name &quot;</span></span><br><span class="line">          + queueName + <span class="string">&quot;. &quot;</span></span><br><span class="line">          + <span class="string">&quot;The queue name cannot start/end with period.&quot;</span>;</span><br><span class="line">      LOG.info(message);</span><br><span class="line">      rmContext.getDispatcher().getEventHandler()</span><br><span class="line">          .handle(<span class="keyword">new</span> RMAppRejectedEvent(applicationId, message));</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    RMApp rmApp = rmContext.getRMApps().get(applicationId);</span><br><span class="line">    FSLeafQueue queue = assignToQueue(rmApp, queueName, user);</span><br><span class="line">    <span class="keyword">if</span> (queue == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 队列的 ACL 访问权限判断</span></span><br><span class="line">    UserGroupInformation userUgi = UserGroupInformation.createRemoteUser(user);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!queue.hasAccess(QueueACL.SUBMIT_APPLICATIONS, userUgi)</span><br><span class="line">        &amp;&amp; !queue.hasAccess(QueueACL.ADMINISTER_QUEUE, userUgi)) &#123;</span><br><span class="line">      String msg = <span class="string">&quot;User &quot;</span> + userUgi.getUserName() +</span><br><span class="line">              <span class="string">&quot; cannot submit applications to queue &quot;</span> + queue.getName();</span><br><span class="line">      LOG.info(msg);</span><br><span class="line">      rmContext.getDispatcher().getEventHandler()</span><br><span class="line">          .handle(<span class="keyword">new</span> RMAppRejectedEvent(applicationId, msg));</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    SchedulerApplication&lt;FSAppAttempt&gt; application =</span><br><span class="line">        <span class="keyword">new</span> SchedulerApplication&lt;FSAppAttempt&gt;(queue, user);</span><br><span class="line">    applications.put(applicationId, application);</span><br><span class="line">    queue.getMetrics().submitApp(user);</span><br><span class="line"></span><br><span class="line">    LOG.info(<span class="string">&quot;Accepted application &quot;</span> + applicationId + <span class="string">&quot; from user: &quot;</span> + user</span><br><span class="line">        + <span class="string">&quot;, in queue: &quot;</span> + queue.getName()</span><br><span class="line">        + <span class="string">&quot;, currently num of applications: &quot;</span> + applications.size());</span><br><span class="line">    <span class="keyword">if</span> (isAppRecovering) &#123;</span><br><span class="line">      <span class="comment">// 判断 APP 是否事 Recover 状态（暂时不考虑 Recover 情况）</span></span><br><span class="line">      <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">        LOG.debug(applicationId</span><br><span class="line">            + <span class="string">&quot; is recovering. Skip notifying APP_ACCEPTED&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 重点：向调度器发送 RMAppEventType.APP_ACCEPTED 事件</span></span><br><span class="line">      rmContext.getDispatcher().getEventHandler()</span><br><span class="line">        .handle(<span class="keyword">new</span> RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>RMAppEventType.APP_ACCEPTED 事件的注册，会触发 StartAppAttemptTransition 状态机，并将 APP 的状态从 SUBMITED 转换为 ACCEPTED。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java</span></span><br><span class="line">    <span class="comment">// Transitions from SUBMITTED state</span></span><br><span class="line">    .addTransition(RMAppState.SUBMITTED, RMAppState.ACCEPTED,</span><br><span class="line">        RMAppEventType.APP_ACCEPTED, <span class="keyword">new</span> StartAppAttemptTransition())</span><br></pre></td></tr></table></figure>

<p>StartAppAttemptTransition 状态机会发送 RMAppAttemptEventType.START 事件，以开始启动 AppAttempt。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">StartAppAttemptTransition</span> <span class="keyword">extends</span> <span class="title">RMAppTransition</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(RMAppImpl app, RMAppEvent event)</span> </span>&#123;</span><br><span class="line">      app.createAndStartNewAttempt(<span class="keyword">false</span>);</span><br><span class="line">    &#125;;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 开始启动 AppAttempt</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createAndStartNewAttempt</span><span class="params">(<span class="keyword">boolean</span> transferStateFromPreviousAttempt)</span> </span>&#123;</span><br><span class="line">    createNewAttempt();</span><br><span class="line">    <span class="comment">// 向调度器发送 RMAppAttemptEventType.START 事件</span></span><br><span class="line">    handler.handle(<span class="keyword">new</span> RMAppStartAttemptEvent(currentAttempt.getAppAttemptId(),</span><br><span class="line">      transferStateFromPreviousAttempt));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>RMAppAttemptEventType.START 事件的注册，会调用 AttemptStartedTransition 状态机，触发 AppAttempt 状态从 NEW 转变为 SUBMITED。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java</span></span><br><span class="line">      <span class="comment">// Transitions from NEW State</span></span><br><span class="line">      .addTransition(RMAppAttemptState.NEW, RMAppAttemptState.SUBMITTED,</span><br><span class="line">          RMAppAttemptEventType.START, <span class="keyword">new</span> AttemptStartedTransition())</span><br></pre></td></tr></table></figure>

<p>AttemptStartedTransition 状态机会触发 AppAttemptAddedSchedulerEvent 事件，发送 SchedulerEventType.APP_ATTEMPT_ADDED 请求。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">AttemptStartedTransition</span> <span class="keyword">extends</span> <span class="title">BaseTransition</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(RMAppAttemptImpl appAttempt,</span></span></span><br><span class="line"><span class="params"><span class="function">        RMAppAttemptEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 跳过一些神圣的检查工作</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 向调度器发送 SchedulerEventType.APP_ATTEMPT_ADDED 事件</span></span><br><span class="line">      appAttempt.eventHandler.handle(<span class="keyword">new</span> AppAttemptAddedSchedulerEvent(</span><br><span class="line">        appAttempt.applicationAttemptId, transferStateFromPreviousAttempt));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>AppAttemptAddedSchedulerEvent 类继承自 SchedulerEvent 类，进入具体代码看看 SchedulerEventType.APP_ATTEMPT_ADDED 事件的处理逻辑，还是在 handle() 方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(SchedulerEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">    <span class="keyword">case</span> NODE_ADDED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> NODE_REMOVED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> NODE_UPDATE: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> APP_ADDED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> APP_REMOVED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> NODE_RESOURCE_UPDATE: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> APP_ATTEMPT_ADDED:</span><br><span class="line">      <span class="keyword">if</span> (!(event <span class="keyword">instanceof</span> AppAttemptAddedSchedulerEvent)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Unexpected event type: &quot;</span> + event);</span><br><span class="line">      &#125;</span><br><span class="line">      AppAttemptAddedSchedulerEvent appAttemptAddedEvent =</span><br><span class="line">          (AppAttemptAddedSchedulerEvent) event;</span><br><span class="line">      addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),</span><br><span class="line">        appAttemptAddedEvent.getTransferStateFromPreviousAttempt(),</span><br><span class="line">        appAttemptAddedEvent.getIsAttemptRecovering());</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> APP_ATTEMPT_REMOVED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CONTAINER_EXPIRED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CONTAINER_RESCHEDULED: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">      LOG.error(<span class="string">&quot;Unknown event arrived at FairScheduler: &quot;</span> + event.toString());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>addApplicationAttempt() 方法会调度器发送 RMAppAttemptEventType.ATTEMPT_ADDED 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">addApplicationAttempt</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      ApplicationAttemptId applicationAttemptId,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">boolean</span> transferStateFromPreviousAttempt,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">boolean</span> isAttemptRecovering)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 跳过前期的检查和初始化工作</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isAttemptRecovering) &#123;</span><br><span class="line">      <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">        LOG.debug(applicationAttemptId</span><br><span class="line">            + <span class="string">&quot; is recovering. Skipping notifying ATTEMPT_ADDED&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 向调度器发送 RMAppAttemptEventType.ATTEMPT_ADDED 事件</span></span><br><span class="line">      rmContext.getDispatcher().getEventHandler().handle(</span><br><span class="line">        <span class="keyword">new</span> RMAppAttemptEvent(applicationAttemptId,</span><br><span class="line">            RMAppAttemptEventType.ATTEMPT_ADDED));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>RMAppAttemptEventType.ATTEMPT_ADDED 注册，并触发 ScheduleTransition 状态机，将 AppAttempt 状态从 SUBMITED 转变为 LAUNCHED_UNMANAGED_SAVING 或者 SCHEDULED。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java</span></span><br><span class="line">      <span class="comment">// Transitions from SUBMITTED state</span></span><br><span class="line">      .addTransition(RMAppAttemptState.SUBMITTED, </span><br><span class="line">          EnumSet.of(RMAppAttemptState.LAUNCHED_UNMANAGED_SAVING,</span><br><span class="line">                     RMAppAttemptState.SCHEDULED),</span><br><span class="line">          RMAppAttemptEventType.ATTEMPT_ADDED,</span><br><span class="line">          <span class="keyword">new</span> ScheduleTransition())</span><br></pre></td></tr></table></figure>

<p>看看 ScheduleTransition 状态机，if 语句开关判断是否应该获取管理 AM 的执行，如果为 true，则 RM 不会为 AM 分配一个容器并启动，默认是 false，所以这里返回的状态是 SCHEDULED。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ScheduleTransition</span></span></span><br><span class="line"><span class="class">      <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">MultipleArcTransition</span>&lt;<span class="title">RMAppAttemptImpl</span>, <span class="title">RMAppAttemptEvent</span>, <span class="title">RMAppAttemptState</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RMAppAttemptState <span class="title">transition</span><span class="params">(RMAppAttemptImpl appAttempt,</span></span></span><br><span class="line"><span class="params"><span class="function">        RMAppAttemptEvent event)</span> </span>&#123;</span><br><span class="line">      ApplicationSubmissionContext subCtx = appAttempt.submissionContext;</span><br><span class="line">      <span class="keyword">if</span> (!subCtx.getUnmanagedAM()) &#123;</span><br><span class="line">        <span class="comment">// 跳过一部分操作</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 分配 Container，这里暂不做解释</span></span><br><span class="line">        Allocation amContainerAllocation =</span><br><span class="line">            appAttempt.scheduler.allocate(</span><br><span class="line">                appAttempt.applicationAttemptId,</span><br><span class="line">                appAttempt.amReqs,</span><br><span class="line">                EMPTY_CONTAINER_RELEASE_LIST,</span><br><span class="line">                amBlacklist.getAdditions(),</span><br><span class="line">                amBlacklist.getRemovals());</span><br><span class="line">        <span class="keyword">if</span> (amContainerAllocation != <span class="keyword">null</span></span><br><span class="line">            &amp;&amp; amContainerAllocation.getContainers() != <span class="keyword">null</span>) &#123;</span><br><span class="line">          <span class="keyword">assert</span> (amContainerAllocation.getContainers().size() == <span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回的状态也会进行状态机转换</span></span><br><span class="line">        <span class="keyword">return</span> RMAppAttemptState.SCHEDULED;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// save state and then go to LAUNCHED state</span></span><br><span class="line">        appAttempt.storeAttempt();</span><br><span class="line">        <span class="keyword">return</span> RMAppAttemptState.LAUNCHED_UNMANAGED_SAVING;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>RMAppAttemptState.SCHEDULED 状态，会触发 RMAppAttemptEventType.CONTAINER_ALLOCATED 事件，使得 AppAttempt 状态从 SCHEDULED 转换到 ALLOCATED_SAVING，对应的处理状态机为 AMContainerAllocatedTransition。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java</span></span><br><span class="line">       <span class="comment">// Transitions from SCHEDULED State</span></span><br><span class="line">      .addTransition(RMAppAttemptState.SCHEDULED,</span><br><span class="line">          EnumSet.of(RMAppAttemptState.ALLOCATED_SAVING,</span><br><span class="line">            RMAppAttemptState.SCHEDULED),</span><br><span class="line">          RMAppAttemptEventType.CONTAINER_ALLOCATED,</span><br><span class="line">          <span class="keyword">new</span> AMContainerAllocatedTransition())</span><br></pre></td></tr></table></figure>

<p>AMContainerAllocatedTransition 状态机主要是 AM 获取分配的资源，并发送 RMAppAttemptState.ALLOCATED_SAVING 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">AMContainerAllocatedTransition</span></span></span><br><span class="line"><span class="class">      <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">MultipleArcTransition</span>&lt;<span class="title">RMAppAttemptImpl</span>, <span class="title">RMAppAttemptEvent</span>, <span class="title">RMAppAttemptState</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RMAppAttemptState <span class="title">transition</span><span class="params">(RMAppAttemptImpl appAttempt,</span></span></span><br><span class="line"><span class="params"><span class="function">        RMAppAttemptEvent event)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// 从调度器获取启动 AM 的 Container，这里的 allocate 并没有传入 AM 请求信息，表示先尝试直接获取 Container</span></span><br><span class="line">      Allocation amContainerAllocation =</span><br><span class="line">          appAttempt.scheduler.allocate(appAttempt.applicationAttemptId,</span><br><span class="line">            EMPTY_CONTAINER_REQUEST_LIST, EMPTY_CONTAINER_RELEASE_LIST, <span class="keyword">null</span>,</span><br><span class="line">            <span class="keyword">null</span>);</span><br><span class="line">      <span class="comment">// 对 AM 资源进行判空处理，如果没有获取到之前分配的资源，在这里重新进行分配</span></span><br><span class="line">      <span class="keyword">if</span> (amContainerAllocation.getContainers().size() == <span class="number">0</span>) &#123;</span><br><span class="line">        appAttempt.retryFetchingAMContainer(appAttempt);</span><br><span class="line">        <span class="keyword">return</span> RMAppAttemptState.SCHEDULED;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Set the masterContainer</span></span><br><span class="line">      appAttempt.setMasterContainer(amContainerAllocation.getContainers()</span><br><span class="line">          .get(<span class="number">0</span>));</span><br><span class="line">      RMContainerImpl rmMasterContainer = (RMContainerImpl)appAttempt.scheduler</span><br><span class="line">          .getRMContainer(appAttempt.getMasterContainer().getId());</span><br><span class="line">      rmMasterContainer.setAMContainer(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">      appAttempt.rmContext.getNMTokenSecretManager()</span><br><span class="line">        .clearNodeSetForAttempt(appAttempt.applicationAttemptId);</span><br><span class="line">      appAttempt.getSubmissionContext().setResource(</span><br><span class="line">        appAttempt.getMasterContainer().getResource());</span><br><span class="line">      appAttempt.storeAttempt();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 向调度器发送 RMAppAttemptState.ALLOCATED_SAVING 事件</span></span><br><span class="line">      <span class="keyword">return</span> RMAppAttemptState.ALLOCATED_SAVING;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br></pre></td></tr></table></figure>

<p>RMAppAttemptState.ALLOCATED_SAVING 事件的注册状态机为 AttemptStoredTransition，此时 AppAttempt 状态已从 ALLOCATED_SAVING 转换为 ALLOCATED。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java</span></span><br><span class="line">       <span class="comment">// Transitions from ALLOCATED_SAVING State</span></span><br><span class="line">      .addTransition(RMAppAttemptState.ALLOCATED_SAVING, </span><br><span class="line">          RMAppAttemptState.ALLOCATED,</span><br><span class="line">          RMAppAttemptEventType.ATTEMPT_NEW_SAVED, <span class="keyword">new</span> AttemptStoredTransition())</span><br></pre></td></tr></table></figure>

<p>我们接着看 AttemptStoredTransition 状态机做了什么。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">AttemptStoredTransition</span> <span class="keyword">extends</span> <span class="title">BaseTransition</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(RMAppAttemptImpl appAttempt, RMAppAttemptEvent event)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// 运行 AppAttempt</span></span><br><span class="line">      appAttempt.launchAttempt();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">launchAttempt</span><span class="params">()</span></span>&#123;</span><br><span class="line">    launchAMStartTime = System.currentTimeMillis();</span><br><span class="line">    <span class="comment">// 重点：发送 AMLauncherEventType.LAUNCH 事件启动 AM Container</span></span><br><span class="line">    eventHandler.handle(<span class="keyword">new</span> AMLauncherEvent(AMLauncherEventType.LAUNCH, <span class="keyword">this</span>));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，终于看到了 AM Container 启动的曙光了，可具体是怎么启动的呢？我们接着分析。</p>
<h2 id="2-3-启动AM"><a href="#2-3-启动AM" class="headerlink" title="2.3 启动AM"></a>2.3 启动AM</h2><p>上面的发送的 AMLauncherEventType.LAUNCH 事件是启动 AM 的关键入口，可由谁来处理这个事件呢？这就需要进入到 ApplicationMasterLauncher 类来分析了，我们先来看看这个类的基本属性。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/amlauncher/ApplicationMasterLauncher.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ApplicationMasterLauncher</span> <span class="keyword">extends</span> <span class="title">AbstractService</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">    <span class="title">EventHandler</span>&lt;<span class="title">AMLauncherEvent</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">// 创建线程池实例，针对处理的每一个 AM 事件都启动一个线程</span></span><br><span class="line">  <span class="keyword">private</span> ThreadPoolExecutor launcherPool;</span><br><span class="line">  <span class="comment">// 独立线程处理 AM 的 LAUNCH 和 CLEANUP 事件</span></span><br><span class="line">  <span class="keyword">private</span> LauncherThread launcherHandlingThread;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 事件接收和处理的队列</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> BlockingQueue&lt;Runnable&gt; masterEvents</span><br><span class="line">    = <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;();</span><br><span class="line">  <span class="comment">// 资源管理器上下文</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> RMContext context;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ApplicationMasterLauncher</span><span class="params">(RMContext context)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(ApplicationMasterLauncher.class.getName());</span><br><span class="line">    <span class="keyword">this</span>.context = context;</span><br><span class="line">    <span class="comment">// 新建事件处理的线程</span></span><br><span class="line">    <span class="keyword">this</span>.launcherHandlingThread = <span class="keyword">new</span> LauncherThread();</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> threadCount = conf.getInt(</span><br><span class="line">        YARNConfiguration.RM_AMLAUNCHER_THREAD_COUNT,</span><br><span class="line">        YARNConfiguration.DEFAULT_RM_AMLAUNCHER_THREAD_COUNT);</span><br><span class="line">    ThreadFactory tf = <span class="keyword">new</span> ThreadFactoryBuilder()</span><br><span class="line">        .setNameFormat(<span class="string">&quot;ApplicationMasterLauncher #%d&quot;</span>)</span><br><span class="line">        .build();</span><br><span class="line">    <span class="comment">// 初始化线程池</span></span><br><span class="line">    launcherPool = <span class="keyword">new</span> ThreadPoolExecutor(threadCount, threadCount, <span class="number">1</span>,</span><br><span class="line">        TimeUnit.HOURS, <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;());</span><br><span class="line">    launcherPool.setThreadFactory(tf);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 跳过一些配置初始化操作</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里主要是创建一些执行环境，包括事件处理的独立线程 launcherHandlingThread、所需的线程池 launcherPool 及一个负责接收和处理 AM 事件的 masterEvents 事件队列。而 ApplicationMasterLauncher 类中主要处理 AM 的两种事件：LAUNCH 和 CLEANUP。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/amlauncher/ApplicationMasterLauncher.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span>  <span class="title">handle</span><span class="params">(AMLauncherEvent appEvent)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取 AMLauncherEvent 的事件类型</span></span><br><span class="line">    AMLauncherEventType event = appEvent.getType();</span><br><span class="line">    RMAppAttempt application = appEvent.getAppAttempt();</span><br><span class="line">    <span class="keyword">switch</span> (event) &#123;</span><br><span class="line">    <span class="comment">// 处理 AM LAUNCH 事件</span></span><br><span class="line">    <span class="keyword">case</span> LAUNCH:</span><br><span class="line">      launch(application);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="comment">// 处理 AM CLEANUP 事件</span></span><br><span class="line">    <span class="keyword">case</span> CLEANUP:</span><br><span class="line">      cleanup(application);</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>上面 2.2 小节中最后发送的 AMLauncherEventType.LAUNCH 事件正是在这里处理的，我们就以 LAUNCH 事件为例来看看具体的处理逻辑。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/amlauncher/ApplicationMasterLauncher.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">launch</span><span class="params">(RMAppAttempt application)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建一个 AMLauncher 实例，AMLauncher 继承自 Runnable</span></span><br><span class="line">    Runnable launcher = createRunnableLauncher(application, </span><br><span class="line">        AMLauncherEventType.LAUNCH);</span><br><span class="line">    <span class="comment">// 将事件添加到 masterEvents 队列中</span></span><br><span class="line">    masterEvents.add(launcher);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> Runnable <span class="title">createRunnableLauncher</span><span class="params">(RMAppAttempt application, </span></span></span><br><span class="line"><span class="params"><span class="function">      AMLauncherEventType event)</span> </span>&#123;</span><br><span class="line">    Runnable launcher =</span><br><span class="line">        <span class="keyword">new</span> AMLauncher(context, application, event, getConfig());</span><br><span class="line">    <span class="keyword">return</span> launcher;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>事件被加入到事件队列之后，是如何被处理的呢？这里就是独立线程 launcherHandlingThread 所做的事了，通过消息队列的形式，在线程中逐一被消费处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/amlauncher/ApplicationMasterLauncher.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">LauncherThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LauncherThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">super</span>(<span class="string">&quot;ApplicationMaster Launcher&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">while</span> (!<span class="keyword">this</span>.isInterrupted()) &#123;   <span class="comment">// 死循环不停地处理事件请求</span></span><br><span class="line">        Runnable toLaunch;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 从事件队列中取出事件</span></span><br><span class="line">          toLaunch = masterEvents.take();</span><br><span class="line">          <span class="comment">// 从线程池中取出一个线程执行事件请求</span></span><br><span class="line">          launcherPool.execute(toLaunch);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          LOG.warn(<span class="keyword">this</span>.getClass().getName() + <span class="string">&quot; interrupted. Returning.&quot;</span>);</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;   </span><br></pre></td></tr></table></figure>

<p>取出事件后具体的执行逻辑就交给 AMLaunch 类了，这里的 AMLaunch 类本身就是一个 Runnable 实例，我们直接看其 run() 方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (eventType) &#123;</span><br><span class="line">    <span class="keyword">case</span> LAUNCH:</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        LOG.info(<span class="string">&quot;Launching master&quot;</span> + application.getAppAttemptId());</span><br><span class="line">        <span class="comment">// 启动 launch() 方法</span></span><br><span class="line">        launch();</span><br><span class="line">        <span class="comment">// 发送 RMAppAttemptEventType.LAUNCHED 事件</span></span><br><span class="line">        handler.handle(<span class="keyword">new</span> RMAppAttemptEvent(application.getAppAttemptId(),</span><br><span class="line">            RMAppAttemptEventType.LAUNCHED));</span><br><span class="line">      &#125; <span class="keyword">catch</span>(Exception ie) &#123;</span><br><span class="line">        String message = <span class="string">&quot;Error launching &quot;</span> + application.getAppAttemptId()</span><br><span class="line">            + <span class="string">&quot;. Got exception: &quot;</span> + StringUtils.stringifyException(ie);</span><br><span class="line">        LOG.info(message);</span><br><span class="line">        handler.handle(<span class="keyword">new</span> RMAppAttemptLaunchFailedEvent(application</span><br><span class="line">            .getAppAttemptId(), message));</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> CLEANUP: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">      LOG.warn(<span class="string">&quot;Received unknown event-type &quot;</span> + eventType + <span class="string">&quot;. Ignoring.&quot;</span>);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>AMLaunch 类的 launch() 方法操作会调用 RPC 函数与 NodeManager 通信，来启动 AM Container，这里 AM 与 NM 交互是通过 ContainerManagementProtocol 协议来实现 RPC 调用的。launch() 方法运行完成后会向调度器发送 RMAppAttemptEventType.LAUNCHED 事件，并将 AppAttempt 的状态从 ALLOCATED 转换为 LAUNCHED。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">launch</span><span class="params">()</span> <span class="keyword">throws</span> IOException, YARNException </span>&#123;</span><br><span class="line">    connect();</span><br><span class="line">    ContainerId masterContainerID = masterContainer.getId();</span><br><span class="line">    ApplicationSubmissionContext applicationContext =</span><br><span class="line">      application.getSubmissionContext();</span><br><span class="line">    LOG.info(<span class="string">&quot;Setting up container &quot;</span> + masterContainer</span><br><span class="line">        + <span class="string">&quot; for AM &quot;</span> + application.getAppAttemptId());  </span><br><span class="line">    ContainerLaunchContext launchContext =</span><br><span class="line">        createAMContainerLaunchContext(applicationContext, masterContainerID);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构建 Container 请求信息</span></span><br><span class="line">    StartContainerRequest scRequest =</span><br><span class="line">        StartContainerRequest.newInstance(launchContext,</span><br><span class="line">          masterContainer.getContainerToken());</span><br><span class="line">    List&lt;StartContainerRequest&gt; list = <span class="keyword">new</span> ArrayList&lt;StartContainerRequest&gt;();</span><br><span class="line">    list.add(scRequest);</span><br><span class="line">    StartContainersRequest allRequests =</span><br><span class="line">        StartContainersRequest.newInstance(list);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 重点：调用 RPC 函数启动 Container</span></span><br><span class="line">    StartContainersResponse response =</span><br><span class="line">        containerMgrProxy.startContainers(allRequests);</span><br><span class="line">    <span class="keyword">if</span> (response.getFailedRequests() != <span class="keyword">null</span></span><br><span class="line">        &amp;&amp; response.getFailedRequests().containsKey(masterContainerID)) &#123;</span><br><span class="line">      Throwable t =</span><br><span class="line">          response.getFailedRequests().get(masterContainerID).deSerialize();</span><br><span class="line">      parseAndThrowException(t);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Done launching container &quot;</span> + masterContainer + <span class="string">&quot; for AM &quot;</span></span><br><span class="line">          + application.getAppAttemptId());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，用于运行 ApplicationMaster 的 Container 已经启动，具体的 Container 启动逻辑在这里不做分析，AM Container 在具体的 NodeManager 上启动后，Container 会根据上下文信息启动 ApplicationMaster 进程，ApplicationMaster 生命周期的第一步 ApplicationMaster 启动在这里已经完成了。</p>
<h1 id="3-AM-心跳上报及资源申请"><a href="#3-AM-心跳上报及资源申请" class="headerlink" title="3. AM 心跳上报及资源申请"></a>3. AM 心跳上报及资源申请</h1><p>这部分主要介绍 ApplicationMaster 启动是做了哪些工作，如何向 ResourceManager 进行注册和心跳，以及如何申请 Container 资源。</p>
<h2 id="3-1-AM注册-心跳上报"><a href="#3-1-AM注册-心跳上报" class="headerlink" title="3.1 AM注册/心跳上报"></a>3.1 AM注册/心跳上报</h2><p>Container 的启动会触发 ApplicationMaster 进程的启动，于是我们从 ApplicationMaster 类的 main() 方法作为入口，来看看 ApplicationMaster 启动时做了哪些工作。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> result = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      ApplicationMaster appMaster = <span class="keyword">new</span> ApplicationMaster();</span><br><span class="line">      LOG.info(<span class="string">&quot;Initializing ApplicationMaster&quot;</span>);</span><br><span class="line">      <span class="keyword">boolean</span> doRun = appMaster.init(args);</span><br><span class="line">      <span class="keyword">if</span> (!doRun) &#123;</span><br><span class="line">        System.exit(<span class="number">0</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// AM 启动的核心 run() 方法</span></span><br><span class="line">      appMaster.run();</span><br><span class="line">      result = appMaster.finish();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">      LOG.fatal(<span class="string">&quot;Error running ApplicationMaster&quot;</span>, t);</span><br><span class="line">      LogManager.shutdown();</span><br><span class="line">      ExitUtil.terminate(<span class="number">1</span>, t);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (result) &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Application Master completed successfully. exiting&quot;</span>);</span><br><span class="line">      System.exit(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Application Master failed. exiting&quot;</span>);</span><br><span class="line">      System.exit(<span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>run() 方法是 AM 启动的核心入口方法。这里主要是初始化相关 RPC 客户端实例，并开始向 RM 进行注册和心跳。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> YARNException, IOException </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">&quot;Starting ApplicationMaster&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 跳过 tokens 的检查工作</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化 AMRMClientAsync 实例，用于 AM 与 RM 之间进行交互</span></span><br><span class="line">    AMRMClientAsync.CallbackHandler allocListener = <span class="keyword">new</span> RMCallbackHandler();</span><br><span class="line">    amRMClient = AMRMClientAsync.createAMRMClientAsync(<span class="number">1000</span>, allocListener);</span><br><span class="line">    amRMClient.init(conf);</span><br><span class="line">    amRMClient.start();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化 NMClientAsync 实例，用于 AM 与 NM 之间进行交互</span></span><br><span class="line">    containerListener = createNMCallbackHandler();</span><br><span class="line">    nmClientAsync = <span class="keyword">new</span> NMClientAsyncImpl(containerListener);</span><br><span class="line">    nmClientAsync.init(conf);</span><br><span class="line">    nmClientAsync.start();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重点：AM 向 RM 进行注册，这里也会向 RM 发送心跳请求</span></span><br><span class="line">    appMasterHostname = NetUtils.getHostname();</span><br><span class="line">    RegisterApplicationMasterResponse response = amRMClient</span><br><span class="line">        .registerApplicationMaster(appMasterHostname, appMasterRpcPort,</span><br><span class="line">            appMasterTrackingUrl);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 跳过资源限制检查及 Container 状态的记录过程</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>AM 与 RM 进行 RPC 通信是通过 ApplicationMasterService 服务实现的，在看服务端 registerApplicationMaster 注册函数前，先来看看客户端的注册函数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/async/impl/AMRMClientAsyncImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> RegisterApplicationMasterResponse <span class="title">registerApplicationMaster</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      String appHostName, <span class="keyword">int</span> appHostPort, String appTrackingUrl)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> YARNException, IOException </span>&#123;</span><br><span class="line">    <span class="comment">// AM 注册</span></span><br><span class="line">    RegisterApplicationMasterResponse response = client</span><br><span class="line">        .registerApplicationMaster(appHostName, appHostPort, appTrackingUrl);</span><br><span class="line">    <span class="comment">// 启动 AM 心跳上报线程</span></span><br><span class="line">    heartbeatThread.start();</span><br><span class="line">    <span class="keyword">return</span> response;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>先来看看 AM 注册过程。注册时做了两件事，一个是更新 AM 在 AMLivelinessMonitor 中的最新事件，另一个是发送 RMAppAttemptEventType.REGISTERED 事件，触发 AMRegisteredTransition 状态机，并将 AppAttempt 状态从 LAUNCHED 转换为 RUNNING。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> RegisterApplicationMasterResponse <span class="title">registerApplicationMaster</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      RegisterApplicationMasterRequest request)</span> <span class="keyword">throws</span> YARNException,</span></span><br><span class="line"><span class="function">      IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//省略</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allow only one thread in AM to do registerApp at a time.</span></span><br><span class="line">    <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">      <span class="comment">// 省略</span></span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 更新 AM 在 AMLivelinessMonitor 中最近汇报心跳的事件</span></span><br><span class="line">      <span class="keyword">this</span>.amLivelinessMonitor.receivedPing(applicationAttemptId);</span><br><span class="line">      RMApp app = <span class="keyword">this</span>.rmContext.getRMApps().get(appID);</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// Setting the response id to 0 to identify if the</span></span><br><span class="line">      <span class="comment">// application master is register for the respective attemptid</span></span><br><span class="line">      lastResponse.setResponseId(<span class="number">0</span>);</span><br><span class="line">      lock.setAllocateResponse(lastResponse);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// AM 注册关键逻辑，发送 RMAppAttemptEventType.REGISTERED 事件</span></span><br><span class="line">      LOG.info(<span class="string">&quot;AM registration &quot;</span> + applicationAttemptId);</span><br><span class="line">      <span class="keyword">this</span>.rmContext</span><br><span class="line">        .getDispatcher()</span><br><span class="line">        .getEventHandler()</span><br><span class="line">        .handle(</span><br><span class="line">          <span class="keyword">new</span> RMAppAttemptRegistrationEvent(applicationAttemptId, request</span><br><span class="line">            .getHost(), request.getRpcPort(), request.getTrackingUrl()));</span><br><span class="line">      RMAuditLogger.logSuccess(app.getUser(), AuditConstants.REGISTER_AM,</span><br><span class="line">        <span class="string">&quot;ApplicationMasterService&quot;</span>, appID, applicationAttemptId);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 省略</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>接着来看看 AM 心跳上报流程。heartbeatThread 线程是处理 AM 的独立线程，其初始化过程如下。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/async/impl/AMRMClientAsyncImpl.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AMRMClientAsyncImpl</span>&lt;<span class="title">T</span> <span class="keyword">extends</span> <span class="title">ContainerRequest</span>&gt; </span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">AMRMClientAsync</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// AM 心跳线程对象</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> HeartbeatThread heartbeatThread;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Private</span></span><br><span class="line">  <span class="meta">@VisibleForTesting</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">AMRMClientAsyncImpl</span><span class="params">(AMRMClient&lt;T&gt; client, <span class="keyword">int</span> intervalMs,</span></span></span><br><span class="line"><span class="params"><span class="function">      CallbackHandler callbackHandler)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(client, intervalMs, callbackHandler);</span><br><span class="line">    <span class="comment">// 初始化 AM 心跳线程实例</span></span><br><span class="line">    heartbeatThread = <span class="keyword">new</span> HeartbeatThread();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>AM 向 RM 注册后，周期性地通过 RPC 函数 ApplicationMasterProtocol#allocate() 方法与 RM 通信，该方法主要有以下是三个作用：</p>
<ul>
<li>请求申请；</li>
<li>获取新分配地资源；</li>
<li>形成周期性心跳，告诉 RM 自己还活着。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/async/impl/AMRMClientAsyncImpl.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">HeartbeatThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HeartbeatThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">super</span>(<span class="string">&quot;AMRM Heartbeater thread&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;   <span class="comment">// 心跳线程死循环的跑</span></span><br><span class="line">        AllocateResponse response = <span class="keyword">null</span>;</span><br><span class="line">        <span class="comment">// synchronization ensures we don&#x27;t send heartbeats after unregistering</span></span><br><span class="line">        <span class="keyword">synchronized</span> (unregisterHeartbeatLock) &#123;</span><br><span class="line">          <span class="keyword">if</span> (!keepRunning) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 重点：心跳线程其实就是周期性的调用 allocate() 方法</span></span><br><span class="line">            response = client.allocate(progress);</span><br><span class="line">          &#125; <span class="keyword">catch</span> (ApplicationAttemptNotFoundException e) &#123;</span><br><span class="line">            handler.onShutdownRequest();</span><br><span class="line">            LOG.info(<span class="string">&quot;Shutdown requested. Stopping callback.&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Throwable ex) &#123;</span><br><span class="line">            LOG.error(<span class="string">&quot;Exception on heartbeat&quot;</span>, ex);</span><br><span class="line">            savedException = ex;</span><br><span class="line">            <span class="comment">// interrupt handler thread in case it waiting on the queue</span></span><br><span class="line">            handlerThread.interrupt();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (response != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                responseQueue.put(response);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">              &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;</span><br><span class="line">                LOG.debug(<span class="string">&quot;Interrupted while waiting to put on response queue&quot;</span>, ex);</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          Thread.sleep(heartbeatIntervalMs.get());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;</span><br><span class="line">          LOG.debug(<span class="string">&quot;Heartbeater interrupted&quot;</span>, ex);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，AM 已经完成向 RM 的注册及周期性心跳上报的过程，其中心跳上报是通过周期性地调用 ApplicationMasterProtocol#allocate() 方法来实现的。AM 心跳开始后，便会定期的向 RM 申请资源，以在对应的 NodeManager 上启动 Container 进程，在下一部分中会详细介绍。</p>
<h2 id="3-2-AM资源申请与分配"><a href="#3-2-AM资源申请与分配" class="headerlink" title="3.2 AM资源申请与分配"></a>3.2 AM资源申请与分配</h2><p>AM 资源申请与分配的对象都是针对 Container，下面也是以 Container 的申请与分配作为介绍内容。</p>
<h3 id="3-2-1-原理介绍"><a href="#3-2-1-原理介绍" class="headerlink" title="3.2.1 原理介绍"></a>3.2.1 原理介绍</h3><p> <img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/07/76b28970968e92a0a2ae949cea27599f-1372882-20200826172856403-72149053-b8e974.png" alt="img"></p>
<center>Container 分配与申请流程</center>

<p>如上图，应用程序的 AM 在 NM 上成功启动并向 RM 注册后，向 RM 请求资源（Container）到获取到资源的整个过程，分为两个阶段：</p>
<p>阶段一：AM 汇报资源资源并领取已经分配到的资源；</p>
<ol>
<li>AM 通过 RPC 函数 ApplicationMasterProtocol#allocate 向 RM 汇报资源需求（由于是周期性调用，也叫“心跳”），包括包括新的资源需求描述、待释放的 Container 列表、请求加入黑名单的节点列表、请求移除黑名单的节点列表等；</li>
<li>RM 中的 ApplicationMasterService 负责处理来自 ApplicationMaster 的请求，一旦受到请求，会向 RMAppAttemptImpl 发送一个 RMAppAttemptEventType.STATUS_UPDATE 类型事件，而 RMAppAttempImpl 收到该事件后，将更新应用程序执行进度和 AMLivelinessMonitor 中记录的应用程序最近更新事件。</li>
<li>ApplicationMasterService 调用 ResourceScheduler#allocate 函数，将 ApplicationMaster 资源需求汇报给 ResourceScheduler。</li>
<li>ResourceScheduler 首先读取待释放 Contianer 列表，依次向对应的 RMContainerImpl 发送 RMContainerEventType.RELEASED 类型事件，以杀死正在运行的 Container，然后将新的资源需求更新到对应数据中，并返回已经为该应用程序分配的资源。</li>
</ol>
<p>阶段二：NM 向 RM 汇报各个 Container 运行状态，如果 RM 发现它上面又空闲的资源，则进行一次分配，并将分配的资源保存到 RM 数据结构中，等待下次 AM 发送心跳时获取。</p>
<ol>
<li>NM 通过 RPC 函数 ResourceTracker#nodeHeartbeat 向 RM 汇报各个 Container 运行状态。</li>
<li>RM 中的 ResourceTrackerService 负责处理来自 NM 的请求，一旦收到请求，会向 RMNodeImpl 发送一个 RMNodeEventType.STATUS_UPDATE 事件，而 RMNodeImpl 收到事件后，将更新各个 Container 运行状态，并进一步向 ResourceScheduler 发送一个 SchedulerEventType.NODE_UPDATE 事件。</li>
<li>ResourceScheduler 收到事件后，如果该节点又可分配的空闲资源，则会将这些资源分配给各个应用程序，而分配后的资源仅是记录到对应数据结构中，等待 ApplicationMaster 下次通过心跳机制来领取。</li>
</ol>
<h3 id="3-2-2-源码分析"><a href="#3-2-2-源码分析" class="headerlink" title="3.2.2 源码分析"></a>3.2.2 源码分析</h3><p>客户端调用 AMRMClientAsyncImpl#allocate() 方法会通过 RPC 函数向 RM 汇报资源需求，其通信接口是由 ApplicationMasterProtocol 协议来实现，来看看该协议是如何为客户端申请资源。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：rg/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> AllocateResponse <span class="title">allocate</span><span class="params">(AllocateRequest request)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> YARNException, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">      AllocateResponse lastResponse = lock.getAllocateResponse();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 发送 STATUS_UPDATE 更新 AppAttempt 状态</span></span><br><span class="line">      <span class="keyword">this</span>.rmContext.getDispatcher().getEventHandler().handle(</span><br><span class="line">          <span class="keyword">new</span> RMAppAttemptStatusupdateEvent(appAttemptId, request</span><br><span class="line">              .getProgress()));             </span><br><span class="line">      <span class="comment">// 检查队列中的 memory 和 vcore 是否足够</span></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        RMServerUtils.normalizeAndValidateRequests(ask,</span><br><span class="line">            rScheduler.getMaximumResourceCapability(), app.getQueue(),</span><br><span class="line">            rScheduler);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InvalidResourceRequestException e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">&quot;Invalid resource ask by application &quot;</span> + appAttemptId, e);</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 重点：调用调度器的 allocate() 方法向 RM 上报资源需求</span></span><br><span class="line">      Allocation allocation =</span><br><span class="line">          <span class="keyword">this</span>.rScheduler.allocate(appAttemptId, ask, release, </span><br><span class="line">              blacklistAdditions, blacklistRemovals);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 更新请求的 response 和 AMRMToken 的状态，省略具体流程</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> allocateResponse;</span><br><span class="line">    &#125;    </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ApplicationMasterService#allocate() 方法会调用 YARNScheduler 的 allocate() 分配方法，由于采用是 FairScheduler 调度器，我们来分析下 FairScheduler#allocate() 方法。分配过程的核心在 pullNewlyAllocatedContainersAndNMTokens() 方法，该方法的核心是从 newlyAllocatedContainers 这个 List 数据结构中取 Container，那取到的 Container 是从哪儿来的呢？其实就是 NoddeManager 心跳发生时进行资源分配逻辑分配出来的 Container，是保存在 RM 的内存数据结构 newlyAllocatedContainers 中，AM 则直接从该数据结构中取对应的 Container。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Allocation <span class="title">allocate</span><span class="params">(ApplicationAttemptId appAttemptId,</span></span></span><br><span class="line"><span class="params"><span class="function">      List&lt;ResourceRequest&gt; ask, List&lt;ContainerId&gt; release,</span></span></span><br><span class="line"><span class="params"><span class="function">      List&lt;String&gt; blacklistAdditions, List&lt;String&gt; blacklistRemovals)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 规整化资源请求</span></span><br><span class="line">    SchedulerUtils.normalizeRequests(ask, DOMINANT_RESOURCE_CALCULATOR,</span><br><span class="line">        getClusterResource(), minimumAllocation, getMaximumResourceCapability(),</span><br><span class="line">        incrAllocation);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 记录 Container 分配的开始时间</span></span><br><span class="line">    application.recordContainerRequestTime(getClock().getTime());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Release containers</span></span><br><span class="line">    releaseContainers(release, application);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">synchronized</span> (application) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!ask.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">          LOG.debug(<span class="string">&quot;allocate: pre-update&quot;</span> +</span><br><span class="line">              <span class="string">&quot; applicationAttemptId=&quot;</span> + appAttemptId +</span><br><span class="line">              <span class="string">&quot; application=&quot;</span> + application.getApplicationId());</span><br><span class="line">        &#125;</span><br><span class="line">        application.showRequests();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Update application requests</span></span><br><span class="line">        application.updateResourceRequests(ask);</span><br><span class="line"></span><br><span class="line">        application.showRequests();</span><br><span class="line">      &#125;</span><br><span class="line">      ... <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 重点：对 Container 进行鉴权，并拿到之前为 AppAttempt 分配的 Container 资源</span></span><br><span class="line">      <span class="comment">// 该资源保存在 RM 内存数据结构中，由 assignContainer() 方法分配出来的，具体分配逻辑可以看 YARN 的调度逻辑</span></span><br><span class="line">      ContainersAndNMTokensAllocation allocation =</span><br><span class="line">          application.pullNewlyAllocatedContainersAndNMTokens();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Record container allocation time</span></span><br><span class="line">      <span class="keyword">if</span> (!(allocation.getContainerList().isEmpty())) &#123;</span><br><span class="line">        application.recordContainerAllocationTime(getClock().getTime());</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 将分配的 Container 资源返回给客户端（AM）</span></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> Allocation(allocation.getContainerList(),</span><br><span class="line">        application.getHeadroom(), preemptionContainerIds, <span class="keyword">null</span>, <span class="keyword">null</span>,</span><br><span class="line">        allocation.getNMTokenList());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，AM 周期性心跳进行资源申请的逻辑在这里已经拿到了 Container，那拿到 Container 后又怎样启动呢，不同任务类型的启动方式不太一样，这里就不做详细介绍。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li>董西成，《Hadoop技术内幕：深入分析YARN架构设计与实现原理》</li>
<li><a href="https://blog.csdn.net/weixin_42642341/article/details/81636135">YARN源码剖析（三）— ApplicationMaster的启动</a></li>
<li><a href="https://blog.csdn.net/Androidlushangderen/article/details/48128955">YARN源码分析(一)—–ApplicationMaster</a></li>
<li><a href="https://blog.csdn.net/weixin_42642341/article/details/82354964">Yarn源码剖析（四）– AM的注册与资源调度申请Container及启动</a></li>
</ul>
]]></content>
      <categories>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>YARN</tag>
        <tag>YARN源码</tag>
      </tags>
  </entry>
  <entry>
    <title>YARN Container启动原理源码分析</title>
    <url>/2021/11/07/YARN-Container%E5%90%AF%E5%8A%A8%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>在 《YARN ApplicationMaster启动原理及源码分析》文章中，AM 向 RM 注册后，会周期性地通过 RPC 函数 ApplicationMaster#allocate() 与 RM 通信，通信目的包括请求资源、获取新分配的资源及形成周期性心跳，本文中我们重点看看 AM 向 RM 申请到 Container 资源后，如何在 NM 节点上启动 Container，接下来会详细介绍 Container 从申请资源、启动到资源清理整个过程的源码。</p>
<h1 id="1-Container启动流程介绍"><a href="#1-Container启动流程介绍" class="headerlink" title="1. Container启动流程介绍"></a>1. Container启动流程介绍</h1><p>Container 启动是由 ApplicationMaster 通过 RPC 函数 ContainerManagementProtocol#startContainers() 向 NM 发起的，NM 中的 ContainerManagerImpl 组件负责接收并处理该请求。Container 启动过程主要经历三个阶段：资源本地化、启动并运行 Container和资源清理。</p>
<ul>
<li><strong>资源本地化</strong>主要是指分布式缓存机制完成的工作，功能包括初始化各种服务组件、创建工作目录、从 HDFS 下载运行所需的各种资源（比如文本文件、JAR 包、可执行文件）等。资源本地化主要有两部分组成，分别是应用程序初始化和 Container 本地化。其中，应用程序初始化的主要工作是初始化各类必需的服务组件（比如日志记录组件 LogHandler、资源状态追踪器 LocalResourceTrackerImpl等），供后续 Container 使用，通常由 Application 的第一个 Container 完成；Container 本地化则是创建工作目录，从 HDFS 下载各类文件资源。</li>
<li><strong>Container 启动</strong>是由 ContainerLauncher 服务完成，该服务将进一步调用插拔式组件 ContainerExecutor。YARN 中提供了三种 ContainerExecutor 实现，一种是 DefaultContainerExecutor，一种是 LinuxContainerExecutor，另一种是 DockerContainerExecutor，由参数 yarn.nodemanager.container-executor.class 控制具体采用的方式。</li>
<li><strong>资源清理</strong>则是资源本地化的逆过程，它负责清理各类资源，均由 ResourceLocalizationService 服务完成。</li>
</ul>
<h1 id="2-Container启动源码分析"><a href="#2-Container启动源码分析" class="headerlink" title="2. Container启动源码分析"></a>2. Container启动源码分析</h1><h2 id="2-1-AM调用api请求启动Container"><a href="#2-1-AM调用api请求启动Container" class="headerlink" title="2.1 AM调用api请求启动Container"></a>2.1 AM调用api请求启动Container</h2><p>在介绍 Container 启动前，我们先来看看 AM 在心跳时如何根据申请到的资源来请求 Container 的启动。AM 通过 RPC 函数 ApplicationMaster#allocate() 周期性向 RM 申请资源，并将申请到的资源保存在阻塞队列 responseQueue 中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/async/impl/AMRMClientAsyncImpl.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">HeartbeatThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HeartbeatThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">super</span>(<span class="string">&quot;AMRM Heartbeater thread&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;   <span class="comment">// 心跳线程死循环的跑</span></span><br><span class="line">        AllocateResponse response = <span class="keyword">null</span>;</span><br><span class="line">        <span class="comment">// synchronization ensures we don&#x27;t send heartbeats after unregistering</span></span><br><span class="line">        <span class="keyword">synchronized</span> (unregisterHeartbeatLock) &#123;</span><br><span class="line">          <span class="keyword">if</span> (!keepRunning) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"> </span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 重点：心跳线程其实就是周期性的调用 allocate() 方法，将分配出来的 Container 保存在 AllocateResponse 实例中</span></span><br><span class="line">            response = client.allocate(progress);</span><br><span class="line">          &#125; <span class="keyword">catch</span> (ApplicationAttemptNotFoundException e) &#123;</span><br><span class="line">            handler.onShutdownRequest();</span><br><span class="line">            LOG.info(<span class="string">&quot;Shutdown requested. Stopping callback.&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Throwable ex) &#123;</span><br><span class="line">            LOG.error(<span class="string">&quot;Exception on heartbeat&quot;</span>, ex);</span><br><span class="line">            savedException = ex;</span><br><span class="line">            <span class="comment">// interrupt handler thread in case it waiting on the queue</span></span><br><span class="line">            handlerThread.interrupt();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (response != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                  <span class="comment">// 将 RM 返回的 AllocateResponse 对象资源添加到阻塞队列 responseQueue 中</span></span><br><span class="line">                responseQueue.put(response);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">              &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;</span><br><span class="line">                LOG.debug(<span class="string">&quot;Interrupted while waiting to put on response queue&quot;</span>, ex);</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          Thread.sleep(heartbeatIntervalMs.get());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;</span><br><span class="line">          LOG.debug(<span class="string">&quot;Heartbeater interrupted&quot;</span>, ex);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>那 responseQueue 队列保存申请到的 Container 资源怎么使用呢？通过查看 responseQueue.take() 函数，可以发现 AMRMClientAsyncImpl 类中的独立线程 CallbackHandlerThread 会不断地从队列中取出 AllocateResponse 对象进行处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/async/impl/AMRMClientAsyncImpl.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">CallbackHandlerThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CallbackHandlerThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">super</span>(<span class="string">&quot;AMRM Callback Handler Thread&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;    <span class="comment">// 死循环取出申请到的 Container 资源并进行处理</span></span><br><span class="line">        <span class="keyword">if</span> (!keepRunning) &#123;</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          AllocateResponse response;</span><br><span class="line">          <span class="keyword">if</span>(savedException != <span class="keyword">null</span>) &#123;</span><br><span class="line">            LOG.error(<span class="string">&quot;Stopping callback due to: &quot;</span>, savedException);</span><br><span class="line">            handler.onError(savedException);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">              <span class="comment">// 从阻塞队列 responseQueue 取出 Container 资源</span></span><br><span class="line">            response = responseQueue.take();</span><br><span class="line">          &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;</span><br><span class="line">            LOG.info(<span class="string">&quot;Interrupted while waiting for queue&quot;</span>, ex);</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          List&lt;NodeReport&gt; updatedNodes = response.getUpdatedNodes();</span><br><span class="line">          <span class="keyword">if</span> (!updatedNodes.isEmpty()) &#123;</span><br><span class="line">            handler.onNodesUpdated(updatedNodes);</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          List&lt;ContainerStatus&gt; completed =</span><br><span class="line">              response.getCompletedContainersStatuses();</span><br><span class="line">          <span class="keyword">if</span> (!completed.isEmpty()) &#123;</span><br><span class="line">            handler.onContainersCompleted(completed);</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          List&lt;Container&gt; allocated = response.getAllocatedContainers();</span><br><span class="line">          <span class="keyword">if</span> (!allocated.isEmpty()) &#123;</span><br><span class="line">              <span class="comment">// 重点：处理分配出来的 Container</span></span><br><span class="line">            handler.onContainersAllocated(allocated);</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 更新 Container 的执行进度</span></span><br><span class="line">          progress = handler.getProgress();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable ex) &#123;</span><br><span class="line">          handler.onError(ex);</span><br><span class="line">          <span class="comment">// re-throw exception to end the thread</span></span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(ex);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>handler.onContainersAllocated(allocated) 方法会对分配出来的 Container 资源进行处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onContainersAllocated</span><span class="params">(List&lt;Container&gt; allocatedContainers)</span> </span>&#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Got response from RM for container ask, allocatedCnt=&quot;</span></span><br><span class="line">          + allocatedContainers.size());</span><br><span class="line">      numAllocatedContainers.addAndGet(allocatedContainers.size());</span><br><span class="line">      <span class="keyword">for</span> (Container allocatedContainer : allocatedContainers) &#123;</span><br><span class="line">        String yarnShellId = Integer.toString(yarnShellIdCounter);</span><br><span class="line">        yarnShellIdCounter++;</span><br><span class="line">        LOG.info(<span class="string">&quot;Launching shell command on a new container.&quot;</span></span><br><span class="line">            + <span class="string">&quot;, containerId=&quot;</span> + allocatedContainer.getId()</span><br><span class="line">            + <span class="string">&quot;, yarnShellId=&quot;</span> + yarnShellId</span><br><span class="line">            + <span class="string">&quot;, containerNode=&quot;</span> + allocatedContainer.getNodeId().getHost()</span><br><span class="line">            + <span class="string">&quot;:&quot;</span> + allocatedContainer.getNodeId().getPort()</span><br><span class="line">            + <span class="string">&quot;, containerNodeURI=&quot;</span> + allocatedContainer.getNodeHttpAddress()</span><br><span class="line">            + <span class="string">&quot;, containerResourceMemory&quot;</span></span><br><span class="line">            + allocatedContainer.getResource().getMemory()</span><br><span class="line">            + <span class="string">&quot;, containerResourceVirtualCores&quot;</span></span><br><span class="line">            + allocatedContainer.getResource().getVirtualCores());</span><br><span class="line">        <span class="comment">// + &quot;, containerToken&quot;</span></span><br><span class="line">        <span class="comment">// +allocatedContainer.getContainerToken().getIdentifier().toString());</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建运行 Container 的 LaunchContainerRunnable 线程</span></span><br><span class="line">        Thread launchThread = createLaunchContainerThread(allocatedContainer,</span><br><span class="line">            yarnShellId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// launch and start the container on a separate thread to keep</span></span><br><span class="line">        <span class="comment">// the main thread unblocked</span></span><br><span class="line">        <span class="comment">// as all containers may not be allocated at one go.</span></span><br><span class="line">        launchThreads.add(launchThread);</span><br><span class="line">        launchedContainers.add(allocatedContainer.getId());</span><br><span class="line">        <span class="comment">// 启动 LaunchContainerRunnable 线程</span></span><br><span class="line">        launchThread.start();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@VisibleForTesting</span></span><br><span class="line">    <span class="function">Thread <span class="title">createLaunchContainerThread</span><span class="params">(Container allocatedContainer,</span></span></span><br><span class="line"><span class="params"><span class="function">        String shellId)</span> </span>&#123;</span><br><span class="line">      LaunchContainerRunnable runnableLaunchContainer =</span><br><span class="line">          <span class="keyword">new</span> LaunchContainerRunnable(allocatedContainer, containerListener,</span><br><span class="line">              shellId);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> Thread(runnableLaunchContainer);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>上面的逻辑启动了一个 LaunchContainerRunnable 线程，LaunchContainerRunnable 是 ApplicationMaster 类的内部类，继承自 Runnable 接口，通过该类的 run() 方法，可以知道该类主要做了两件事：</p>
<ul>
<li>初始化 Contianer 的本地资源，并构建 Container 的启动脚本</li>
<li>调用 NMClientAsync#startContainerAsync() api 接口启动 Container。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Setting up container launch container for containerid=&quot;</span></span><br><span class="line">          + container.getId() + <span class="string">&quot; with shellid=&quot;</span> + shellId);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 省略构建 Container 启动脚本逻辑</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// Set up ContainerLaunchContext, setting local resource, environment,</span></span><br><span class="line">      <span class="comment">// command and token for constructor.</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// Note for tokens: Set up tokens for the container too. Today, for normal</span></span><br><span class="line">      <span class="comment">// shell commands, the container in distribute-shell doesn&#x27;t need any</span></span><br><span class="line">      <span class="comment">// tokens. We are populating them mainly for NodeManagers to be able to</span></span><br><span class="line">      <span class="comment">// download anyfiles in the distributed file-system. The tokens are</span></span><br><span class="line">      <span class="comment">// otherwise also useful in cases, for e.g., when one is running a</span></span><br><span class="line">      <span class="comment">// &quot;hadoop dfs&quot; command inside the distributed shell.</span></span><br><span class="line">      Map&lt;String, String&gt; myShellEnv = <span class="keyword">new</span> HashMap&lt;String, String&gt;(shellEnv);</span><br><span class="line">      myShellEnv.put(YARN_SHELL_ID, shellId);</span><br><span class="line">      ContainerLaunchContext ctx = ContainerLaunchContext.newInstance(</span><br><span class="line">        localResources, myShellEnv, commands, <span class="keyword">null</span>, allTokens.duplicate(),</span><br><span class="line">          <span class="keyword">null</span>);</span><br><span class="line">      containerListener.addContainer(container.getId(), container);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 2. 重点：通过 NMClientAsync api 启动分配出来的 Container</span></span><br><span class="line">      nmClientAsync.startContainerAsync(container, ctx);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>可以看到 nmClientAsync.startContainerAsync() 方法并没有真正启动 Container，而是将 ContainerEventType.START_CONTAINER 事件封装成 ContainerEvent 对象（StartContainerEvent 类继承自 ContainerEvent 类），并添加到 Container 事件处理的阻塞队列 events 中，具体操作处理流程由 events 队列的消费逻辑处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/async/impl/NMClientAsyncImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">startContainerAsync</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      Container container, ContainerLaunchContext containerLaunchContext)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (containers.putIfAbsent(container.getId(),</span><br><span class="line">        <span class="keyword">new</span> StatefulContainer(<span class="keyword">this</span>, container.getId())) != <span class="keyword">null</span>) &#123;</span><br><span class="line">      callbackHandler.onStartContainerError(container.getId(),</span><br><span class="line">          RPCUtil.getRemoteException(<span class="string">&quot;Container &quot;</span> + container.getId() +</span><br><span class="line">              <span class="string">&quot; is already started or scheduled to start&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      events.put(<span class="keyword">new</span> StartContainerEvent(container, containerLaunchContext));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">      LOG.warn(<span class="string">&quot;Exception when scheduling the event of starting Container &quot;</span> +</span><br><span class="line">          container.getId());</span><br><span class="line">      callbackHandler.onStartContainerError(container.getId(), e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>那这里的阻塞队列 events 又是怎么处理呢？还是来找找 events.take() 方法，发现在 NMClientAsyncImpl 类执行 serviceStart() 方法时会启动一个线程去消费 events 队列的事件，队列取出来的事件对象为内部封装有 ContainerEventType.START_CONTAINER 事件的 ContainerEvent 对象，通过 getContainerEventProcessor(event) 方法，获取对应的 ContainerEvent 对象的处理器 ContainerEventProcessor，并以线程池的方式运行该处理器。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/async/impl/NMClientAsyncImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceStart</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    client.start();</span><br><span class="line"></span><br><span class="line">    ThreadFactory tf = <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(</span><br><span class="line">        <span class="keyword">this</span>.getClass().getName() + <span class="string">&quot; #%d&quot;</span>).setDaemon(<span class="keyword">true</span>).build();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Start with a default core-pool size and change it dynamically.</span></span><br><span class="line">    <span class="keyword">int</span> initSize = Math.min(INITIAL_THREAD_POOL_SIZE, maxThreadPoolSize);</span><br><span class="line">    threadPool = <span class="keyword">new</span> ThreadPoolExecutor(initSize, Integer.MAX_VALUE, <span class="number">1</span>,</span><br><span class="line">        TimeUnit.HOURS, <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;(), tf);</span><br><span class="line"></span><br><span class="line">    eventDispatcherThread = <span class="keyword">new</span> Thread() &#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ContainerEvent event = <span class="keyword">null</span>;</span><br><span class="line">        Set&lt;String&gt; allNodes = <span class="keyword">new</span> HashSet&lt;String&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (!stopped.get() &amp;&amp; !Thread.currentThread().isInterrupted()) &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">              <span class="comment">// 从阻塞队列 events 中取出 ContainerEvent 事件</span></span><br><span class="line">            event = events.take();</span><br><span class="line">          &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!stopped.get()) &#123;</span><br><span class="line">              LOG.error(<span class="string">&quot;Returning, thread interrupted&quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          allNodes.add(event.getNodeId().toString());</span><br><span class="line"></span><br><span class="line">          <span class="keyword">int</span> threadPoolSize = threadPool.getCorePoolSize();</span><br><span class="line"></span><br><span class="line">          <span class="comment">// We can increase the pool size only if haven&#x27;t reached the maximum</span></span><br><span class="line">          <span class="comment">// limit yet.</span></span><br><span class="line">          <span class="keyword">if</span> (threadPoolSize != maxThreadPoolSize) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// nodes where containers will run at *this* point of time. This is</span></span><br><span class="line">            <span class="comment">// *not* the cluster size and doesn&#x27;t need to be.</span></span><br><span class="line">            <span class="keyword">int</span> nodeNum = allNodes.size();</span><br><span class="line">            <span class="keyword">int</span> idealThreadPoolSize = Math.min(maxThreadPoolSize, nodeNum);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (threadPoolSize &lt; idealThreadPoolSize) &#123;</span><br><span class="line">              <span class="comment">// Bump up the pool size to idealThreadPoolSize +</span></span><br><span class="line">              <span class="comment">// INITIAL_POOL_SIZE, the later is just a buffer so we are not</span></span><br><span class="line">              <span class="comment">// always increasing the pool-size</span></span><br><span class="line">              <span class="keyword">int</span> newThreadPoolSize = Math.min(maxThreadPoolSize,</span><br><span class="line">                  idealThreadPoolSize + INITIAL_THREAD_POOL_SIZE);</span><br><span class="line">              LOG.info(<span class="string">&quot;Set NMClientAsync thread pool size to &quot;</span> +</span><br><span class="line">                  newThreadPoolSize + <span class="string">&quot; as the number of nodes to talk to is &quot;</span></span><br><span class="line">                  + nodeNum);</span><br><span class="line">              threadPool.setCorePoolSize(newThreadPoolSize);</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 重点：根据获取到的 Container 事件类型为 ContainerEventType.START_CONTAINER</span></span><br><span class="line">          <span class="comment">// getContainerEventProcessor(event) 返回一个 ContainerEventProcessor 线程对象，并在线程池中启动</span></span><br><span class="line">          threadPool.execute(getContainerEventProcessor(event));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="comment">// 启动线程</span></span><br><span class="line">    eventDispatcherThread.setName(<span class="string">&quot;Container  Event Dispatcher&quot;</span>);</span><br><span class="line">    eventDispatcherThread.setDaemon(<span class="keyword">false</span>);</span><br><span class="line">    eventDispatcherThread.start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">super</span>.serviceStart();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ContainerEventProcessor 处理器类是 NMClientAsyncImpl 类的内部类，继承自 Runnable 类，那我们来看看该类的 run() 方法，根据事件类型 ContainerEventType.START_CONTAINER 进入到对应的执行逻辑中，并通过 handle() 方法交给对应的状态机执行。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/async/impl/NMClientAsyncImpl.java</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      ContainerId containerId = event.getContainerId();</span><br><span class="line">      LOG.info(<span class="string">&quot;Processing Event &quot;</span> + event + <span class="string">&quot; for Container &quot;</span> + containerId);</span><br><span class="line">      <span class="comment">// 对 ContainerEventType.QUERY_CONTAINER 事件单独处理</span></span><br><span class="line">      <span class="keyword">if</span> (event.getType() == ContainerEventType.QUERY_CONTAINER) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          ContainerStatus containerStatus = client.getContainerStatus(</span><br><span class="line">              containerId, event.getNodeId());</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            callbackHandler.onContainerStatusReceived(</span><br><span class="line">                containerId, containerStatus);</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Throwable thr) &#123;</span><br><span class="line">            <span class="comment">// Don&#x27;t process user created unchecked exception</span></span><br><span class="line">            LOG.info(</span><br><span class="line">                <span class="string">&quot;Unchecked exception is thrown from onContainerStatusReceived&quot;</span> +</span><br><span class="line">                    <span class="string">&quot; for Container &quot;</span> + event.getContainerId(), thr);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (YarnException e) &#123;</span><br><span class="line">          onExceptionRaised(containerId, e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">          onExceptionRaised(containerId, e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">          onExceptionRaised(containerId, t);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// ContainerEventType.START_CONTAINER 和 ContainerEventType.STOP_CONTAINER 事件处理逻辑</span></span><br><span class="line">        StatefulContainer container = containers.get(containerId);</span><br><span class="line">        <span class="keyword">if</span> (container == <span class="keyword">null</span>) &#123;</span><br><span class="line">          LOG.info(<span class="string">&quot;Container &quot;</span> + containerId + <span class="string">&quot; is already stopped or failed&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 根据事件类型交给对应的状态机处理</span></span><br><span class="line">          container.handle(event);</span><br><span class="line">          <span class="keyword">if</span> (isCompletelyDone(container)) &#123;</span><br><span class="line">            containers.remove(containerId);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>ContainerEventType.START_CONTAINER 事件的注册状态机为 StartContainerTransition。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/async/impl/NMClientAsyncImpl.java</span></span><br><span class="line">    <span class="comment">// Transitions from PREP state</span></span><br><span class="line">    .addTransition(ContainerState.PREP,</span><br><span class="line">        EnumSet.of(ContainerState.RUNNING, ContainerState.FAILED),</span><br><span class="line">        ContainerEventType.START_CONTAINER,</span><br><span class="line">        <span class="keyword">new</span> StartContainerTransition())</span><br></pre></td></tr></table></figure>

<p>StartContainerTransition 状态机里的转换方法 transition()。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/client/api/async/impl/NMClientAsyncImpl.java</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> ContainerState <span class="title">transition</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">          StatefulContainer container, ContainerEvent event)</span> </span>&#123;</span><br><span class="line">        ContainerId containerId = event.getContainerId();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          StartContainerEvent scEvent = <span class="keyword">null</span>;</span><br><span class="line">          <span class="keyword">if</span> (event <span class="keyword">instanceof</span> StartContainerEvent) &#123;</span><br><span class="line">            scEvent = (StartContainerEvent) event;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">assert</span> scEvent != <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">          <span class="comment">//重点：调用 NMClient 类的 startContainer() 启动 Container</span></span><br><span class="line">          Map&lt;String, ByteBuffer&gt; allServiceResponse =</span><br><span class="line">              container.nmClientAsync.getClient().startContainer(</span><br><span class="line">                  scEvent.getContainer(), scEvent.getContainerLaunchContext());</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">              <span class="comment">// 通过回调的方式更新 Container 状态</span></span><br><span class="line">            container.nmClientAsync.getCallbackHandler().onContainerStarted(</span><br><span class="line">                containerId, allServiceResponse);</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Throwable thr) &#123;</span><br><span class="line">            <span class="comment">// Don&#x27;t process user created unchecked exception</span></span><br><span class="line">            LOG.info(<span class="string">&quot;Unchecked exception is thrown from onContainerStarted for &quot;</span></span><br><span class="line">                + <span class="string">&quot;Container &quot;</span> + containerId, thr);</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// 返回 Container 的 RUNNING 状态</span></span><br><span class="line">          <span class="keyword">return</span> ContainerState.RUNNING;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (YarnException e) &#123;</span><br><span class="line">          <span class="keyword">return</span> onExceptionRaised(container, event, e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">          <span class="keyword">return</span> onExceptionRaised(container, event, e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">          <span class="keyword">return</span> onExceptionRaised(container, event, t);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>

<p>在这里看到了激动人心的 startContainer() 方法，不过别急，这里还没有到真正的启动 Container 的时候，这里首先获取到 AM 真正与 NM 交互的客户端 NMClient，并调用其实现类 NMClientImpl 的 startContainer() 方法，获取到与 NM 交互的 RPC 协议 ContainerManagementProtocol，并通过其协议的 startContainers() 方法实现 RPC 远程调用，来实现 Container 的启动。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/client/api/impl/NMClientImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Map&lt;String, ByteBuffer&gt; <span class="title">startContainer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      Container container, ContainerLaunchContext containerLaunchContext)</span></span></span><br><span class="line"><span class="function">          <span class="keyword">throws</span> YarnException, IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 构建 StartContainer 对象</span></span><br><span class="line">    StartedContainer startingContainer =</span><br><span class="line">        <span class="keyword">new</span> StartedContainer(container.getId(), container.getNodeId());</span><br><span class="line">    <span class="keyword">synchronized</span> (startingContainer) &#123;</span><br><span class="line">      addStartingContainer(startingContainer);</span><br><span class="line">      </span><br><span class="line">      Map&lt;String, ByteBuffer&gt; allServiceResponse;</span><br><span class="line">      ContainerManagementProtocolProxyData proxy = <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        proxy =</span><br><span class="line">            cmProxy.getProxy(container.getNodeId().toString(),</span><br><span class="line">                container.getId());</span><br><span class="line">        StartContainerRequest scRequest =</span><br><span class="line">            StartContainerRequest.newInstance(containerLaunchContext,</span><br><span class="line">              container.getContainerToken());</span><br><span class="line">        List&lt;StartContainerRequest&gt; list = <span class="keyword">new</span> ArrayList&lt;StartContainerRequest&gt;();</span><br><span class="line">        list.add(scRequest);</span><br><span class="line">        StartContainersRequest allRequests =</span><br><span class="line">            StartContainersRequest.newInstance(list);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 重点：获取到 RPC 调用协议 ContainerManagementProtocol，并通过 RPC 函数 startContainers 启动 Container</span></span><br><span class="line">        StartContainersResponse response =</span><br><span class="line">            proxy</span><br><span class="line">                .getContainerManagementProtocol().startContainers(allRequests);</span><br><span class="line">        <span class="keyword">if</span> (response.getFailedRequests() != <span class="keyword">null</span></span><br><span class="line">            &amp;&amp; response.getFailedRequests().containsKey(container.getId())) &#123;</span><br><span class="line">          Throwable t =</span><br><span class="line">              response.getFailedRequests().get(container.getId()).deSerialize();</span><br><span class="line">          parseAndThrowException(t);</span><br><span class="line">        &#125;</span><br><span class="line">        allServiceResponse = response.getAllServicesMetaData();</span><br><span class="line">        startingContainer.state = ContainerState.RUNNING;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (YarnException e) &#123;</span><br><span class="line">        <span class="comment">// 省略异常的状态返回</span></span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (proxy != <span class="keyword">null</span>) &#123;</span><br><span class="line">          cmProxy.mayBeCloseProxy(proxy);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> allServiceResponse;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>NMClient 调用 RPC 函数 ContainerManagementProtocol#startContainers() 启动 Container。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> StartContainersResponse</span></span><br><span class="line"><span class="function">      <span class="title">startContainers</span><span class="params">(StartContainersRequest requests)</span> <span class="keyword">throws</span> YarnException,</span></span><br><span class="line"><span class="function">          IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (blockNewContainerRequests.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> NMNotYetReadyException(</span><br><span class="line">        <span class="string">&quot;Rejecting new containers as NodeManager has not&quot;</span></span><br><span class="line">            + <span class="string">&quot; yet connected with ResourceManager&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    UserGroupInformation remoteUgi = getRemoteUgi();</span><br><span class="line">    NMTokenIdentifier nmTokenIdentifier = selectNMTokenIdentifier(remoteUgi);</span><br><span class="line">    authorizeUser(remoteUgi,nmTokenIdentifier);</span><br><span class="line">    List&lt;ContainerId&gt; succeededContainers = <span class="keyword">new</span> ArrayList&lt;ContainerId&gt;();</span><br><span class="line">    Map&lt;ContainerId, SerializedException&gt; failedContainers =</span><br><span class="line">        <span class="keyword">new</span> HashMap&lt;ContainerId, SerializedException&gt;();</span><br><span class="line">    <span class="keyword">for</span> (StartContainerRequest request : requests.getStartContainerRequests()) &#123;</span><br><span class="line">      ContainerId containerId = <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        ContainerTokenIdentifier containerTokenIdentifier =</span><br><span class="line">            BuilderUtils.newContainerTokenIdentifier(request.getContainerToken());</span><br><span class="line">        verifyAndGetContainerTokenIdentifier(request.getContainerToken(),</span><br><span class="line">          containerTokenIdentifier);</span><br><span class="line">        containerId = containerTokenIdentifier.getContainerID();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动 Contain 的内部逻辑</span></span><br><span class="line">        startContainerInternal(nmTokenIdentifier, containerTokenIdentifier,</span><br><span class="line">          request);</span><br><span class="line">        succeededContainers.add(containerId);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (YarnException e) &#123;</span><br><span class="line">        failedContainers.put(containerId, SerializedException.newInstance(e));</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InvalidToken ie) &#123;</span><br><span class="line">        failedContainers.put(containerId, SerializedException.newInstance(ie));</span><br><span class="line">        <span class="keyword">throw</span> ie;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> RPCUtil.getRemoteException(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> StartContainersResponse.newInstance(getAuxServiceMetaData(),</span><br><span class="line">      succeededContainers, failedContainers);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，AM 与 NM 的交互流程已实现，通过  RPC 函数 ContainerManagementProtocol#startContainers() 来启动 Container，那 Container 又是如何在 NM 上启动的呢？这一块我们留在后面介绍。</p>
<h2 id="2-2-Container资源本地化"><a href="#2-2-Container资源本地化" class="headerlink" title="2.2 Container资源本地化"></a>2.2 Container资源本地化</h2><p>上面过程中 AM 通过调用 RPC 函数 ContainerManagementProtocol#startContainers() 开始启动 Container，这部分我们来看看具体的启动逻辑，即 startContainerInternal() 方法。这里做了两件事</p>
<ul>
<li>发送 ApplicationEventType.INIT_APPLICATION 事件，对应用程序资源的初始化，主要是初始化各类必需的服务组件（如日志记录组件 LogHandler、资源状态追踪组件 LocalResourcesTrackerImpl等），供后续 Container 启动，通常来自 ApplicationMaster 的第一个 Container 完成，后续的 Container 跳过这段 Application 初始化过程。</li>
<li>发送 ApplicationEventType.INIT_CONTAINER 事件，对 Container 进行初始化操作。（这部分事件留在 Container 启动环节介绍）</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startContainerInternal</span><span class="params">(NMTokenIdentifier nmTokenIdentifier,</span></span></span><br><span class="line"><span class="params"><span class="function">      ContainerTokenIdentifier containerTokenIdentifier,</span></span></span><br><span class="line"><span class="params"><span class="function">      StartContainerRequest request)</span> <span class="keyword">throws</span> YarnException, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略Token认证及ContainerLaunchContext上下文初始化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.readLock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (!serviceStopped) &#123;</span><br><span class="line">        <span class="comment">// Create the application</span></span><br><span class="line">        Application application =</span><br><span class="line">            <span class="keyword">new</span> ApplicationImpl(dispatcher, user, applicationID, credentials, context);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 应用程序的初始化，供后续Container使用，这个逻辑只调用一次，通常由来自ApplicationMaster的第一个Container完成</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == context.getApplications().putIfAbsent(applicationID,</span><br><span class="line">          application)) &#123;</span><br><span class="line">          LOG.info(<span class="string">&quot;Creating a new application reference for app &quot;</span> + applicationID);</span><br><span class="line">          LogAggregationContext logAggregationContext =</span><br><span class="line">              containerTokenIdentifier.getLogAggregationContext();</span><br><span class="line">          Map&lt;ApplicationAccessType, String&gt; appAcls =</span><br><span class="line">              container.getLaunchContext().getApplicationACLs();</span><br><span class="line">          context.getNMStateStore().storeApplication(applicationID,</span><br><span class="line">              buildAppProto(applicationID, user, credentials, appAcls,</span><br><span class="line">                logAggregationContext));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">          <span class="comment">// 1.向 ApplicationImpl 发送 ApplicationEventType.INIT_APPLICATION 事件</span></span><br><span class="line">          dispatcher.getEventHandler().handle(</span><br><span class="line">            <span class="keyword">new</span> ApplicationInitEvent(applicationID, appAcls,</span><br><span class="line">              logAggregationContext));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.向 ApplicationImpl 发送 ApplicationEventType.INIT_CONTAINER 事件</span></span><br><span class="line">        <span class="keyword">this</span>.context.getNMStateStore().storeContainer(containerId, request);</span><br><span class="line">        dispatcher.getEventHandler().handle(</span><br><span class="line">          <span class="keyword">new</span> ApplicationContainerInitEvent(container));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.context.getContainerTokenSecretManager().startContainerSuccessful(</span><br><span class="line">          containerTokenIdentifier);</span><br><span class="line">        NMAuditLogger.logSuccess(user, AuditConstants.START_CONTAINER,</span><br><span class="line">          <span class="string">&quot;ContainerManageImpl&quot;</span>, applicationID, containerId);</span><br><span class="line">        <span class="comment">// TODO launchedContainer misplaced -&gt; doesn&#x27;t necessarily mean a container</span></span><br><span class="line">        <span class="comment">// launch. A finished Application will not launch containers.</span></span><br><span class="line">        metrics.launchedContainer();</span><br><span class="line">        metrics.allocateContainer(containerTokenIdentifier.getResource());</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> YarnException(</span><br><span class="line">            <span class="string">&quot;Container start failed as the NodeManager is &quot;</span> +</span><br><span class="line">            <span class="string">&quot;in the process of shutting down&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">this</span>.readLock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ApplicationEventType.INIT_APPLICATION 事件的状态转换过程，状态由 NEW 转变为 INITING，对应的状态机为 AppInitTransition。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java</span></span><br><span class="line">           <span class="comment">// Transitions from NEW state</span></span><br><span class="line">     .addTransition(ApplicationState.NEW, ApplicationState.INITING,</span><br><span class="line">         ApplicationEventType.INIT_APPLICATION, <span class="keyword">new</span> AppInitTransition())</span><br></pre></td></tr></table></figure>

<p>AppInitTransition 状态机设置 ACL 属性后，并向 LogHandler（目前有两种实现方式，分别是 LogAggregationService 和 NonAggregatingLogHandler 发送一个 LogHandlerEventType.APPLICATION_STARTED 事件。 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java</span></span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AppInitTransition</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">SingleArcTransition</span>&lt;<span class="title">ApplicationImpl</span>, <span class="title">ApplicationEvent</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(ApplicationImpl app, ApplicationEvent event)</span> </span>&#123;</span><br><span class="line">      ApplicationInitEvent initEvent = (ApplicationInitEvent)event;</span><br><span class="line">      <span class="comment">// 设置 ACL 属性</span></span><br><span class="line">      app.applicationACLs = initEvent.getApplicationACLs();</span><br><span class="line">      app.aclsManager.addApplication(app.getAppId(), app.applicationACLs);</span><br><span class="line">      <span class="comment">// Inform the logAggregator</span></span><br><span class="line">      app.logAggregationContext = initEvent.getLogAggregationContext();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 向 LogHandler 发送 LogHandlerEventType.APPLICATION_STARTED 事件</span></span><br><span class="line">      app.dispatcher.getEventHandler().handle(</span><br><span class="line">          <span class="keyword">new</span> LogHandlerAppStartedEvent(app.appId, app.user,</span><br><span class="line">              app.credentials, ContainerLogsRetentionPolicy.ALL_CONTAINERS,</span><br><span class="line">              app.applicationACLs, app.logAggregationContext)); </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里以 LogAggregationService 服务为例，当 LogHandler 收到 ApplicationEventType.APPLICATION_LOG_HANDLING_INITED 事件后，将创建应用程序日志目录、设置目录权限等。然后向 ApplicationImpl 发送一个 ApplicationEventType.APPLICATION_LOG_HANDLING_INITED 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(LogHandlerEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">      <span class="keyword">case</span> APPLICATION_STARTED:</span><br><span class="line">        LogHandlerAppStartedEvent appStartEvent =</span><br><span class="line">            (LogHandlerAppStartedEvent) event;</span><br><span class="line">        <span class="comment">// 事情处理逻辑</span></span><br><span class="line">        initApp(appStartEvent.getApplicationId(), appStartEvent.getUser(),</span><br><span class="line">            appStartEvent.getCredentials(),</span><br><span class="line">            appStartEvent.getLogRetentionPolicy(),</span><br><span class="line">            appStartEvent.getApplicationAcls(),</span><br><span class="line">            appStartEvent.getLogAggregationContext());</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> CONTAINER_FINISHED:  <span class="comment">// 省略</span></span><br><span class="line">      <span class="keyword">case</span> APPLICATION_FINISHED:  <span class="comment">// 省略</span></span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        ; <span class="comment">// Ignore</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initApp</span><span class="params">(<span class="keyword">final</span> ApplicationId appId, String user,</span></span></span><br><span class="line"><span class="params"><span class="function">      Credentials credentials, ContainerLogsRetentionPolicy logRetentionPolicy,</span></span></span><br><span class="line"><span class="params"><span class="function">      Map&lt;ApplicationAccessType, String&gt; appAcls,</span></span></span><br><span class="line"><span class="params"><span class="function">      LogAggregationContext logAggregationContext)</span> </span>&#123;</span><br><span class="line">    ApplicationEvent eventResponse;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 创建应用程序日志目录、设置目录权限等</span></span><br><span class="line">      verifyAndCreateRemoteLogDir(getConfig());</span><br><span class="line">      initAppAggregator(appId, user, credentials, logRetentionPolicy, appAcls,</span><br><span class="line">          logAggregationContext);</span><br><span class="line">      eventResponse = <span class="keyword">new</span> ApplicationEvent(appId,</span><br><span class="line">          ApplicationEventType.APPLICATION_LOG_HANDLING_INITED);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (YarnRuntimeException e) &#123;</span><br><span class="line">      LOG.warn(<span class="string">&quot;Application failed to init aggregation&quot;</span>, e);</span><br><span class="line">      eventResponse = <span class="keyword">new</span> ApplicationEvent(appId,</span><br><span class="line">          ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 向 ApplicationImpl 发送 ApplicationEventType.APPLICATION_LOG_HANDLING_INITED 事件</span></span><br><span class="line">    <span class="keyword">this</span>.dispatcher.getEventHandler().handle(eventResponse);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ApplicationImpl 收到 ApplicationEventType.APPLICATION_LOG_HANDLING_INITED 事件后，直接向 ResourceLocalizationService 发送 LocalizationEventType.INIT_APPLICATION_RESOURCES 事件，此时 ApplicationImpl 仍处于 INITING 状态。ResourceLocalizationService 收到事件请求时进入到 handle() 逻辑处理，这里会创建一个 LocalResourcesTrackerImpl 对象，为接下来资源下载做准备，并向 ApplicationImpl 发送一个 ApplicationEventType.APPLICATION_INITED 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(LocalizationEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> create log dir as $logdir/$user/$appId</span></span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">    <span class="keyword">case</span> INIT_APPLICATION_RESOURCES:  <span class="comment">// 处理 LocalizationEventType.INIT_APPLICATION_RESOURCES 事件</span></span><br><span class="line">      handleInitApplicationResources(</span><br><span class="line">          ((ApplicationLocalizationEvent)event).getApplication());</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> INIT_CONTAINER_RESOURCES: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CONTAINER_RESOURCES_LOCALIZED:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CACHE_CLEANUP:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CLEANUP_CONTAINER_RESOURCES: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> DESTROY_APPLICATION_RESOURCES: <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(<span class="string">&quot;Unknown localization event: &quot;</span> + event);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handleInitApplicationResources</span><span class="params">(Application app)</span> </span>&#123;</span><br><span class="line">    String userName = app.getUser();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 创建 LocalResourcesTrackerImpl 对象，为接下来的资源下载做准备</span></span><br><span class="line">    privateRsrc.putIfAbsent(userName, <span class="keyword">new</span> LocalResourcesTrackerImpl(userName,</span><br><span class="line">        <span class="keyword">null</span>, dispatcher, <span class="keyword">true</span>, <span class="keyword">super</span>.getConfig(), stateStore));</span><br><span class="line">    String appIdStr = ConverterUtils.toString(app.getAppId());</span><br><span class="line">    appRsrc.putIfAbsent(appIdStr, <span class="keyword">new</span> LocalResourcesTrackerImpl(app.getUser(),</span><br><span class="line">        app.getAppId(), dispatcher, <span class="keyword">false</span>, <span class="keyword">super</span>.getConfig(), stateStore));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 向 ApplicationImpl 发送 ApplicationEventType.APPLICATION_INITED 事件</span></span><br><span class="line">    dispatcher.getEventHandler().handle(<span class="keyword">new</span> ApplicationInitedEvent(</span><br><span class="line">          app.getAppId()));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ApplicationImpl 收到 ApplicationEventType.APPLICATION_INITED 事件后，依次向该应用程序已经保持的所有 Container 发送一个 INIT_CONTAINER 事件以通知它们进行初始化。此时，ApplicationImpl 运行状态由 INITING 转换为 RUNNING。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java</span></span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AppInitDoneTransition</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">SingleArcTransition</span>&lt;<span class="title">ApplicationImpl</span>, <span class="title">ApplicationEvent</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(ApplicationImpl app, ApplicationEvent event)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// Start all the containers waiting for ApplicationInit</span></span><br><span class="line">      <span class="keyword">for</span> (Container container : app.containers.values()) &#123;</span><br><span class="line">        <span class="comment">// 向应用程序保存的 Container 发送 INIT_CONTAINER 事件</span></span><br><span class="line">        app.dispatcher.getEventHandler().handle(<span class="keyword">new</span> ContainerInitEvent(</span><br><span class="line">              container.getContainerId()));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ContainerImpl 收到 INIT_CONTAINER 事件后，先向附属服务 AuxServices 发送 APPLICATION_INIT 事件，以通知它有新的应用程序 Container 启动，然后从 ContainerLaunchContext 中获取各类可见性资源，并保存到 ContainerImpl 中特定的数据结构中，之后向 ResourceLocalizationService 发送 LocalizationEventType.INIT_CONTAINER_RESOURCES 事件，此时 ContainerImpl 运行状态已由 NEW 转换为 LOCALIZING。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java</span></span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RequestResourcesTransition</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">MultipleArcTransition</span>&lt;<span class="title">ContainerImpl</span>,<span class="title">ContainerEvent</span>,<span class="title">ContainerState</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ContainerState <span class="title">transition</span><span class="params">(ContainerImpl container,</span></span></span><br><span class="line"><span class="params"><span class="function">        ContainerEvent event)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// 向 AuxService 发送 AuxServicesEventType.CONTAINER_INIT 事件</span></span><br><span class="line">      container.dispatcher.getEventHandler().handle(<span class="keyword">new</span> AuxServicesEvent</span><br><span class="line">          (AuxServicesEventType.CONTAINER_INIT, container));</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Inform the AuxServices about the opaque serviceData</span></span><br><span class="line">      Map&lt;String,ByteBuffer&gt; csd = ctxt.getServiceData();</span><br><span class="line">      <span class="keyword">if</span> (csd != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// This can happen more than once per Application as each container may</span></span><br><span class="line">        <span class="comment">// have distinct service data</span></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String,ByteBuffer&gt; service : csd.entrySet()) &#123;</span><br><span class="line">          container.dispatcher.getEventHandler().handle(</span><br><span class="line">              <span class="keyword">new</span> AuxServicesEvent(AuxServicesEventType.APPLICATION_INIT,</span><br><span class="line">                  container.user, container.containerId</span><br><span class="line">                      .getApplicationAttemptId().getApplicationId(),</span><br><span class="line">                  service.getKey().toString(), service.getValue()));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      container.containerLocalizationStartTime = clock.getTime();</span><br><span class="line">      <span class="comment">// 从 ContainerLaunchContext 获取各类资源，并保持在数据结构中</span></span><br><span class="line">      Map&lt;String,LocalResource&gt; cntrRsrc = ctxt.getLocalResources();</span><br><span class="line">      <span class="keyword">if</span> (!cntrRsrc.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">for</span> (Map.Entry&lt;String,LocalResource&gt; rsrc : cntrRsrc.entrySet()) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              LocalResourceRequest req =</span><br><span class="line">                  <span class="keyword">new</span> LocalResourceRequest(rsrc.getValue());</span><br><span class="line">              List&lt;String&gt; links = container.pendingResources.get(req);</span><br><span class="line">              <span class="keyword">if</span> (links == <span class="keyword">null</span>) &#123;</span><br><span class="line">                links = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">                container.pendingResources.put(req, links);</span><br><span class="line">              &#125;</span><br><span class="line">              links.add(rsrc.getKey());</span><br><span class="line">              <span class="keyword">switch</span> (rsrc.getValue().getVisibility()) &#123;</span><br><span class="line">              <span class="keyword">case</span> PUBLIC:</span><br><span class="line">                container.publicRsrcs.add(req);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">              <span class="keyword">case</span> PRIVATE:</span><br><span class="line">                container.privateRsrcs.add(req);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">              <span class="keyword">case</span> APPLICATION:</span><br><span class="line">                container.appRsrcs.add(req);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (URISyntaxException e) &#123;</span><br><span class="line">              LOG.info(<span class="string">&quot;Got exception parsing &quot;</span> + rsrc.getKey()</span><br><span class="line">                  + <span class="string">&quot; and value &quot;</span> + rsrc.getValue());</span><br><span class="line">              <span class="keyword">throw</span> e;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; </span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 向 ResourceLocalizationService 发送 LocalizationEventType.INIT_CONTAINER_RESOURCES 事件</span></span><br><span class="line">        container.dispatcher.getEventHandler().handle(</span><br><span class="line">              <span class="keyword">new</span> ContainerLocalizationRequestEvent(container, req));</span><br><span class="line">        <span class="keyword">return</span> ContainerState.LOCALIZING;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 这种情况是 Contaienr 已经进行了资源初始化操作，这里直接运行 Container</span></span><br><span class="line">        container.sendLaunchEvent();</span><br><span class="line">        container.metrics.endInitingContainer();</span><br><span class="line">        <span class="keyword">return</span> ContainerState.LOCALIZED;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ResourceLocalizationService 收到 LocalizationEventType.INIT_CONTAINER_RESOURCES 事件后，依次将 Container 所需的资源封装成一个 REQUEST 事件，发送给对应的资源状态追踪器 LocalResourcesTrackerImpl。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(LocalizationEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> create log dir as $logdir/$user/$appId</span></span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">    <span class="keyword">case</span> INIT_APPLICATION_RESOURCES:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> INIT_CONTAINER_RESOURCES:</span><br><span class="line">       <span class="comment">// 将 Container 所需的资源单独封装成一个 REQUEST 事件，发送给对应的资源状态跟踪器 LocalResourcesTrackerImpl</span></span><br><span class="line">      handleInitContainerResources((ContainerLocalizationRequestEvent) event);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> CONTAINER_RESOURCES_LOCALIZED:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CACHE_CLEANUP:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CLEANUP_CONTAINER_RESOURCES:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> DESTROY_APPLICATION_RESOURCES:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(<span class="string">&quot;Unknown localization event: &quot;</span> + event);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>LocalResourcesTrackerImpl 收到 REQUEST 事件后，将为对应的资源创建一个状态机对象 LocalizeResource 以跟踪资源的生命周期，并将 REQUEST 事件进一步传送给 LocalizedResource。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalizedResource.java</span></span><br><span class="line">    <span class="comment">// From INIT (ref == 0, awaiting req)</span></span><br><span class="line">    .addTransition(ResourceState.INIT, ResourceState.DOWNLOADING,</span><br><span class="line">        ResourceEventType.REQUEST, <span class="keyword">new</span> FetchResourceTransition())</span><br></pre></td></tr></table></figure>

<p>LocalizedResource 收到 REQUEST 事件后，将待下载资源信息通过 LocalizerEventType.REQUEST_RESOURCE_LOCALIZATION 事件发送给资源下载服务 ResourceLocalizationService，之后 LocalizedResource 状态由 NEW 转换为 DOWNLOADING。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalizedResource.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FetchResourceTransition</span> <span class="keyword">extends</span> <span class="title">ResourceTransition</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(LocalizedResource rsrc, ResourceEvent event)</span> </span>&#123;</span><br><span class="line">      ResourceRequestEvent req = (ResourceRequestEvent) event;</span><br><span class="line">      LocalizerContext ctxt = req.getContext();</span><br><span class="line">      ContainerId container = ctxt.getContainerId();</span><br><span class="line">      rsrc.ref.add(container);</span><br><span class="line">      rsrc.dispatcher.getEventHandler().handle(</span><br><span class="line">          <span class="keyword">new</span> LocalizerResourceRequestEvent(rsrc, req.getVisibility(), ctxt, </span><br><span class="line">              req.getLocalResourceRequest().getPattern()));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ResourceLocalizationService 收到 LocalizerEventType.REQUEST_RESOURCE_LOCALIZATION 事件后，将交给 LocalizerTracker 服务处理，如果是 PUBLIC 资源，则统一交给 PublicLocalizer 处理，否则检查是否已经为该 Container 创建了 LocalizerRunner 线程，如果没有，则创建一个，否则直接添加到该线程的下载队列中。该线程会调用 ContainerExecutor#startLocalizer() 函数下载资源，该函数通过协议 LocalizationProtocol 与 ResourceLocalizationService 通信，以顺序获取待下载资源位置下载。待资源下载完成后，PublicLocalize 或者 LocalizerRunner 都会向 LocalizedResource 发送一个 LOCALIZED 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(LocalizerEvent event)</span> </span>&#123;</span><br><span class="line">      String locId = event.getLocalizerId();</span><br><span class="line">      <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">      <span class="keyword">case</span> REQUEST_RESOURCE_LOCALIZATION:</span><br><span class="line">        <span class="comment">// 0) find running localizer or start new thread</span></span><br><span class="line">        LocalizerResourceRequestEvent req =</span><br><span class="line">          (LocalizerResourceRequestEvent)event;</span><br><span class="line">        <span class="comment">//根据 REQUEST 资源判断资源的可见性</span></span><br><span class="line">        <span class="keyword">switch</span> (req.getVisibility()) &#123;</span><br><span class="line">        <span class="comment">// 如果是 PUBLIC 资源，则交给线程 PublicLocalizer 处理</span></span><br><span class="line">        <span class="keyword">case</span> PUBLIC:</span><br><span class="line">          publicLocalizer.addResource(req);</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> PRIVATE:</span><br><span class="line">        <span class="keyword">case</span> APPLICATION:</span><br><span class="line">          <span class="keyword">synchronized</span> (privLocalizers) &#123;</span><br><span class="line">            LocalizerRunner localizer = privLocalizers.get(locId);</span><br><span class="line">            <span class="comment">// 检查是否创建了 LocalizerRunner 线程</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> == localizer) &#123;</span><br><span class="line">              LOG.info(<span class="string">&quot;Created localizer for &quot;</span> + locId);</span><br><span class="line">              localizer = <span class="keyword">new</span> LocalizerRunner(req.getContext(), locId);</span><br><span class="line">              privLocalizers.put(locId, localizer);</span><br><span class="line">              localizer.start();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 1) propagate event</span></span><br><span class="line">            localizer.addResource(req);</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>LocalizedResource 收到 LOCALIZED 事件后，会向 ContainerImpl 发送一个 ContainerEventType.RESOURCE_LOCALIZED 事件，并且将状态从 DOWNLOADING 转换为 LOCALIZED。ContainerImpl 收到事件后，会检查所依赖的资源是否全部下载完毕，如果下载完成则向 ContainersLauncher 服务发送一个 LAUNCH_CONTAINER 事件，以启动对应 Container。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalizedResource.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FetchSuccessTransition</span> <span class="keyword">extends</span> <span class="title">ResourceTransition</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(LocalizedResource rsrc, ResourceEvent event)</span> </span>&#123;</span><br><span class="line">      ResourceLocalizedEvent locEvent = (ResourceLocalizedEvent) event;</span><br><span class="line">      rsrc.localPath =</span><br><span class="line">          Path.getPathWithoutSchemeAndAuthority(locEvent.getLocation());</span><br><span class="line">      rsrc.size = locEvent.getSize();</span><br><span class="line">      <span class="keyword">for</span> (ContainerId container : rsrc.ref) &#123;</span><br><span class="line">        <span class="comment">// 向 ContainerImpl 发送 ContainerEventType.RESOURCE_LOCALIZED 事件</span></span><br><span class="line">        rsrc.dispatcher.getEventHandler().handle(</span><br><span class="line">            <span class="keyword">new</span> ContainerResourceLocalizedEvent(</span><br><span class="line">              container, rsrc.rsrc, rsrc.localPath));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，Container 资源本地化资源已下载完毕，接下来就开始启动和运行 Container。</p>
<h2 id="2-3-启动和运行Container"><a href="#2-3-启动和运行Container" class="headerlink" title="2.3 启动和运行Container"></a>2.3 启动和运行Container</h2><p>Container 运行是由 ContainersLauncher 服务实现的，主要过程可概括为：将待运行的 Container 所需的环境和运行命令写到 Shell 脚本 launch_container.sh 脚本中，并将启动该脚本的命令写入 default_container_executro.sh 中，然后通过该脚本启动 Container。之所以要将 Container 运行命令写到脚本中并通过运行脚本来执行它，主要是直接执行命令可能让一些特殊符号发生转义。</p>
<p>上面主要介绍 startContainerInternal() 的第一个事件处理，接下来看第一个事件的处理，以及如何启动和运行 Container。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startContainerInternal</span><span class="params">(NMTokenIdentifier nmTokenIdentifier,</span></span></span><br><span class="line"><span class="params"><span class="function">      ContainerTokenIdentifier containerTokenIdentifier,</span></span></span><br><span class="line"><span class="params"><span class="function">      StartContainerRequest request)</span> <span class="keyword">throws</span> YarnException, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略Token认证及ContainerLaunchContext上下文初始化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.readLock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (!serviceStopped) &#123;</span><br><span class="line">        <span class="comment">// Create the application</span></span><br><span class="line">        Application application =</span><br><span class="line">            <span class="keyword">new</span> ApplicationImpl(dispatcher, user, applicationID, credentials, context);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 应用程序的初始化，供后续Container使用，这个逻辑只调用一次，通常由来自ApplicationMaster的第一个Container完成</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == context.getApplications().putIfAbsent(applicationID,</span><br><span class="line">          application)) &#123;</span><br><span class="line">          LOG.info(<span class="string">&quot;Creating a new application reference for app &quot;</span> + applicationID);</span><br><span class="line">          LogAggregationContext logAggregationContext =</span><br><span class="line">              containerTokenIdentifier.getLogAggregationContext();</span><br><span class="line">          Map&lt;ApplicationAccessType, String&gt; appAcls =</span><br><span class="line">              container.getLaunchContext().getApplicationACLs();</span><br><span class="line">          context.getNMStateStore().storeApplication(applicationID,</span><br><span class="line">              buildAppProto(applicationID, user, credentials, appAcls,</span><br><span class="line">                logAggregationContext));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">          <span class="comment">// 1.向 ApplicationImpl 发送 ApplicationEventType.INIT_APPLICATION 事件</span></span><br><span class="line">          dispatcher.getEventHandler().handle(</span><br><span class="line">            <span class="keyword">new</span> ApplicationInitEvent(applicationID, appAcls,</span><br><span class="line">              logAggregationContext));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.向 ApplicationImpl 发送 ApplicationEventType.INIT_CONTAINER 事件</span></span><br><span class="line">        <span class="keyword">this</span>.context.getNMStateStore().storeContainer(containerId, request);</span><br><span class="line">        dispatcher.getEventHandler().handle(</span><br><span class="line">          <span class="keyword">new</span> ApplicationContainerInitEvent(container));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.context.getContainerTokenSecretManager().startContainerSuccessful(</span><br><span class="line">          containerTokenIdentifier);</span><br><span class="line">        NMAuditLogger.logSuccess(user, AuditConstants.START_CONTAINER,</span><br><span class="line">          <span class="string">&quot;ContainerManageImpl&quot;</span>, applicationID, containerId);</span><br><span class="line">        <span class="comment">// TODO launchedContainer misplaced -&gt; doesn&#x27;t necessarily mean a container</span></span><br><span class="line">        <span class="comment">// launch. A finished Application will not launch containers.</span></span><br><span class="line">        metrics.launchedContainer();</span><br><span class="line">        metrics.allocateContainer(containerTokenIdentifier.getResource());</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> YarnException(</span><br><span class="line">            <span class="string">&quot;Container start failed as the NodeManager is &quot;</span> +</span><br><span class="line">            <span class="string">&quot;in the process of shutting down&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">this</span>.readLock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里触发了 Application 的事件 ApplicationEventType.INIT_CONTAINER，下面是该事件的状态转换过程及对应注册的状态机。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java</span></span><br><span class="line">    <span class="comment">// Transitions from NEW state</span></span><br><span class="line">    .addTransition(ApplicationState.NEW, ApplicationState.NEW,</span><br><span class="line">        ApplicationEventType.INIT_CONTAINER,</span><br><span class="line">        <span class="keyword">new</span> InitContainerTransition())</span><br></pre></td></tr></table></figure>

<p>InitContainerTransition 状态机的处理逻辑。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java</span></span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">InitContainerTransition</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">SingleArcTransition</span>&lt;<span class="title">ApplicationImpl</span>, <span class="title">ApplicationEvent</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(ApplicationImpl app, ApplicationEvent event)</span> </span>&#123;</span><br><span class="line">      ApplicationContainerInitEvent initEvent =</span><br><span class="line">        (ApplicationContainerInitEvent) event;</span><br><span class="line">      Container container = initEvent.getContainer();</span><br><span class="line">      app.containers.put(container.getContainerId(), container);</span><br><span class="line">      LOG.info(<span class="string">&quot;Adding &quot;</span> + container.getContainerId()</span><br><span class="line">          + <span class="string">&quot; to application &quot;</span> + app.toString());</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">switch</span> (app.getApplicationState()) &#123;</span><br><span class="line">      <span class="keyword">case</span> RUNNING:</span><br><span class="line">        <span class="comment">// 应用程序提交后app是RUNNING状态，这里向调度器发送 ContainerEventType.INIT_CONTAINER 事件</span></span><br><span class="line">        app.dispatcher.getEventHandler().handle(<span class="keyword">new</span> ContainerInitEvent(</span><br><span class="line">            container.getContainerId()));</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> INITING:</span><br><span class="line">      <span class="keyword">case</span> NEW:</span><br><span class="line">        <span class="comment">// these get queued up and sent out in AppInitDoneTransition</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">false</span> : <span class="string">&quot;Invalid state for InitContainerTransition: &quot;</span> +</span><br><span class="line">            app.getApplicationState();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ContainerEventType.INIT_CONTAINER 事件对应的状态转换及注册的状态机。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java</span></span><br><span class="line">    <span class="comment">// From NEW State</span></span><br><span class="line">    .addTransition(ContainerState.NEW,</span><br><span class="line">        EnumSet.of(ContainerState.LOCALIZING,</span><br><span class="line">            ContainerState.LOCALIZED,</span><br><span class="line">            ContainerState.LOCALIZATION_FAILED,</span><br><span class="line">            ContainerState.DONE),</span><br><span class="line">        ContainerEventType.INIT_CONTAINER, <span class="keyword">new</span> RequestResourcesTransition())</span><br></pre></td></tr></table></figure>

<p>RequestResourcesTransition 状态机行为的关键在于 sendLaunchEvent() 方法的调用，发送 Container 启动的事情请求，向调度器发送 ContainersLauncherEventType.LAUNCH_CONTAINER 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java</span></span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RequestResourcesTransition</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">MultipleArcTransition</span>&lt;<span class="title">ContainerImpl</span>,<span class="title">ContainerEvent</span>,<span class="title">ContainerState</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ContainerState <span class="title">transition</span><span class="params">(ContainerImpl container,</span></span></span><br><span class="line"><span class="params"><span class="function">        ContainerEvent event)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 省略一些检查逻辑</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 重点：发送启动Container的操作</span></span><br><span class="line">        container.sendLaunchEvent();</span><br><span class="line">        container.metrics.endInitingContainer();</span><br><span class="line">        <span class="keyword">return</span> ContainerState.LOCALIZED;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendLaunchEvent</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ContainersLauncherEventType launcherEvent =</span><br><span class="line">        ContainersLauncherEventType.LAUNCH_CONTAINER;</span><br><span class="line">    <span class="keyword">if</span> (recoveredStatus == RecoveredContainerStatus.LAUNCHED) &#123;</span><br><span class="line">      <span class="comment">// try to recover a container that was previously launched</span></span><br><span class="line">      launcherEvent = ContainersLauncherEventType.RECOVER_CONTAINER;</span><br><span class="line">    &#125;</span><br><span class="line">    containerLaunchStartTime = clock.getTime();</span><br><span class="line">    <span class="comment">// 向调度器发送 ContainersLauncherEventType.LAUNCH_CONTAINER 事件请求</span></span><br><span class="line">    dispatcher.getEventHandler().handle(</span><br><span class="line">        <span class="keyword">new</span> ContainersLauncherEvent(<span class="keyword">this</span>, launcherEvent));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里向调度器发送 ContainersLauncherEventType.LAUNCH_CONTAINER 事件请求，之前发送事件状态转换过程不太一样，在代码中我们找到该事件的状态转换过程及注册状态机，那是由谁来处理这个事件请求呢？我们就需要看看 ContainersLauncherEventType 事件类注册的地方。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ContainerManagerImpl</span><span class="params">(Context context, ContainerExecutor exec,</span></span></span><br><span class="line"><span class="params"><span class="function">      DeletionService deletionContext, NodeStatusUpdater nodeStatusUpdater,</span></span></span><br><span class="line"><span class="params"><span class="function">      NodeManagerMetrics metrics, ApplicationACLsManager aclsManager,</span></span></span><br><span class="line"><span class="params"><span class="function">      LocalDirsHandlerService dirsHandler)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    dispatcher.register(ContainerEventType.class,</span><br><span class="line">        <span class="keyword">new</span> ContainerEventDispatcher());</span><br><span class="line">    dispatcher.register(ApplicationEventType.class,</span><br><span class="line">        <span class="keyword">new</span> ApplicationEventDispatcher());</span><br><span class="line">    dispatcher.register(LocalizationEventType.class, rsrcLocalizationSrvc);</span><br><span class="line">    dispatcher.register(AuxServicesEventType.class, auxiliaryServices);</span><br><span class="line">    dispatcher.register(ContainersMonitorEventType.class, containersMonitor);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ContainersLauncherEventType 事件类的注册方法</span></span><br><span class="line">    dispatcher.register(ContainersLauncherEventType.class, containersLauncher);</span><br><span class="line">    </span><br><span class="line">    addService(dispatcher);</span><br><span class="line"></span><br><span class="line">    ReentrantReadWriteLock lock = <span class="keyword">new</span> ReentrantReadWriteLock();</span><br><span class="line">    <span class="keyword">this</span>.readLock = lock.readLock();</span><br><span class="line">    <span class="keyword">this</span>.writeLock = lock.writeLock();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>可以看出 ContainersLauncherEventType 事件类型类注册的事件处理器为 ContainersLauncher 类，那该类又是如何处理 ContainersLauncherEventType.LAUNCH_CONTAINER 事件请求呢？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：rg/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainersLauncher.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(ContainersLauncherEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> ContainersLauncher launches containers one by one!!</span></span><br><span class="line">    Container container = event.getContainer();</span><br><span class="line">    ContainerId containerId = container.getContainerId();</span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">      <span class="keyword">case</span> LAUNCH_CONTAINER:</span><br><span class="line">        Application app =</span><br><span class="line">          context.getApplications().get(</span><br><span class="line">              containerId.getApplicationAttemptId().getApplicationId());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// LAUNCH_CONTAINER 事件的处理逻辑，创建 ContainerLaunch 线程并启动线程</span></span><br><span class="line">        ContainerLaunch launch =</span><br><span class="line">            <span class="keyword">new</span> ContainerLaunch(context, getConfig(), dispatcher, exec, app,</span><br><span class="line">              event.getContainer(), dirsHandler, containerManager);</span><br><span class="line">        containerLauncher.submit(launch);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将其加入到运行的 Container 数据结构 running 中</span></span><br><span class="line">        running.put(containerId, launch);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> RECOVER_CONTAINER: <span class="comment">// 省略</span></span><br><span class="line">      <span class="keyword">case</span> CLEANUP_CONTAINER: <span class="comment">//省略</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里的 ContainerLaunch 类是真正启动 Container 的类，ContainerLaunch 类继承自 Callable 类，线程启动的方式是通过 submit() 方法提交，调用 Callable 类的实现方法 call() 来真正执行线程。启动过程主要做了三件事：</p>
<ul>
<li>准备 Container 的执行环境；</li>
<li>更新 Container 状态，从 LOCALIZED 转换为 RUNNING；</li>
<li>调用 ContainerExecutor 对象在 NM 节点上启动 Container</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> ContainerLaunchContext launchContext = container.getLaunchContext();</span><br><span class="line">    </span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 启动 Container 前的准备工作：（省略）</span></span><br><span class="line"><span class="comment">      * 1.shell启动脚本的封装与拓展（添加自定义脚本）</span></span><br><span class="line"><span class="comment">      * 2.创建本地工作目录</span></span><br><span class="line"><span class="comment">      * 3.设置token的保存路径</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">   </span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 由于 call() 方法调用是阻塞的，这里先发送 ContainerEventType.CONTAINER_LAUNCHED 事件，将 Container 状态从LOCALIZED 转换为 RUNNING</span></span><br><span class="line">          dispatcher.getEventHandler().handle(<span class="keyword">new</span> ContainerEvent(</span><br><span class="line">                containerID,</span><br><span class="line">                ContainerEventType.CONTAINER_LAUNCHED));</span><br><span class="line">          context.getNMStateStore().storeContainerLaunched(containerID);</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Check if the container is signalled to be killed.</span></span><br><span class="line">          <span class="keyword">if</span> (!shouldLaunchContainer.compareAndSet(<span class="keyword">false</span>, <span class="keyword">true</span>)) &#123;</span><br><span class="line">            LOG.info(<span class="string">&quot;Container &quot;</span> + containerIdStr + <span class="string">&quot; not launched as &quot;</span></span><br><span class="line">                + <span class="string">&quot;cleanup already called&quot;</span>);</span><br><span class="line">            ret = ExitCode.TERMINATED.getExitCode();</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="comment">// 重点：调用 ContainerExecutor 对象启动 Contianer</span></span><br><span class="line">            exec.activateContainer(containerID, pidFilePath);</span><br><span class="line">            ret = exec.launchContainer(container, nmPrivateContainerScriptPath,</span><br><span class="line">                    nmPrivateTokensPath, user, appIdStr, containerWorkDir,</span><br><span class="line">                    localDirs, logDirs);</span><br><span class="line">          &#125;</span><br><span class="line">    &#125; </span><br><span class="line"></span><br><span class="line">    <span class="comment">// Container 执行结果返回，判断是否成功执行（省略）</span></span><br><span class="line">    </span><br><span class="line">    LOG.info(<span class="string">&quot;Container &quot;</span> + containerIdStr + <span class="string">&quot; succeeded &quot;</span>);</span><br><span class="line">    dispatcher.getEventHandler().handle(</span><br><span class="line">        <span class="keyword">new</span> ContainerEvent(containerID,</span><br><span class="line">            ContainerEventType.CONTAINER_EXITED_WITH_SUCCESS));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p> Container 的运行环境已经准备好，接下来就是真正在 NM 上真正启动 Container 的过程，具体启动是调用 ContainerExecutor#launchContainer() 方法。运行 Container 是由插拔式组件 ContainerExecutor 完成，YARN 中提供了三种 ContainerExecutor 实现，一种是 DefaultContainerExecutor，一种是 LinuxContainerExecutor，另一种是 DockerContainerExecutor，由参数 yarn.nodemanager.container-executor.class 控制其具体使用方式。</p>
<h2 id="2-4-Container资源清理"><a href="#2-4-Container资源清理" class="headerlink" title="2.4 Container资源清理"></a>2.4 Container资源清理</h2><p>Container 资源清理是指 Container 运行完成后（可能成功或者失败），NM 需回收它占用的资源，这些资源主要是 Container 运行时使用的临时文件，主要来源是 ResourceLocalizationService 和 ContianerExecutor 两个服务/组件，其中 ResourceLocalizationService 将数据 HDFS 文件下载到本地，ContainerExecutor 为 Container 创建私有工作目录，并保存一些临时文件（比如 Container 进程 pid 文件）。因此，Container 资源清理过程主要是通知这两个组件删除临时目录。</p>
<p>从 ContainerLaunch#call() 方法结束处，当 Container 成功运行完成后，会向调度器发送 ContainerEventType.CONTAINER_EXITED_WITH_SUCCESS 事件。该事件的注册状态转换如下，将 Container 状态 从 RUNNING 转换为 EXITED_WITH_SUCCESS，并触发状态机 ExitedWithSuccessTransition。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java</span></span><br><span class="line">    <span class="comment">// From RUNNING State</span></span><br><span class="line">    .addTransition(ContainerState.RUNNING,</span><br><span class="line">        ContainerState.EXITED_WITH_SUCCESS,</span><br><span class="line">        ContainerEventType.CONTAINER_EXITED_WITH_SUCCESS,</span><br><span class="line">        <span class="keyword">new</span> ExitedWithSuccessTransition(<span class="keyword">true</span>))</span><br></pre></td></tr></table></figure>

<p>ExitedWithSuccessTransition 状态过程会发送 ContainersLauncherEventType.CLEANUP_CONTAINER 事件，该事件发送了两个事件：</p>
<ul>
<li>向 ContainerLauncher 发送 ContainersLauncherEventType.CLEANUP_CONTAINER 清理事件；</li>
<li>向 ResourceLocalizationService 发送 LocalizationEventType.CLEANUP_CONTAINER_RESOURCES 清理事件。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java</span></span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ExitedWithSuccessTransition</span> <span class="keyword">extends</span> <span class="title">ContainerTransition</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> clCleanupRequired;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ExitedWithSuccessTransition</span><span class="params">(<span class="keyword">boolean</span> clCleanupRequired)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.clCleanupRequired = clCleanupRequired;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transition</span><span class="params">(ContainerImpl container, ContainerEvent event)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// Set exit code to 0 on success        </span></span><br><span class="line">      container.exitCode = <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">      <span class="comment">// <span class="doctag">TODO:</span> Add containerWorkDir to the deletion service.</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (clCleanupRequired) &#123;</span><br><span class="line">          <span class="comment">// 向 ContainerLauncher 发送 ContainersLauncherEventType.CLEANUP_CONTAINER 清理事件</span></span><br><span class="line">        container.dispatcher.getEventHandler().handle(</span><br><span class="line">            <span class="keyword">new</span> ContainersLauncherEvent(container,</span><br><span class="line">                ContainersLauncherEventType.CLEANUP_CONTAINER));</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 向 ResourceLocalizationService 发送 LocalizationEventType.CLEANUP_CONTAINER_RESOURCES 清理事件</span></span><br><span class="line">      container.cleanup();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>先来看看 ContainerLauncher 清理临时目录的过程。ContainersLauncherEventType.CLEANUP_CONTAINER 事件的处理逻辑最终会进入到 ContainersLauncher 的 handle() 方法，将 Container 从正在运行的 Container 列表中移除，并调用 ContainerLaunch#cleanupContainer() 方法清除 Container 占用的临时目录。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainersLauncher.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(ContainersLauncherEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> ContainersLauncher launches containers one by one!!</span></span><br><span class="line">    Container container = event.getContainer();</span><br><span class="line">    ContainerId containerId = container.getContainerId();</span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">      <span class="keyword">case</span> LAUNCH_CONTAINER:  <span class="comment">// 省略</span></span><br><span class="line">      <span class="keyword">case</span> RECOVER_CONTAINER: <span class="comment">// 省略</span></span><br><span class="line">      <span class="keyword">case</span> CLEANUP_CONTAINER:</span><br><span class="line">        <span class="comment">// 将 Container 从正在运行 Container 列表中移除</span></span><br><span class="line">        ContainerLaunch launcher = running.remove(containerId);</span><br><span class="line">        <span class="keyword">if</span> (launcher == <span class="keyword">null</span>) &#123;</span><br><span class="line">          <span class="comment">// Container not launched. So nothing needs to be done.</span></span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Cleanup a container whether it is running/killed/completed, so that</span></span><br><span class="line">        <span class="comment">// no sub-processes are alive.</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 清理 Container 占用的临时目录</span></span><br><span class="line">          launcher.cleanupContainer();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">          LOG.warn(<span class="string">&quot;Got exception while cleaning container &quot;</span> + containerId</span><br><span class="line">              + <span class="string">&quot;. Ignoring.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<p>再来看看 ResourceLocalizationService 清除 Container 用户工作目录和 NM 私有目录下的 Container 目录。根据发送的发送 LocalizationEventType.CLEANUP_CONTAINER_RESOURCES 清理事件，可以进入到对应的清理逻辑 handleCleanupContainerResources()，执行具体的清理逻辑。该逻辑将会删除用户工作 <code>$&#123;yarn.nodemanager.local-dirs&#125;/usercache/&lt;user&gt;/appcache/$&#123;appid&#125;/$&#123;containerid&#125;</code> 的数据（即从 HDFS 下载的数据）和 <code>$&#123;yarn.nodemanager.local-dirs&#125;/nmPrivate/$&#123;appid&#125;/$&#123;containerid&#125;</code> 私有目录数据，这两个目标都存放了 Tokens 文件和 Shell 运行脚本。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(LocalizationEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">    <span class="keyword">case</span> INIT_APPLICATION_RESOURCES:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> INIT_CONTAINER_RESOURCES:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CONTAINER_RESOURCES_LOCALIZED:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CACHE_CLEANUP:  <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">case</span> CLEANUP_CONTAINER_RESOURCES:</span><br><span class="line">      handleCleanupContainerResources((ContainerLocalizationCleanupEvent)event);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> DESTROY_APPLICATION_RESOURCES: <span class="comment">//省略</span></span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(<span class="string">&quot;Unknown localization event: &quot;</span> + event);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，Container 资源清理流程已完成。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li> 董西成. 《Hadoop技术内幕 · 深入解析 YARN 架构设计与实现原理》</li>
<li><a href="https://blog.csdn.net/gaopenghigh/article/details/45507765">YARN Container 启动流程分析</a></li>
</ul>
]]></content>
      <categories>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>YARN</tag>
        <tag>YARN源码</tag>
      </tags>
  </entry>
  <entry>
    <title>YARN Dispatcher设计思想及源码引读</title>
    <url>/2021/11/28/YARN-Dispatcher%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E5%8F%8A%E6%BA%90%E7%A0%81%E5%BC%95%E8%AF%BB/</url>
    <content><![CDATA[<blockquote>
<p>源码版本：Cloudera Hadoop 2.6.0-cdh5.14.4</p>
<p>导读：Dispatcher 是 YARN 源码设计中使用场景比较多的一个接口，各种事件的状态流转都是通过 Dispatcher 转发的，理解了 YARN Dispatcher 的设计思想，才能更清晰地阅读 YARN 的源码。</p>
</blockquote>
<h1 id="1-YARN事件处理模型"><a href="#1-YARN事件处理模型" class="headerlink" title="1. YARN事件处理模型"></a>1. YARN事件处理模型</h1><p>YARN 整个框架代码都建立在状态机和事件驱动的模型之上，框架整体的流转过程如下图，主要包括以下几个步骤：</p>
<ol>
<li><p>Event 事件根据 EventType（事件类型）到达指定的 AsyncDispatcher（中央异步调度器）内部的阻塞队列 EventQueue；</p>
</li>
<li><p>AsyncDispatcher 阻塞从 EventQueue 中取出事件，交给具体的 EventHandler（事件处理器）处理；</p>
</li>
<li><p>EventHandler 可能会直接处理 Event 事件，也可能将 Event 事件转发给一个带有限状态机的 EventHandler 处理，处理结果又将 EventType 反馈给 AsyncDispatcher；</p>
</li>
<li><p>新抵达的 Event 事件会再次被 AsycnDispatcher 消费，转发给下一个 EventHandler，直到 Event 事件处理完成。</p>
</li>
</ol>
<blockquote>
<p>这里留几个小疑问供大家思考（可以带着问题去阅读文章后部分内容）：</p>
<p>Q: YARN 的 EventType 可能有哪些，可结合有哪些服务会与 RM（ResourceManager）交互一起思考。</p>
<p>Q: YARN 的中央异步调度器 AsyncDispatcher 在哪些服务中会用到？</p>
<p>Q: EventType 抵达 AsyncDiapatcher 后，AsyncDiapatcher 如何直到要转交给哪个 EventHandler 处理？</p>
</blockquote>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/28/b414ee4ad22f56302d0c9bcbdbda02d2-1637891150721-0b3d4ea8-815d-4e19-b425-ce7a13a6eacb-ce89e7.png" alt="img"></h1><center>YARN事件驱动和状态机模型流程图</center>

<p>事件处理模型是 YARN 最重要的设计思想之一，一旦理解了这种思想，阅读 YARN 的源码几乎没太多问题，同样，这种思想并不是 YARN 独有的，后面介绍 Spark 组件时会发现也是相同的设计思想。</p>
<h1 id="2-YARN-Dispatcher介绍"><a href="#2-YARN-Dispatcher介绍" class="headerlink" title="2. YARN Dispatcher介绍"></a>2. YARN Dispatcher介绍</h1><p>Dispatcher 是 YARN 源码中使用场景比较多的一个类，整体的设计思路是一个<code>生产者和消费者模型</code>，不过支持的多生产者和多消费者的模式，不同的生产者和消费者之间的映射是通过 <code>protected final Map&lt;Class&lt;? extends Enum&gt;, EventHandler&gt; eventDispatchers;</code> 实现，数据的传递是通过无界阻塞队列 <code>private final BlockingQueue&lt;Event&gt; eventQueue;</code> 实现，消费者是通过 <code>private Thread eventHandlingThread;</code> 实现，而真正的 EventHandler 处理逻辑则是 Thread 根据注册的事件类型，切换到不同的 EventHandler 同步处理。</p>
<h2 id="2-1-Dispatcher总览"><a href="#2-1-Dispatcher总览" class="headerlink" title="2.1 Dispatcher总览"></a>2.1 Dispatcher总览</h2><p>在 YARN 中，AsyncDispatcher 是 Dispatcher 接口的子类， 所有核心服务的异步请求处理逻辑都是通过 AsyncDispatcher 转发的， 包括 ResourceManager、NodeManager、 ContainerManagerImpl 等， 它们维护了事先注册的事件与事件处理器， 并根据接收的事件类型驱动服务的运行。具体实现上有 MultiThreadedDispatcher、AsyncDispatcher 两种：</p>
<ul>
<li>AsyncDispatcher：单线程的事件生产和消费消息</li>
<li>MultiThreadedDispatcher：持有多个AsyncDispatcher，也就是一个 AsyncDispatcher 集合，添加事件时候通过轮训的方式添加到对应的AsyncDispatcher 中，每个 AsyncDispatcher 维护自己的异步消费线程。</li>
</ul>
<p>YARN 源码的实际应用场景有（主要介绍每个 服务的 Dispatcher 如何定义，已经注册了哪些 <code>EventType -&gt; EventHandler</code> 的映射：</p>
<h3 id="2-1-1-ResourceManager实现"><a href="#2-1-1-ResourceManager实现" class="headerlink" title="2.1.1 ResourceManager实现"></a>2.1.1 ResourceManager实现</h3><ul>
<li><strong>ResourceManager — AsyncDispatcher</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//  AsyncDispatcher 定义</span></span><br><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// register the handlers for all AlwaysOn services using setupDispatcher().</span></span><br><span class="line">    rmDispatcher = setupDispatcher();</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">super</span>.serviceInit(<span class="keyword">this</span>.conf);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Register the handlers for alwaysOn services</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> Dispatcher <span class="title">setupDispatcher</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Dispatcher dispatcher = createDispatcher();</span><br><span class="line">    dispatcher.register(RMFatalEventType.class,</span><br><span class="line">        <span class="keyword">new</span> ResourceManager.RMFatalEventDispatcher());</span><br><span class="line">    <span class="keyword">return</span> dispatcher;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 真正构造 AsyncDispatcher</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> Dispatcher <span class="title">createDispatcher</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> AsyncDispatcher();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 注册的事件类型 EventType –&gt; 事件处理器 EventHandler（dispatcher.register() 方法注册）</span></span><br><span class="line">RMAppEventType –&gt; ApplicationEventDispatcher</span><br><span class="line">RMAppAttemptEventType –&gt; ApplicationAttemptEventDispatcher</span><br><span class="line">NodesListManagerEventType –&gt; NodesListManager</span><br><span class="line">SchedulerEventType –&gt; SchedulerEventDispatcher</span><br><span class="line">RMNodeEventType –&gt; NodeEventDispatcher</span><br><span class="line">RMAppManagerEventType –&gt; RMAppManager</span><br><span class="line">AMLauncherEventType –&gt; ApplicationMasterLauncher</span><br><span class="line">RMFatalEventType –&gt; ResourceManager.RMFatalEventDispatcher</span><br></pre></td></tr></table></figure>

<h3 id="2-1-2-NodeManager实现"><a href="#2-1-2-NodeManager实现" class="headerlink" title="2.1.2 NodeManager实现"></a>2.1.2 NodeManager实现</h3><ul>
<li><strong>NodeManager — AsyncDispatcher</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AsyncDispatcher 定义</span></span><br><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/nodemanager/NodeManager.java</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> AsyncDispatcher dispatcher;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// 省略</span></span><br><span class="line">      </span><br><span class="line">    <span class="comment">// NodeManager level dispatcher</span></span><br><span class="line">    <span class="keyword">this</span>.dispatcher = <span class="keyword">new</span> AsyncDispatcher();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">super</span>.serviceInit(conf);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 注册的事件类型 EventType –&gt; 事件处理器 EventHandler（dispatcher.register() 方法注册）</span></span><br><span class="line">ContainerManagerEventType –&gt; ContainerManagerImpl</span><br><span class="line">NodeManagerEventType –&gt; NodeManager</span><br></pre></td></tr></table></figure>

<h3 id="2-1-3-ContainerManagerImpl实现"><a href="#2-1-3-ContainerManagerImpl实现" class="headerlink" title="2.1.3 ContainerManagerImpl实现"></a>2.1.3 ContainerManagerImpl实现</h3><ul>
<li><strong>ContainerManagerImpl — AsyncDispatcher</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AsyncDispatcher 定义</span></span><br><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> AsyncDispatcher dispatcher;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ContainerManagerImpl</span><span class="params">(Context context, ContainerExecutor exec,</span></span></span><br><span class="line"><span class="params"><span class="function">      DeletionService deletionContext, NodeStatusUpdater nodeStatusUpdater,</span></span></span><br><span class="line"><span class="params"><span class="function">      NodeManagerMetrics metrics, ApplicationACLsManager aclsManager,</span></span></span><br><span class="line"><span class="params"><span class="function">      LocalDirsHandlerService dirsHandler)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(ContainerManagerImpl.class.getName());</span><br><span class="line">    <span class="keyword">this</span>.context = context;</span><br><span class="line">    <span class="keyword">this</span>.dirsHandler = dirsHandler;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ContainerManager level dispatcher.</span></span><br><span class="line">    dispatcher = <span class="keyword">new</span> AsyncDispatcher();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 注册的事件类型 EventType –&gt; 事件处理器 EventHandler（dispatcher.register() 方法注册）</span></span><br><span class="line">ContainerEventType –&gt; ContainerEventDispatcher</span><br><span class="line">ApplicationEventType –&gt; ApplicationEventDispatcher</span><br><span class="line">LocalizationEventType –&gt; ResourceLocalizationService</span><br><span class="line">ContainersMonitorEventType –&gt; ContainersMonitor</span><br><span class="line">ContainersLauncherEventType –&gt; ContainersLauncher</span><br><span class="line">LogHandlerEventType –&gt; LogHandler</span><br><span class="line">SharedCacheUploadEventType –&gt; SharedCacheUploadService</span><br></pre></td></tr></table></figure>

<h3 id="2-1-4-ResourceLocalizationService实现"><a href="#2-1-4-ResourceLocalizationService实现" class="headerlink" title="2.1.4 ResourceLocalizationService实现"></a>2.1.4 ResourceLocalizationService实现</h3><ul>
<li><strong>ResourceLocalizationService 复用的是 ContainerManagerImpl 的 AsyncDispatcher</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AsyncDispatcher 定义</span></span><br><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> Dispatcher dispatcher;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// dispatcher 初始化是通过 ResourceLocalizationService 构造器构造的</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ResourceLocalizationService</span><span class="params">(Dispatcher dispatcher,</span></span></span><br><span class="line"><span class="params"><span class="function">      ContainerExecutor exec, DeletionService delService,</span></span></span><br><span class="line"><span class="params"><span class="function">      LocalDirsHandlerService dirsHandler, Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">super</span>(ResourceLocalizationService.class.getName());</span><br><span class="line">    <span class="keyword">this</span>.exec = exec;</span><br><span class="line">    <span class="keyword">this</span>.dispatcher = dispatcher;</span><br><span class="line">    <span class="keyword">this</span>.delService = delService;</span><br><span class="line">    <span class="keyword">this</span>.dirsHandler = dirsHandler;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.cacheCleanup = <span class="keyword">new</span> ScheduledThreadPoolExecutor(<span class="number">1</span>,</span><br><span class="line">        <span class="keyword">new</span> ThreadFactoryBuilder()</span><br><span class="line">          .setNameFormat(<span class="string">&quot;ResourceLocalizationService Cache Cleanup&quot;</span>)</span><br><span class="line">          .build());</span><br><span class="line">    <span class="keyword">this</span>.stateStore = context.getNMStateStore();</span><br><span class="line">    <span class="keyword">this</span>.nmContext = context;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ResourceLocalizationService 构造器 Dispatcher 对象的调用来源</span></span><br><span class="line">  <span class="comment">// 所以说 ResourceLocalizationService 的 Dispatcher 是复用 ContainerManagerImpl 的 AsyncDispatcher</span></span><br><span class="line">  <span class="comment">// 位置：org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> ResourceLocalizationService <span class="title">createResourceLocalizationService</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      ContainerExecutor exec, DeletionService deletionContext, Context context)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ResourceLocalizationService(<span class="keyword">this</span>.dispatcher, exec,</span><br><span class="line">        deletionContext, dirsHandler, context);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 注册的事件类型 EventType –&gt; 事件处理器 EventHandler</span></span><br><span class="line">LocalizerEventType –&gt; LocalizerTracker</span><br></pre></td></tr></table></figure>

<h3 id="2-1-5-CommonNodeLabelsManager实现"><a href="#2-1-5-CommonNodeLabelsManager实现" class="headerlink" title="2.1.5 CommonNodeLabelsManager实现"></a>2.1.5 CommonNodeLabelsManager实现</h3><ul>
<li><strong>CommonNodeLabelsManager — AsyncDispatcher</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AsyncDispatcher 定义</span></span><br><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> Dispatcher dispatcher;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// for UT purpose（这里 dispatcher 做了一层转换，暂时还不明白设计初衷）</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initDispatcher</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// create async handler</span></span><br><span class="line">    dispatcher = <span class="keyword">new</span> AsyncDispatcher();</span><br><span class="line">    AsyncDispatcher asyncDispatcher = (AsyncDispatcher) dispatcher;</span><br><span class="line">    asyncDispatcher.init(conf);</span><br><span class="line">    asyncDispatcher.setDrainEventsOnStop();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 注册的事件类型 EventType –&gt; 事件处理器 EventHandler（dispatcher.register() 方法注册）</span></span><br><span class="line">NodeLabelsStoreEventType –&gt; ForwardingEventHandler</span><br></pre></td></tr></table></figure>

<h3 id="2-1-6-RMApplicationHistoryWriter实现"><a href="#2-1-6-RMApplicationHistoryWriter实现" class="headerlink" title="2.1.6 RMApplicationHistoryWriter实现"></a>2.1.6 RMApplicationHistoryWriter实现</h3><ul>
<li><strong>RMApplicationHistoryWriter — MultiThreadedDispatcher</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AsyncDispatcher 定义</span></span><br><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ahs/RMApplicationHistoryWriter.java</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> Dispatcher dispatcher;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">      </span><br><span class="line">    dispatcher = createDispatcher(conf);</span><br><span class="line">     </span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">super</span>.serviceInit(conf);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> Dispatcher <span class="title">createDispatcher</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">    MultiThreadedDispatcher dispatcher =</span><br><span class="line">        <span class="keyword">new</span> MultiThreadedDispatcher(</span><br><span class="line">          conf</span><br><span class="line">            .getInt(</span><br><span class="line">              YarnConfiguration.RM_HISTORY_WRITER_MULTI_THREADED_DISPATCHER_POOL_SIZE,</span><br><span class="line">              YarnConfiguration.DEFAULT_RM_HISTORY_WRITER_MULTI_THREADED_DISPATCHER_POOL_SIZE));</span><br><span class="line">    dispatcher.setDrainEventsOnStop();</span><br><span class="line">    <span class="keyword">return</span> dispatcher;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 和 AsyncDispatcher 并行存在的一种 Dispatcher，单独实现 Dispatcher 接口</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MultiThreadedDispatcher</span> <span class="keyword">extends</span> <span class="title">CompositeService</span></span></span><br><span class="line"><span class="class">      <span class="keyword">implements</span> <span class="title">Dispatcher</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;AsyncDispatcher&gt; dispatchers =</span><br><span class="line">        <span class="keyword">new</span> ArrayList&lt;AsyncDispatcher&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MultiThreadedDispatcher</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">super</span>(MultiThreadedDispatcher.class.getName());</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num; ++i) &#123;</span><br><span class="line">        AsyncDispatcher dispatcher = createDispatcher();</span><br><span class="line">        dispatchers.add(dispatcher);</span><br><span class="line">        addIfService(dispatcher);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 注册的事件类型 EventType –&gt; 事件处理器 EventHandler（dispatcher.register() 方法注册）</span></span><br><span class="line">WritingHistoryEventType –&gt; ForwardingEventHandler</span><br><span class="line">dispatcher个数 ：yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size=10</span><br></pre></td></tr></table></figure>

<h3 id="2-1-7-SystemMetricsPublisher实现"><a href="#2-1-7-SystemMetricsPublisher实现" class="headerlink" title="2.1.7 SystemMetricsPublisher实现"></a>2.1.7 SystemMetricsPublisher实现</h3><ul>
<li><strong>SystemMetricsPublisher — MultiThreadedDispatcher</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AsyncDispatcher 定义</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> Dispatcher dispatcher;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 和 RMApplicationHistoryWriter 实现逻辑一致，这里就不展开介绍</span></span><br><span class="line">    dispatcher = createDispatcher(conf);</span><br><span class="line">     </span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">super</span>.serviceInit(conf);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 注册的事件类型 EventType –&gt; 事件处理器 EventHandler（dispatcher.register() 方法注册）</span></span><br><span class="line">SystemMetricsEventType –&gt; ForwardingEventHandler</span><br><span class="line">dispatcher个数 ：yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10</span><br></pre></td></tr></table></figure>

<h2 id="2-2-事件类型EventType"><a href="#2-2-事件类型EventType" class="headerlink" title="2.2 事件类型EventType"></a>2.2 事件类型EventType</h2><p>在 <code>Dispatcher总览</code> 小节中介绍了 YARN 各个服务 <code>注册的事件类型 EventType –&gt; 事件处理器 EventHandler</code> 的映射关系，这里以 RM 的 RMAppEvent、RMNodeEvent、SchedulerEvent 为例，看看 YARN 的注册的这些事件类型长什么样。可以发现，各种 EventType 事件类型就是一个枚举类型。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// RM 事件类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// APP 执行过程与 RM 交互的事件类型</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">RMAppEventType</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Source: ClientRMService</span></span><br><span class="line">  START,</span><br><span class="line">  RECOVER,</span><br><span class="line">  KILL,</span><br><span class="line">  MOVE, <span class="comment">// Move app to a new queue</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: Scheduler and RMAppManager</span></span><br><span class="line">  APP_REJECTED,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: Scheduler</span></span><br><span class="line">  APP_ACCEPTED,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: RMAppAttempt</span></span><br><span class="line">  ATTEMPT_REGISTERED,</span><br><span class="line">  ATTEMPT_UNREGISTERED,</span><br><span class="line">  ATTEMPT_FINISHED, <span class="comment">// Will send the final state</span></span><br><span class="line">  ATTEMPT_FAILED,</span><br><span class="line">  ATTEMPT_KILLED,</span><br><span class="line">  NODE_UPDATE,</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Source: Container and ResourceTracker</span></span><br><span class="line">  APP_RUNNING_ON_NODE,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: RMStateStore</span></span><br><span class="line">  APP_NEW_SAVED,</span><br><span class="line">  APP_UPDATE_SAVED,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// NodeManager 与 RM 交互的事件类型</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">RMNodeEventType</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  STARTED,</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Source: AdminService</span></span><br><span class="line">  DECOMMISSION,</span><br><span class="line">  GRACEFUL_DECOMMISSION,</span><br><span class="line">  RECOMMISSION,</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Source: AdminService, ResourceTrackerService</span></span><br><span class="line">  RESOURCE_UPDATE,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ResourceTrackerService</span></span><br><span class="line">  STATUS_UPDATE,</span><br><span class="line">  REBOOTING,</span><br><span class="line">  RECONNECTED,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: Application</span></span><br><span class="line">  CLEANUP_APP,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: Container</span></span><br><span class="line">  CONTAINER_ALLOCATED,</span><br><span class="line">  CLEANUP_CONTAINER,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: RMAppAttempt</span></span><br><span class="line">  FINISHED_CONTAINERS_PULLED_BY_AM,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: NMLivelinessMonitor</span></span><br><span class="line">  EXPIRE</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// RM 调度逻辑的事件类型</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">SchedulerEventType</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: Node</span></span><br><span class="line">  NODE_ADDED,</span><br><span class="line">  NODE_REMOVED,</span><br><span class="line">  NODE_UPDATE,</span><br><span class="line">  NODE_RESOURCE_UPDATE,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: RMApp</span></span><br><span class="line">  APP_ADDED,</span><br><span class="line">  APP_REMOVED,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: RMAppAttempt</span></span><br><span class="line">  APP_ATTEMPT_ADDED,</span><br><span class="line">  APP_ATTEMPT_REMOVED,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: ContainerAllocationExpirer</span></span><br><span class="line">  CONTAINER_EXPIRED,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: RMContainer</span></span><br><span class="line">  CONTAINER_RESCHEDULED,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Source: SchedulingEditPolicy</span></span><br><span class="line">  DROP_RESERVATION,</span><br><span class="line">  PREEMPT_CONTAINER,</span><br><span class="line">  KILL_CONTAINER</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-3-事件处理器EventHandler"><a href="#2-3-事件处理器EventHandler" class="headerlink" title="2.3 事件处理器EventHandler"></a>2.3 事件处理器EventHandler</h2><p>所有的 handler 都是实现 EventHandler 接口或者是 MultiListenerHandler ，MultiListenerHandler 的实现是为了满足一个事件有不同的事件通知，其中 EventHandler 是一对一的消息模型，MultiListenerHandler 是一对多的消息模型。还是以 RM 的 RMAppEventType、RMNodeEventType、SchedulerEventType 事件类型为例，看看对应的事件处理器 ApplicationEventDispatcher、NodeEventDispatcher、SchedulerEventDispatcher 是如何定义的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// RMAppEventType 事件类型对应的 EventHandler</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ApplicationAttemptEventDispatcher</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">    <span class="title">EventHandler</span>&lt;<span class="title">RMAppAttemptEvent</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// RMNodeEventType 事件类型对应的 EventHandler</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">NodeEventDispatcher</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">    <span class="title">EventHandler</span>&lt;<span class="title">RMNodeEvent</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// SchedulerEventType 事件类型对应的 EventHandler</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SchedulerEventDispatcher</span> <span class="keyword">extends</span> <span class="title">AbstractService</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">EventHandler</span>&lt;<span class="title">SchedulerEvent</span>&gt; </span>&#123;</span><br></pre></td></tr></table></figure>

<h1 id="3-AsyncDispatcher设计原理"><a href="#3-AsyncDispatcher设计原理" class="headerlink" title="3. AsyncDispatcher设计原理"></a>3. AsyncDispatcher设计原理</h1><p>Dispatcher接口定义了基本的事件派发行为：</p>
<ul>
<li>register() 方法负责注册事件类型 EventType 对应的事件处理器 EventHandler。</li>
<li>getEventHandler() 方法获得对应的 EventHandler 以派发事件。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/event/Dispatcher.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Dispatcher</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Configuration to make sure dispatcher crashes but doesn&#x27;t do system-exit in</span></span><br><span class="line">  <span class="comment">// case of errors. By default, it should be false, so that tests are not</span></span><br><span class="line">  <span class="comment">// affected. For all daemons it should be explicitly set to true so that</span></span><br><span class="line">  <span class="comment">// daemons can crash instead of hanging around.</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String DISPATCHER_EXIT_ON_ERROR_KEY =</span><br><span class="line">      <span class="string">&quot;yarn.dispatcher.exit-on-error&quot;</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> DEFAULT_DISPATCHER_EXIT_ON_ERROR = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function">EventHandler <span class="title">getEventHandler</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">register</span><span class="params">(Class&lt;? extends Enum&gt; eventType, EventHandler handler)</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>AsyncDispatcher 是 YARN 中的核心调度器，实现了 Dispatcher 接口，通过阻塞队列扩展出异步派发事件的行为，也称为“中央异步调度器”，典型的生产者-消费者设计思想。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/event/AsyncDispatcher.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncDispatcher</span> <span class="keyword">extends</span> <span class="title">AbstractService</span> <span class="keyword">implements</span> <span class="title">Dispatcher</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-1-事件注册逻辑"><a href="#3-1-事件注册逻辑" class="headerlink" title="3.1 事件注册逻辑"></a>3.1 事件注册逻辑</h2><p>以下是 RM（ResourceManager）中注册各种 <code>EventType -&gt; EventHandler</code> 的位置。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="comment">// Register event handler for NodesListManager</span></span><br><span class="line">      nodesListManager = <span class="keyword">new</span> NodesListManager(rmContext);</span><br><span class="line">      rmDispatcher.register(NodesListManagerEventType.class, nodesListManager);</span><br><span class="line">      addService(nodesListManager);</span><br><span class="line">      rmContext.setNodesListManager(nodesListManager);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Initialize the scheduler</span></span><br><span class="line">      scheduler = createScheduler();</span><br><span class="line">      scheduler.setRMContext(rmContext);</span><br><span class="line">      addIfService(scheduler);</span><br><span class="line">      rmContext.setScheduler(scheduler);</span><br><span class="line"></span><br><span class="line">      schedulerDispatcher = createSchedulerEventDispatcher();</span><br><span class="line">      addIfService(schedulerDispatcher);</span><br><span class="line">      rmDispatcher.register(SchedulerEventType.class, schedulerDispatcher);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Register event handler for RmAppEvents</span></span><br><span class="line">      rmDispatcher.register(RMAppEventType.class,</span><br><span class="line">          <span class="keyword">new</span> ApplicationEventDispatcher(rmContext));</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Register event handler for RmAppAttemptEvents</span></span><br><span class="line">      rmDispatcher.register(RMAppAttemptEventType.class,</span><br><span class="line">          <span class="keyword">new</span> ApplicationAttemptEventDispatcher(rmContext));</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Register event handler for RmNodes</span></span><br><span class="line">      rmDispatcher.register(</span><br><span class="line">          RMNodeEventType.class, <span class="keyword">new</span> NodeEventDispatcher(rmContext));      </span><br><span class="line">      </span><br><span class="line">      rmAppManager = createRMAppManager();</span><br><span class="line">      <span class="comment">// Register event handler for RMAppManagerEvents</span></span><br><span class="line">      rmDispatcher.register(RMAppManagerEventType.class, rmAppManager);</span><br><span class="line">        </span><br><span class="line">      applicationMasterLauncher = createAMLauncher();</span><br><span class="line">      <span class="comment">// Register event handler for AMLauncherEvents</span></span><br><span class="line">      rmDispatcher.register(AMLauncherEventType.class,</span><br><span class="line">          applicationMasterLauncher);</span><br><span class="line">       </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Register the handlers for alwaysOn services</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> Dispatcher <span class="title">setupDispatcher</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Dispatcher dispatcher = createDispatcher();</span><br><span class="line">    <span class="comment">// Register event handler for RMFatalEvents</span></span><br><span class="line">    dispatcher.register(RMFatalEventType.class,</span><br><span class="line">        <span class="keyword">new</span> ResourceManager.RMFatalEventDispatcher());</span><br><span class="line">    <span class="keyword">return</span> dispatcher;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>可以看到 EventHandler 的注册都用调用 AsyncDispatcher#register() 方法实现，通过该方法将 <code>EventType -&gt; EventHandler</code> 的映射关系添加到 AsyncDispatcher 类的 eventDispatchers Map 结果中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/event/AsyncDispatcher.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">register</span><span class="params">(Class&lt;? extends Enum&gt; eventType,</span></span></span><br><span class="line"><span class="params"><span class="function">      EventHandler handler)</span> </span>&#123;</span><br><span class="line">    <span class="comment">/* check to see if we have a listener registered */</span></span><br><span class="line">    <span class="comment">// 判断 EventType 对应的 EventHandler 是否已经注册过</span></span><br><span class="line">    EventHandler&lt;Event&gt; registeredHandler = (EventHandler&lt;Event&gt;)</span><br><span class="line">    eventDispatchers.get(eventType);</span><br><span class="line">    LOG.info(<span class="string">&quot;Registering &quot;</span> + eventType + <span class="string">&quot; for &quot;</span> + handler.getClass());</span><br><span class="line">    <span class="keyword">if</span> (registeredHandler == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// 没有注册则添加到 eventDispatchers 结构中</span></span><br><span class="line">      eventDispatchers.put(eventType, handler);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!(registeredHandler <span class="keyword">instanceof</span> MultiListenerHandler))&#123;</span><br><span class="line">      <span class="comment">// 如果 EventType 对应的 EventHandler 已经注册</span></span><br><span class="line">      <span class="comment">// 当还有一个 EventHandler 要注册到该 EventType 时，将该 EventHandler 转换为 MultiListenerHandler</span></span><br><span class="line">      <span class="comment">/* for multiple listeners of an event add the multiple listener handler */</span></span><br><span class="line">      MultiListenerHandler multiHandler = <span class="keyword">new</span> MultiListenerHandler();</span><br><span class="line">      multiHandler.addHandler(registeredHandler);</span><br><span class="line">      multiHandler.addHandler(handler);</span><br><span class="line">      eventDispatchers.put(eventType, multiHandler);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 如果 EventHandler 已经是 MultiListenerHandler，则直接添加</span></span><br><span class="line">      <span class="comment">/* already a multilistener, just add to it */</span></span><br><span class="line">      MultiListenerHandler multiHandler</span><br><span class="line">      = (MultiListenerHandler) registeredHandler;</span><br><span class="line">      multiHandler.addHandler(handler);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 存放&lt;EventType, EventHandler&gt; 的 Map 结构</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> Map&lt;Class&lt;? extends Enum&gt;, EventHandler&gt; eventDispatchers;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">AsyncDispatcher</span><span class="params">(BlockingQueue&lt;Event&gt; eventQueue)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(<span class="string">&quot;Dispatcher&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>.eventQueue = eventQueue;</span><br><span class="line">    <span class="comment">// eventDispatchers 定义</span></span><br><span class="line">    <span class="keyword">this</span>.eventDispatchers = <span class="keyword">new</span> HashMap&lt;Class&lt;? extends Enum&gt;, EventHandler&gt;();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-2-事件处理逻辑"><a href="#3-2-事件处理逻辑" class="headerlink" title="3.2 事件处理逻辑"></a>3.2 事件处理逻辑</h2><p>由于 AsyncDispatcher 是 生产者-消费者模型，事件处理的逻辑一般包括两个环节：</p>
<ol>
<li>将 Event 事件添加到阻塞队列 eventQueue 中；</li>
<li>从 eventQueue 队列中取出事件转发给 EventHandler 处理。</li>
</ol>
<p><strong>1. 添加事件到队列</strong></p>
<p>先来看看 AsyncDispatcher 是如何将事件添加到阻塞队列 eventQueue 中，我们以 <code>RMAppEventType.APP_ACCEPTED</code> 事件为例，分析如何将事件发送给 RM 的 Dispatcher，并如何转发给对应的 EventHandler 处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java  </span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">addApplication</span><span class="params">(ApplicationId applicationId,</span></span></span><br><span class="line"><span class="params"><span class="function">      String queueName, String user, <span class="keyword">boolean</span> isAppRecovering)</span> </span>&#123;</span><br><span class="line">		...      </span><br><span class="line">        <span class="comment">// rmContext.getDispatcher() 拿到 RM 的 Dispatcher 对象</span></span><br><span class="line">        <span class="comment">// getEventHandler() 拿到 RM Dispatcher 内部的 EventHandler 对象</span></span><br><span class="line">        rmContext.getDispatcher().getEventHandler()</span><br><span class="line">            .handle(<span class="keyword">new</span> RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));</span><br><span class="line">        ...</span><br><span class="line">  &#125;  </span><br><span class="line"></span><br><span class="line">  <span class="comment">// ResourceManager 的 Context 信息</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">volatile</span> RMContext rmContext;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/RMContext.java</span></span><br><span class="line">  <span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RMContext</span> </span>&#123;</span><br><span class="line">    <span class="function">Dispatcher <span class="title">getDispatcher</span><span class="params">()</span></span>;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/RMContextImpl.java</span></span><br><span class="line">  <span class="comment">// 获取 RM Dispatcher 对象</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Dispatcher <span class="title">getDispatcher</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 定义：private Dispatcher rmDispatcher;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.rmDispatcher;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/event/AsyncDispatcher.java</span></span><br><span class="line">  <span class="comment">// 获取 EventHandler 对象</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> EventHandler <span class="title">getEventHandler</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 定义：private final EventHandler handlerInstance = new GenericEventHandler();</span></span><br><span class="line">    <span class="keyword">return</span> handlerInstance;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>handle() 方法是事件的真正处理逻辑，handler() 定义在 AsyncDispatcher 类的内部类 GenericEventHandler 中，可以看到我们需要的操作，调用 <code>eventQueue.put(event)</code> 将 <code>RMAppEventType.APP_ACCEPTED</code> 事件添加到阻塞队列中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/event/AsyncDispatcher.java</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">GenericEventHandler</span> <span class="keyword">implements</span> <span class="title">EventHandler</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (blockNewEvents) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      drained = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">      <span class="comment">/* all this method does is enqueue all the events onto the queue */</span></span><br><span class="line">      <span class="keyword">int</span> qSize = eventQueue.size();</span><br><span class="line">      <span class="keyword">if</span> (qSize != <span class="number">0</span> &amp;&amp; qSize % <span class="number">1000</span> == <span class="number">0</span></span><br><span class="line">          &amp;&amp; lastEventQueueSizeLogged != qSize) &#123;</span><br><span class="line">        lastEventQueueSizeLogged = qSize;</span><br><span class="line">        LOG.info(<span class="string">&quot;Size of event-queue is &quot;</span> + qSize);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">int</span> remCapacity = eventQueue.remainingCapacity();</span><br><span class="line">      <span class="keyword">if</span> (remCapacity &lt; <span class="number">1000</span>) &#123;</span><br><span class="line">        LOG.warn(<span class="string">&quot;Very low remaining capacity in the event-queue: &quot;</span></span><br><span class="line">            + remCapacity);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 关键：将事件添加到 eventQueue</span></span><br><span class="line">        eventQueue.put(event);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!stopped) &#123;</span><br><span class="line">          LOG.warn(<span class="string">&quot;AsyncDispatcher thread interrupted&quot;</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Need to reset drained flag to true if event queue is empty,</span></span><br><span class="line">        <span class="comment">// otherwise dispatcher will hang on stop.</span></span><br><span class="line">        drained = eventQueue.isEmpty();</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><strong>2.从队列中取事件</strong></p>
<p>AsyncDispatcher 通过 <code>eventQueue.put(event)</code> 将 Event 添加到阻塞队列中后该如何取出事件处理呢，AsyncDispatcher 会启动一个独立线程阻塞地从 eventQueue 中取出事件，并转发给对应的 EventHandler 处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/event/AsyncDispatcher.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceStart</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">//start all the components</span></span><br><span class="line">    <span class="keyword">super</span>.serviceStart();</span><br><span class="line">    <span class="comment">// 启动一个 eventHandlingThread 线程</span></span><br><span class="line">    eventHandlingThread = <span class="keyword">new</span> Thread(createThread());</span><br><span class="line">    eventHandlingThread.setName(<span class="string">&quot;AsyncDispatcher event handler&quot;</span>);</span><br><span class="line">    eventHandlingThread.start();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">Runnable <span class="title">createThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (!stopped &amp;&amp; !Thread.currentThread().isInterrupted()) &#123;</span><br><span class="line">          drained = eventQueue.isEmpty();</span><br><span class="line">          <span class="comment">// blockNewEvents is only set when dispatcher is draining to stop,</span></span><br><span class="line">          <span class="comment">// adding this check is to avoid the overhead of acquiring the lock</span></span><br><span class="line">          <span class="comment">// and calling notify every time in the normal run of the loop.</span></span><br><span class="line">          <span class="keyword">if</span> (blockNewEvents) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (waitForDrained) &#123;</span><br><span class="line">              <span class="keyword">if</span> (drained) &#123;</span><br><span class="line">                waitForDrained.notify();</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          Event event;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 关键：从 eventQueue 中取出事件</span></span><br><span class="line">            event = eventQueue.take();</span><br><span class="line">          &#125; <span class="keyword">catch</span>(InterruptedException ie) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!stopped) &#123;</span><br><span class="line">              LOG.warn(<span class="string">&quot;AsyncDispatcher thread interrupted&quot;</span>, ie);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (event != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 关键：取出事件后转发给对应的 EventHandler</span></span><br><span class="line">            dispatch(event);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// dispatcher 转发事件给对应的 EventHandler</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">dispatch</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//all events go thru this loop</span></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Dispatching the event &quot;</span> + event.getClass().getName() + <span class="string">&quot;.&quot;</span></span><br><span class="line">          + event.toString());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Class&lt;? extends Enum&gt; type = event.getType().getDeclaringClass();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>&#123;</span><br><span class="line">      <span class="comment">// 关键：EventType -&gt; EventHandler 映射关系之前注册到 eventDispatchers 的 Map 结构中</span></span><br><span class="line">      <span class="comment">// 根据对应的 EventType 拿到对应 EventHandle，并调用 handle() 执行真正的逻辑</span></span><br><span class="line">      EventHandler handler = eventDispatchers.get(type);</span><br><span class="line">      <span class="keyword">if</span>(handler != <span class="keyword">null</span>) &#123;</span><br><span class="line">        handler.handle(event);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">&quot;No handler for registered for &quot;</span> + type);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">      <span class="comment">//TODO Maybe log the state of the queue</span></span><br><span class="line">      LOG.fatal(<span class="string">&quot;Error in dispatcher thread&quot;</span>, t);</span><br><span class="line">      <span class="comment">// If serviceStop is called, we should exit this thread gracefully.</span></span><br><span class="line">      <span class="keyword">if</span> (exitOnDispatchException</span><br><span class="line">          &amp;&amp; (ShutdownHookManager.get().isShutdownInProgress()) == <span class="keyword">false</span></span><br><span class="line">          &amp;&amp; stopped == <span class="keyword">false</span>) &#123;</span><br><span class="line">        Thread shutDownThread = <span class="keyword">new</span> Thread(createShutDownThread());</span><br><span class="line">        shutDownThread.setName(<span class="string">&quot;AsyncDispatcher ShutDown handler&quot;</span>);</span><br><span class="line">        shutDownThread.start();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>再来看看注册逻辑中发送的 <code>new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED)</code> 事件，这个事件属于 RMAppEventType 事件类型，那这个 EventType 对应的 EventHandler 是什么呢？以及对应的 handle() 是处理什么逻辑呢？通过前面事件注册逻辑中提到，RMAppEventType 事件类型是注册到 ApplicationEventDispatcher EventHandler。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">      <span class="comment">// Register event handler for RmAppEvents</span></span><br><span class="line">  rmDispatcher.register(RMAppEventType.class,</span><br><span class="line">      <span class="keyword">new</span> ApplicationEventDispatcher(rmContext));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ApplicationEventDispatcher EventHandler</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ApplicationEventDispatcher</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">EventHandler</span>&lt;<span class="title">RMAppEvent</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>至此，我们就直到了这个事件注册的 EventHandler，内部的 handle() 方法逻辑逻辑还嵌套了一层 handle() 方法调用，所以真正的处理逻辑是由 RMApp 接口的实现类 RMAppImpl 处理，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ApplicationEventDispatcher</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">EventHandler</span>&lt;<span class="title">RMAppEvent</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> RMContext rmContext;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ApplicationEventDispatcher</span><span class="params">(RMContext rmContext)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.rmContext = rmContext;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(RMAppEvent event)</span> </span>&#123;</span><br><span class="line">      ApplicationId appID = event.getApplicationId();</span><br><span class="line">      RMApp rmApp = <span class="keyword">this</span>.rmContext.getRMApps().get(appID);</span><br><span class="line">      <span class="keyword">if</span> (rmApp != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 关键：RMAppEventType.APP_ACCEPTED 事件对应的 EventHandler handle() 处理逻辑</span></span><br><span class="line">          <span class="comment">// 这里还嵌套了一层 handle() 调用，真正的逻辑在 RMApp 接口的实现类 RMAppImpl 的 handle() 方法。</span></span><br><span class="line">          rmApp.handle(event);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">          LOG.error(<span class="string">&quot;Error in handling event type &quot;</span> + event.getType()</span><br><span class="line">              + <span class="string">&quot; for application &quot;</span> + appID, t);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="4-YARN事件流转源码引读"><a href="#4-YARN事件流转源码引读" class="headerlink" title="4. YARN事件流转源码引读"></a>4. YARN事件流转源码引读</h1><p>在上面 <code>事件处理逻辑</code> 小节中，详细介绍了 RMAppEventType 事件类型的 <code>RMAppEventType.APP_ACCEPTED</code> 事件的处理逻辑，下面再介绍两个 RM 相关的 Event 处理流程，主要以源码走读为例。RMNodeEvent 的处理逻辑和 RMAppEvent 的处理逻辑差不多，SchedulerEvent 又单独介绍一次，是因为 SchedulerEvent 事件处理比较特殊，因为它在 RM 本身的 Dispatcher 基础上又自定义实现了一个 Dispatcher 调度器，目的是为了将 SchedulerEvent 的事件处理与其他事件处理独立出来，避免相互影响，保证调度逻辑的正常运行。</p>
<h2 id="4-1-RMNodeEvent源码引读"><a href="#4-1-RMNodeEvent源码引读" class="headerlink" title="4.1 RMNodeEvent源码引读"></a>4.1 RMNodeEvent源码引读</h2><p>本节以 <code>RMNodeEventType.STATUS_UPDATE</code> 事件为例介绍 RMNodeEvent 事件类型处理流程，事件表示 NodeManager 向 ResourceManager 进行心跳上报。</p>
<p>心跳上报的入口是从 NodeManager 服务触发的，NodeManager 服务启动初始化是会创建 NodeStatusUpdaterImpl 对象，该对象用于执行 NM 的心跳上报逻辑。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/nodemanager/NodeManager.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 构造 NodeStatusUpdaterImpl 对象用于心跳上报</span></span><br><span class="line">    nodeStatusUpdater =</span><br><span class="line">        createNodeStatusUpdater(context, dispatcher, nodeHealthChecker);</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> NodeStatusUpdater <span class="title">createNodeStatusUpdater</span><span class="params">(Context context,</span></span></span><br><span class="line"><span class="params"><span class="function">      Dispatcher dispatcher, NodeHealthCheckerService healthChecker)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> NodeStatusUpdaterImpl(context, dispatcher, healthChecker,</span><br><span class="line">      metrics);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>构造的 NodeStatusUpdaterImpl 对象会执行 NM 向 RM 的注册和心跳上报，心跳上报会定期地调用 ResourceTrackerService#nodeHeartbeat() 方法进行。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceStart</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// NodeManager is the last service to start, so NodeId is available.</span></span><br><span class="line">    <span class="keyword">this</span>.nodeId = <span class="keyword">this</span>.context.getNodeId();</span><br><span class="line">    <span class="keyword">this</span>.httpPort = <span class="keyword">this</span>.context.getHttpPort();</span><br><span class="line">    <span class="keyword">this</span>.nodeManagerVersionId = YarnVersionInfo.getVersion();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// Registration has to be in start so that ContainerManager can get the</span></span><br><span class="line">      <span class="comment">// perNM tokens needed to authenticate ContainerTokens.</span></span><br><span class="line">      <span class="keyword">this</span>.resourceTracker = getRMClient();</span><br><span class="line">      <span class="comment">// NM 向 RM 注册</span></span><br><span class="line">      registerWithRM();</span><br><span class="line">      <span class="keyword">super</span>.serviceStart();</span><br><span class="line">      <span class="comment">// 注册后开始定期心跳上报</span></span><br><span class="line">      startStatusUpdater();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      String errorMessage = <span class="string">&quot;Unexpected error starting NodeStatusUpdater&quot;</span>;</span><br><span class="line">      LOG.error(errorMessage, e);</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">startStatusUpdater</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    statusUpdaterRunnable = <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> lastHeartBeatID = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 独立线程死循环地心跳上报</span></span><br><span class="line">        <span class="keyword">while</span> (!isStopped) &#123;</span><br><span class="line">            ...</span><br><span class="line">            <span class="comment">// 通过 rpc 服务 ResourceTrackerService 向 RM 进行心跳上报</span></span><br><span class="line">            response = resourceTracker.nodeHeartbeat(request);</span><br><span class="line">            ...</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>ResourceTrackerService#nodeHeartbeat() 执行真正的心跳上报，这里有一个关键点，就是向 RM 发送 <code>RMNodeEventType.STATUS_UPDATE</code> 事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> NodeHeartbeatResponse <span class="title">nodeHeartbeat</span><span class="params">(NodeHeartbeatRequest request)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> YarnException, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    NodeStatus remoteNodeStatus = request.getNodeStatus();</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Here is the node heartbeat sequence...</span></span><br><span class="line"><span class="comment">     * 1. Check if it&#x27;s a valid (i.e. not excluded) node</span></span><br><span class="line"><span class="comment">     * 2. Check if it&#x27;s a registered node</span></span><br><span class="line"><span class="comment">     * 3. Check if it&#x27;s a &#x27;fresh&#x27; heartbeat i.e. not duplicate heartbeat</span></span><br><span class="line"><span class="comment">     * 4. Send healthStatus to RMNode</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    ...</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 4. Send status to RMNode, saving the latest response.</span></span><br><span class="line">    <span class="comment">// 关键：构造 RMNodeEventType.STATUS_UPDATE 事件</span></span><br><span class="line">    RMNodeStatusEvent nodeStatusEvent =</span><br><span class="line">        <span class="keyword">new</span> RMNodeStatusEvent(nodeId, remoteNodeStatus.getNodeHealthStatus(),</span><br><span class="line">          remoteNodeStatus.getContainersStatuses(),</span><br><span class="line">          remoteNodeStatus.getKeepAliveApplications(), nodeHeartBeatResponse);</span><br><span class="line">    <span class="keyword">if</span> (request.getLogAggregationReportsForApps() != <span class="keyword">null</span></span><br><span class="line">        &amp;&amp; !request.getLogAggregationReportsForApps().isEmpty()) &#123;</span><br><span class="line">      nodeStatusEvent.setLogAggregationReportsForApps(request</span><br><span class="line">        .getLogAggregationReportsForApps());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 关键：向 RM Dispatcher 发送 RMNodeEventType.STATUS_UPDATE 事件</span></span><br><span class="line">    <span class="keyword">this</span>.rmContext.getDispatcher().getEventHandler().handle(nodeStatusEvent);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nodeHeartBeatResponse;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>可以看到 <code>RMNodeEventType.STATUS_UPDATE</code> 事件是通过 rmContext.getDispatcher() 获取到 RM 的 Dispatcher 对象，并调用 getEventHandler() 获取对应的 EventHandler，那这个 EventHandler 真实是什么 Handler 呢？我们来看看 RMNodeEventType 事件类型的注册位置，发现注册 EventHandler 为 NodeEventDispatcher。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      ...</span><br><span class="line">      <span class="comment">// Register event handler for RmNodes</span></span><br><span class="line">      rmDispatcher.register(</span><br><span class="line">          RMNodeEventType.class, <span class="keyword">new</span> NodeEventDispatcher(rmContext));</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>NodeEventDispatcher EventHandler 会将根据 RMNodeEventType 事件类型最终将处理逻辑交给 RMNode 处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">NodeEventDispatcher</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">EventHandler</span>&lt;<span class="title">RMNodeEvent</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> RMContext rmContext;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">NodeEventDispatcher</span><span class="params">(RMContext rmContext)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.rmContext = rmContext;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(RMNodeEvent event)</span> </span>&#123;</span><br><span class="line">      NodeId nodeId = event.getNodeId();</span><br><span class="line">      <span class="comment">// 将 RMNodeEventType.STATUS_UPDATE 事件交给 RMNode 处理</span></span><br><span class="line">      RMNode node = <span class="keyword">this</span>.rmContext.getRMNodes().get(nodeId);</span><br><span class="line">      <span class="keyword">if</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          ((EventHandler&lt;RMNodeEvent&gt;) node).handle(event);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">          LOG.error(<span class="string">&quot;Error in handling event type &quot;</span> + event.getType()</span><br><span class="line">              + <span class="string">&quot; for node &quot;</span> + nodeId, t);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>RMNode 是个接口类，具体的实现交给 RMNodeImpl 子类处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(RMNodeEvent event)</span> </span>&#123;</span><br><span class="line">    LOG.debug(<span class="string">&quot;Processing &quot;</span> + event.getNodeId() + <span class="string">&quot; of type &quot;</span> + event.getType());</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      writeLock.lock();</span><br><span class="line">      NodeState oldState = getState();</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">         <span class="comment">// 根据 RMNodeEventType.STATUS_UPDATE 事件进行状态机的转换，交给对应状态机处理</span></span><br><span class="line">         stateMachine.doTransition(event.getType(), event);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InvalidStateTransitonException e) &#123;</span><br><span class="line">        LOG.error(<span class="string">&quot;Can&#x27;t handle this event at current state&quot;</span>, e);</span><br><span class="line">        LOG.error(<span class="string">&quot;Invalid event &quot;</span> + event.getType() + </span><br><span class="line">            <span class="string">&quot; on Node  &quot;</span> + <span class="keyword">this</span>.nodeId);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (oldState != getState()) &#123;</span><br><span class="line">        LOG.info(nodeId + <span class="string">&quot; Node Transitioned from &quot;</span> + oldState + <span class="string">&quot; to &quot;</span></span><br><span class="line">                 + getState());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">finally</span> &#123;</span><br><span class="line">      writeLock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><code>RMNodeEventType.STATUS_UPDATE</code> 事件在 RMNodeImpl#handle() 方法中并没有真正去处理，而是根据事件类型交给具体的状态机处理。（这里涉及到状态机的逻辑，本文没有详细介绍，不过我们可以根据事件类型找到对应的事件状态变化，RMNodeImpl 在初始化时创建了大量事件状态流转过程，<code>RMNodeEventType.STATUS_UPDATE</code> 事件也对应这多种状态变化，比如将 NM 状态从 RUNNING 状态转换到到 RUNNING 状态，也可以从 UNHEALTHY 状态转换到 UNHEALTHY 或 RUNNING 状态，显然在 NM 心跳上报时我们关注的是从 RUNNING 到 RUNNING 的状态变化过程。）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> StateMachineFactory&lt;RMNodeImpl,</span><br><span class="line">                                           NodeState,</span><br><span class="line">                                           RMNodeEventType,</span><br><span class="line">                                           RMNodeEvent&gt; stateMachineFactory </span><br><span class="line">                 = <span class="keyword">new</span> StateMachineFactory&lt;RMNodeImpl,</span><br><span class="line">                                           NodeState,</span><br><span class="line">                                           RMNodeEventType,</span><br><span class="line">                                           RMNodeEvent&gt;(NodeState.NEW)</span><br><span class="line">      ...</span><br><span class="line">      <span class="comment">//Transitions from RUNNING state</span></span><br><span class="line">      <span class="comment">// NM 心跳上报表示从 RUNNING 到 RUNNING 的状态，由 StatusUpdateWhenHealthyTransition 状态机处理</span></span><br><span class="line">      .addTransition(NodeState.RUNNING,</span><br><span class="line">          EnumSet.of(NodeState.RUNNING, NodeState.UNHEALTHY),</span><br><span class="line">          RMNodeEventType.STATUS_UPDATE,</span><br><span class="line">          <span class="keyword">new</span> StatusUpdateWhenHealthyTransition())</span><br><span class="line">      ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>StatusUpdateWhenHealthyTransition 状态机会真正处理 <code>RMNodeEventType.STATUS_UPDATE</code> 事件，主要是 NM 心跳上报时更新 Container 状态，包括正在运行或已完成的 Container 信息。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Status update transition when node is healthy.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">StatusUpdateWhenHealthyTransition</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">MultipleArcTransition</span>&lt;<span class="title">RMNodeImpl</span>, <span class="title">RMNodeEvent</span>, <span class="title">NodeState</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NodeState <span class="title">transition</span><span class="params">(RMNodeImpl rmNode, RMNodeEvent event)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">      RMNodeStatusEvent statusEvent = (RMNodeStatusEvent) event;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// NM 职责：Container 状态更新，包括正在运行或已完成的 Container</span></span><br><span class="line">      rmNode.handleContainerStatus(statusEvent.getContainers());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>至此，NM 心跳上报的 <code>RMNodeEventType.STATUS_UPDATE</code> 事件的整个流转过程已介绍完毕。</p>
<h2 id="4-2-SchedulerEvent源码引读"><a href="#4-2-SchedulerEvent源码引读" class="headerlink" title="4.2 SchedulerEvent源码引读"></a>4.2 SchedulerEvent源码引读</h2><p>本节以<code>SchedulerEventType.NODE_UPDATE</code> 事件为例介绍 SchedulerEvent 事件类型的处理流程，上一节中介绍的事件 <code>MNodeEventType.STATUS_UPDATE</code> 是 NM 向 RM 进行心跳上报更新 Container 状态信息，本节介绍的事件也是 NM 心跳上报的另一个作用，NM 通过心跳上报向 RM 上报节点资源使用情况，供调度器进行资源调度。</p>
<p>接着上小节 StatusUpdateWhenHealthyTransition 状态机处理 <code>MNodeEventType.STATUS_UPDATE</code> 的逻辑，我们看到在执行完 <code>rmNode.handleContainerStatus()</code> 逻辑后， 会向 RM 中的调度器发送 <code>SchedulerEventType.NODE_UPDATE</code> 事件。</p>
<blockquote>
<p> Tips：这里有一个小知识点，就是发送 <code>SchedulerEventType.NODE_UPDATE</code> 事件这里有个 nextHeartBeat 心跳检测，nextHeartBeat 变量是 Boolean 类型，初始值为 true，也就是说 NM 第一次心跳是能够正常发送 NODE_UPDATE 事件，发送前 nextHeartBeat 置为 false，然后在 FairScheduler 调度器真正处理 NODE_UPDATE 事件是又会将 nextHeartBeat 变量置为 true，以进行下一次 NM 心跳上报的处理。FairScheduler 将 nextHeartBeat 标识置为 true 的逻辑在 FairScheduler#nodeUpdate() 方法内部的 nm.pullContainerUpdates() 逻辑中，感兴趣的同时可以去看看。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Status update transition when node is healthy.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">StatusUpdateWhenHealthyTransition</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">MultipleArcTransition</span>&lt;<span class="title">RMNodeImpl</span>, <span class="title">RMNodeEvent</span>, <span class="title">NodeState</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NodeState <span class="title">transition</span><span class="params">(RMNodeImpl rmNode, RMNodeEvent event)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">      RMNodeStatusEvent statusEvent = (RMNodeStatusEvent) event;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// NM 职责：Container 状态更新，包括正在运行或已完成的 Container</span></span><br><span class="line">      rmNode.handleContainerStatus(statusEvent.getContainers());</span><br><span class="line"></span><br><span class="line">      <span class="comment">// nextHeartBeat 标识用来保证 NM 每次心跳上报只会为其执行一次调度逻辑，一次调度可分配出去多个 Container</span></span><br><span class="line">      <span class="comment">// nextHeartBeat 默认为 true，开启发送 NODE_UPDATE 事件时置为 false，在 FairScheduler#nodeUpdate() 分配完 Containers 后又会置为 true。</span></span><br><span class="line">      <span class="keyword">if</span>(rmNode.nextHeartBeat) &#123;</span><br><span class="line">        rmNode.nextHeartBeat = <span class="keyword">false</span>;</span><br><span class="line">        <span class="comment">// 关键：向 RM 的调度器发送 SchedulerEventType.NODE_UPDATE 事件</span></span><br><span class="line">        rmNode.context.getDispatcher().getEventHandler().handle(</span><br><span class="line">            <span class="keyword">new</span> NodeUpdateSchedulerEvent(rmNode));</span><br><span class="line">      &#125;        </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/event/NodeUpdateSchedulerEvent.java      </span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NodeUpdateSchedulerEvent</span> <span class="keyword">extends</span> <span class="title">SchedulerEvent</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> RMNode rmNode;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">NodeUpdateSchedulerEvent</span><span class="params">(RMNode rmNode)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(SchedulerEventType.NODE_UPDATE);</span><br><span class="line">    <span class="keyword">this</span>.rmNode = rmNode;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> RMNode <span class="title">getRMNode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> rmNode;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;     </span><br></pre></td></tr></table></figure>

<p>同样的，<code>SchedulerEventType.NODE_UPDATE</code> 事件的处理和 RMNodeEvent 类似，还是会发送到 RM AsyncDispatcher 的 EventHandler#handle() 处理。来看下 SchedulerEventType 事件类型注册的 EventHandler 是什么。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      ...</span><br><span class="line">      schedulerDispatcher = createSchedulerEventDispatcher();</span><br><span class="line">      addIfService(schedulerDispatcher);</span><br><span class="line">      rmDispatcher.register(SchedulerEventType.class, schedulerDispatcher);</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 对应的 EventHandler 为 SchedulerEventDispatcher</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> EventHandler&lt;SchedulerEvent&gt; <span class="title">createSchedulerEventDispatcher</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> SchedulerEventDispatcher(<span class="keyword">this</span>.scheduler);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p><strong>敲重点：SchedulerEventDispatcher 的设计就是处理 SchedulerEvent 的核心所在，也是与 RM 的其他 EventHander 的不同所在。</strong>根据前面介绍 NodeEventDispatcher 只实现了 EventHandler 接口，而 SchedulerEventDispatcher 既实现了 EventHandler 接口，还继承了 AbstractService 抽象类，也就是说，SchedulerEventDispatcher 既是事件处理器也是一个服务。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SchedulerEventDispatcher</span> <span class="keyword">extends</span> <span class="title">AbstractService</span></span></span><br><span class="line"><span class="class">      <span class="keyword">implements</span> <span class="title">EventHandler</span>&lt;<span class="title">SchedulerEvent</span>&gt; </span></span><br></pre></td></tr></table></figure>

<p>SchedulerEventDispatcher 作为 EventHandler 通过 handle() 方法来接收到前面发送的 <code>SchedulerEventType.NODE_UPDATE</code> 事件，并将事件又放入一个 eventQueue 中。那作为 Service 又做了什么事呢？SchedulerEventDispatcher 作为服务又自己的服务初始化、服务启动和服务停止操作。其实现思想和 AsyncDispatcher 的生产者-消费者模型基本一致，将 Event 添加到自己的阻塞队列 eventQueue 中，然后启动一个独立的 EventProcessor 线程从 eventQueue 中取出 Event，最终交由给 ResourceScheduler 处理。</p>
<blockquote>
<p> 小结：也就是说，SchedulerEventDispatcher 的实现是 AsyncDispatcher 的一个嵌套实现，事件发送到 RM 的 AsyncDispatcher 中后，从 RM AsyncDispatcher 的 eventQueue 中取出事件后，SchedulerEventDispatcher 作为 EventHandler 通过 handle() 方法将事件放入自己内部的 eventQueue，最后通过后台独立的 EventProcessor 线程从自己的 eventQueue 中取出事件处理。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SchedulerEventDispatcher</span> <span class="keyword">extends</span> <span class="title">AbstractService</span></span></span><br><span class="line"><span class="class">      <span class="keyword">implements</span> <span class="title">EventHandler</span>&lt;<span class="title">SchedulerEvent</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// RM 调度器，其实现包括 FifoScheduler、FairScheduler、CapacityScheduler</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ResourceScheduler scheduler;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// SchedulerEventDispatcher 内部自己的 eventQueue 阻塞队列</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> BlockingQueue&lt;SchedulerEvent&gt; eventQueue =</span><br><span class="line">      <span class="keyword">new</span> LinkedBlockingQueue&lt;SchedulerEvent&gt;();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">int</span> lastEventQueueSizeLogged = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// SchedulerEventDispatcher 内部的 eventPorcessor 独立线程</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Thread eventProcessor;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> stopped = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> shouldExitOnError = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SchedulerEventDispatcher</span><span class="params">(ResourceScheduler scheduler)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">super</span>(SchedulerEventDispatcher.class.getName());</span><br><span class="line">      <span class="keyword">this</span>.scheduler = scheduler;</span><br><span class="line">      <span class="keyword">this</span>.eventProcessor = <span class="keyword">new</span> Thread(<span class="keyword">new</span> EventProcessor());</span><br><span class="line">      <span class="keyword">this</span>.eventProcessor.setName(<span class="string">&quot;ResourceManager Event Processor&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 服务的初始化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.shouldExitOnError =</span><br><span class="line">          conf.getBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY,</span><br><span class="line">            Dispatcher.DEFAULT_DISPATCHER_EXIT_ON_ERROR);</span><br><span class="line">      <span class="keyword">super</span>.serviceInit(conf);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 服务的启动</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceStart</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="comment">// 启动 EventProcessor 线程</span></span><br><span class="line">      <span class="keyword">this</span>.eventProcessor.start();</span><br><span class="line">      <span class="keyword">super</span>.serviceStart();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">EventProcessor</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        SchedulerEvent event;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (!stopped &amp;&amp; !Thread.currentThread().isInterrupted()) &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 关键：从内部 eventQueue 中阻塞取出事件</span></span><br><span class="line">            event = eventQueue.take();</span><br><span class="line">          &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            LOG.error(<span class="string">&quot;Returning, interrupted : &quot;</span> + e);</span><br><span class="line">            <span class="keyword">return</span>; <span class="comment">// <span class="doctag">TODO:</span> Kill RM.</span></span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 通过 ResourceScheduler 将 event 转发给具体调度器处理</span></span><br><span class="line">            scheduler.handle(event);</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            <span class="comment">// An error occurred, but we are shutting down anyway.</span></span><br><span class="line">            <span class="comment">// If it was an InterruptedException, the very act of </span></span><br><span class="line">            <span class="comment">// shutdown could have caused it and is probably harmless.</span></span><br><span class="line">            <span class="keyword">if</span> (stopped) &#123;</span><br><span class="line">              LOG.warn(<span class="string">&quot;Exception during shutdown: &quot;</span>, t);</span><br><span class="line">              <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            LOG.fatal(<span class="string">&quot;Error in handling event type &quot;</span> + event.getType()</span><br><span class="line">                + <span class="string">&quot; to the scheduler&quot;</span>, t);</span><br><span class="line">            <span class="keyword">if</span> (shouldExitOnError</span><br><span class="line">                &amp;&amp; !ShutdownHookManager.get().isShutdownInProgress()) &#123;</span><br><span class="line">              LOG.info(<span class="string">&quot;Exiting, bbye..&quot;</span>);</span><br><span class="line">              System.exit(-<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 服务的停止</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceStop</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.stopped = <span class="keyword">true</span>;</span><br><span class="line">      <span class="keyword">this</span>.eventProcessor.interrupt();</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.eventProcessor.join();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(e);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">super</span>.serviceStop();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 作为 EventHandler 接收事件的入口</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(SchedulerEvent event)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">int</span> qSize = eventQueue.size();</span><br><span class="line"></span><br><span class="line">        ClusterMetrics.getMetrics().setRMSchedulerEventQueueSize(qSize);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (qSize != <span class="number">0</span> &amp;&amp; qSize % <span class="number">1000</span> == <span class="number">0</span></span><br><span class="line">            &amp;&amp; lastEventQueueSizeLogged != qSize) &#123;</span><br><span class="line">          lastEventQueueSizeLogged = qSize;</span><br><span class="line">          LOG.info(<span class="string">&quot;Size of scheduler event-queue is &quot;</span> + qSize);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> remCapacity = eventQueue.remainingCapacity();</span><br><span class="line">        <span class="keyword">if</span> (remCapacity &lt; <span class="number">1000</span>) &#123;</span><br><span class="line">          LOG.info(<span class="string">&quot;Very low remaining capacity on scheduler event queue: &quot;</span></span><br><span class="line">              + remCapacity);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关键：将接收到的事件放入自己内部的 eventQueue 阻塞队列中</span></span><br><span class="line">        <span class="keyword">this</span>.eventQueue.put(event);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        LOG.info(<span class="string">&quot;Interrupted. Trying to exit gracefully.&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>YARN 中 ResourceScheduler 接口的实现包括 FifoScheduler、FairScheduler 和 CapacityScheduler 三种调度器模型，本文以 FairScheduler 调度器为例，介绍调度器 <code>scheduler.handle(event)</code>  是如何处理事件的。我们直到 <code>SchedulerEventType.NODE_UPDATE</code> 事件是由 NodeUpdateSchedulerEvent 构造的，而 NodeUpdateSchedulerEvent 类继承 SchedulerEvent 类，事件对应的类型为 NODE_UPDATE，于是 FairScheduler 的 handle() 方法会进入到 NODE_UPDATE 代码块。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(SchedulerEvent event)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// FairScheduler 的核心逻辑</span></span><br><span class="line">    <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 处理 SchedulerEventType.NODE_UPDATE 事件</span></span><br><span class="line">    <span class="keyword">case</span> NODE_UPDATE:</span><br><span class="line">      <span class="keyword">if</span> (!(event <span class="keyword">instanceof</span> NodeUpdateSchedulerEvent)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Unexpected event type: &quot;</span> + event);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// SchedulerEventType.NODE_UPDATE 事件的真正处理逻辑</span></span><br><span class="line">      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;</span><br><span class="line">      nodeUpdate(nodeUpdatedEvent.getRMNode());</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>下面这段代码是 FairScheduler 的核心逻辑，也是 YARN 分配资源的核心逻辑，主要是更新 Containers 状态和对NM节点进行资源分配。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Process a heartbeat update from a node.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">nodeUpdate</span><span class="params">(RMNode nm)</span> </span>&#123;</span><br><span class="line">      ...</span><br><span class="line">      <span class="comment">// 处理新运行或运行完成的 Containers 信息</span></span><br><span class="line">      <span class="comment">// Processing the newly launched containers</span></span><br><span class="line">      <span class="keyword">for</span> (ContainerStatus launchedContainer : newlyLaunchedContainers) &#123;</span><br><span class="line">        containerLaunchedOnNode(launchedContainer.getContainerId(), node);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// Process completed containers</span></span><br><span class="line">      <span class="keyword">for</span> (ContainerStatus completedContainer : completedContainers) &#123;</span><br><span class="line">        ContainerId containerId = completedContainer.getContainerId();</span><br><span class="line">        LOG.debug(<span class="string">&quot;Container FINISHED: &quot;</span> + containerId);</span><br><span class="line">        completedContainer(getRMContainer(containerId),</span><br><span class="line">            completedContainer, RMContainerEventType.FINISHED);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">	  <span class="comment">// FairScheudler 资源调度方式有两种，一种是连续调度方式（默认是关闭的），一种是NM心跳调度方式</span></span><br><span class="line">      <span class="keyword">if</span> (continuousSchedulingEnabled) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!completedContainers.isEmpty()) &#123;</span><br><span class="line">          attemptScheduling(node);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// FairScheduler 根据NM节点资源情况进行资源调度</span></span><br><span class="line">        attemptScheduling(node);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，<code>SchedulerEventType.NODE_UPDATE</code> 事件的处理流程介绍就结束了。</p>
<blockquote>
<p>END: 阅读完本文，可以再来思考第一小节中遗留的几个问题哟。</p>
</blockquote>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><p><a href="http://wiki.hacksmeta.com/distributedsystem/yarn-state-machine-and-event-driven.html">yarn-state-machine-and-event-driven</a></p>
</li>
<li><p><a href="http://www.inter12.org/archives/1379">yarn 设计之 Dispatcher</a></p>
</li>
<li><p><a href="https://monkeysayhi.github.io/2018/11/20/%E6%BA%90%E7%A0%81|Yarn%E7%9A%84%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%8A%B6%E6%80%81%E6%9C%BA/">源码|Yarn的事件驱动模型与状态机</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>YARN</tag>
        <tag>YARN源码</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Listener设计思想及源码引读</title>
    <url>/2021/11/28/Spark-Listener%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E5%8F%8A%E6%BA%90%E7%A0%81%E5%BC%95%E8%AF%BB/</url>
    <content><![CDATA[<blockquote>
<p>源码版本：Apache Spark 2.4.7</p>
<p>导读：网上看过一些介绍 Spark Listener 事件监听机制的介绍，大多都是从设计模式中的观察者模式介绍，Spark Listener 作为事件监听器，当有事件请求抵达时，监听器会发生变化，处理对应的事件请求。但在阅读过 YARN 源码 Dispatcher 设计（可以参考文章<a href="https://benkoons.github.io/2021/11/28/YARN-Dispatcher%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E5%8F%8A%E6%BA%90%E7%A0%81%E5%BC%95%E8%AF%BB/">YARN Dispatcher设计思想及源码引读</a>）后，发现 Spark Listener 的整体设计思想和 YARN 非常类似，都是采用经典的基于事件驱动的处理模型，也就是所谓的<code>生产者-消费者模型</code>，本文会参照 YARN Dispatcher 的设计思想，以 <code>Event -&gt; Dispatcher -&gt; EventHandler</code> 的事件流转流程介绍 Spark Listener 的具体实现。</p>
</blockquote>
<h1 id="1-Spark-Listener介绍"><a href="#1-Spark-Listener介绍" class="headerlink" title="1. Spark Listener介绍"></a>1. Spark Listener介绍</h1><p>Spark 和 YARN 都是基于事件驱动的处理模型，所以整体设计思想是一致的，包括：</p>
<ul>
<li><p>事件：各种请求的事件类型</p>
</li>
<li><p>事件转发器：根据请求的事件类型对事件进行转发，转发对应的事件处理器处理。</p>
</li>
<li><p>事件处理器：根据事件类型执行真正的事件处理逻辑。</p>
</li>
</ul>
<p>不同于 YARN 的是，Spark 中的事件、事件转发器和事件处理器的命令并不一样，不过按照我自己的理解，两者也存在相应的对应关系：</p>
<table>
<thead>
<tr>
<th align="center"><strong>对比项</strong></th>
<th align="center"><strong>YARN</strong></th>
<th align="center"><strong>Spark</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>事件</strong></td>
<td align="center">Event</td>
<td align="center">Event</td>
</tr>
<tr>
<td align="center"><strong>事件转发器</strong></td>
<td align="center">Dispatcher</td>
<td align="center">ListenerBus</td>
</tr>
<tr>
<td align="center"><strong>事件处理器</strong></td>
<td align="center">EventHandler</td>
<td align="center">SparkListener</td>
</tr>
</tbody></table>
<p>为理解各个组件在 Spark 源码中的继承关系，下面梳理了部分与事件处理模型相关的类和接口的继承关系，方便对 Spark Listener 源码架构有大概的理解。（这里并没有画 UML 继承图，主要是图方便，能理解不同类型的类的继承结构即可）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Event事件</span></span><br><span class="line">trait SparkListenerEvent</span><br><span class="line">case class SparkListenerJobStart extends SparkListenerEvent</span><br><span class="line">case class SparkListenerStageSubmitted extends SparkListenerEvent</span><br><span class="line">case class SparkListenerTaskStart extends SparkListenerEvent</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ListenerBus总线</span></span><br><span class="line">trait ListenerBus</span><br><span class="line">trait SparkListenerBus extends ListenerBus</span><br><span class="line">class AsyncEventQueue extends SparkListenerBus</span><br><span class="line">class ReplayListenerBus extends SparkListenerBus</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SparkListener事件监听器</span></span><br><span class="line">trait SparkListenerInterface</span><br><span class="line">abstract class SparkListener extends SparkListenerInterface</span><br><span class="line">		class EventLoggingListener extends SparkListener</span><br><span class="line">		class AppStatusListener extends SparkListener</span><br><span class="line">    class ExecutorAllocationListener extends SparkListener</span><br><span class="line">    class HeartbeatReceiver extends SparkListener</span><br><span class="line">class SparkFirehoseListener implements SparkListenerInterface</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> DAGScheduler内部独立的生产者-消费者模型，和YARN的SchedulerEventDispatcher设计是一致的</span></span><br><span class="line">trait DAGSchedulerEvent</span><br><span class="line">case class JobSubmitted extends DAGSchedulerEvent</span><br><span class="line">case class MapStageSubmitted extends DAGSchedulerEvent</span><br><span class="line"></span><br><span class="line">private[scheduler] class DAGSchedulerEventProcessLoop(dagScheduler: DAGScheduler)</span><br><span class="line">  extends EventLoop[DAGSchedulerEvent](&quot;dag-scheduler-event-loop&quot;)</span><br></pre></td></tr></table></figure>

<h1 id="2-Spark-Listener设计原理"><a href="#2-Spark-Listener设计原理" class="headerlink" title="2. Spark Listener设计原理"></a>2. Spark Listener设计原理</h1><h2 id="2-1-ListenerBus总览"><a href="#2-1-ListenerBus总览" class="headerlink" title="2.1 ListenerBus总览"></a>2.1 ListenerBus总览</h2><p>从上面 ListenerBus 的类继承关系，发现 SparkListenerBus 只有 AsyncEventQueue 和 ReplayListenerBus 两类总线，ReplayListenerBus 总线功能比较单一，主要是用于 SparkHistoryServer 服务回放解析 eventlog 日志，而 AsyncEventQueue 总线才是 SparkListenerBus 的核心总线，担负起 Spark 中所有 SparkListener 事件的分发。</p>
<p>AsyncEventQueue 总线在 Spark 中包括四类，总线的名字如下。可以看到四类总线的定义是在 LiveListenerBus 类中， LiveListenerBus 看名字容易误解，其实它不是 Spark 的一种 ListenerBus，是以独立的类存在，但它的作用非常重要，负责管理和分发 SparkListener 事件到四种总线，</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">object</span> <span class="title">LiveListenerBus</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="keyword">val</span> <span class="type">SHARED_QUEUE</span> = <span class="string">&quot;shared&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="keyword">val</span> <span class="type">APP_STATUS_QUEUE</span> = <span class="string">&quot;appStatus&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="keyword">val</span> <span class="type">EXECUTOR_MANAGEMENT_QUEUE</span> = <span class="string">&quot;executorManagement&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="keyword">val</span> <span class="type">EVENT_LOG_QUEUE</span> = <span class="string">&quot;eventLog&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>那四类总线是如何创建的呢？LiveListenerBus 定义了四种总线对应的 addToQueue 方法，在对应 AsyncEventQueue 总线没有被创建时，就会创建一个总线，并添加到总线集合 queues。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala</span></span><br><span class="line">  <span class="comment">/** Add a listener to queue shared by all non-internal listeners. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addToSharedQueue</span></span>(listener: <span class="type">SparkListenerInterface</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    addToQueue(listener, <span class="type">SHARED_QUEUE</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Add a listener to the executor management queue. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addToManagementQueue</span></span>(listener: <span class="type">SparkListenerInterface</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    addToQueue(listener, <span class="type">EXECUTOR_MANAGEMENT_QUEUE</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Add a listener to the application status queue. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addToStatusQueue</span></span>(listener: <span class="type">SparkListenerInterface</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    addToQueue(listener, <span class="type">APP_STATUS_QUEUE</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Add a listener to the event log queue. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addToEventLogQueue</span></span>(listener: <span class="type">SparkListenerInterface</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    addToQueue(listener, <span class="type">EVENT_LOG_QUEUE</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Add a listener to a specific queue, creating a new queue if needed. Queues are independent</span></span><br><span class="line"><span class="comment">   * of each other (each one uses a separate thread for delivering events), allowing slower</span></span><br><span class="line"><span class="comment">   * listeners to be somewhat isolated from others.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">addToQueue</span></span>(</span><br><span class="line">      listener: <span class="type">SparkListenerInterface</span>,</span><br><span class="line">      queue: <span class="type">String</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;LiveListenerBus is stopped.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断 queues 集合中是否已经存在对应的 queue 总线（queue 表示 AsyncEventQueue 总线）</span></span><br><span class="line">    queues.asScala.find(_.name == queue) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(queue) =&gt;</span><br><span class="line">        queue.addListener(listener)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">// 根据 queue 名字创建对应的总线，AsyncEventQueue 就是这个总线</span></span><br><span class="line">        <span class="keyword">val</span> newQueue = <span class="keyword">new</span> <span class="type">AsyncEventQueue</span>(queue, conf, metrics, <span class="keyword">this</span>)</span><br><span class="line">        newQueue.addListener(listener)</span><br><span class="line">        <span class="keyword">if</span> (started.get()) &#123;</span><br><span class="line">          newQueue.start(sparkContext)</span><br><span class="line">        &#125;</span><br><span class="line">        queues.add(newQueue)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-2-SparkListener注册逻辑"><a href="#2-2-SparkListener注册逻辑" class="headerlink" title="2.2 SparkListener注册逻辑"></a>2.2 SparkListener注册逻辑</h2><p>我们知道 YARN 中 Dispatcher 注册对象是 <code>EventType -&gt; EventHandler</code> 的映射关系，而 Spark 设计思想里是以 Listener 对象作为注册对象，将不同的 SparkListener 对象注册是对应的总线上。我们还是以上面的四种总线为例，看看 SparkListener 是如何添加到不同的总线上。</p>
<h3 id="2-2-1-注册到Shared总线"><a href="#2-2-1-注册到Shared总线" class="headerlink" title="2.2.1 注册到Shared总线"></a>2.2.1 注册到Shared总线</h3><p>addToSharedQueue 方式是 SparkListener 注册到 Shared-AsyncEventQueue 总线的入口。</p>
<ul>
<li><strong>自定义扩展 SparkListener 注册。</strong></li>
</ul>
<p>Shared-AsyncEventQueue 总线在 Spark 中默认是没有注册任何 SparkListener 的，主要是用于外部自定义扩展的 SparkListener 注册，可以扩展多个 SparkListener。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/SparkContext.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">setupAndStartListenerBus</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    conf.get(<span class="type">EXTRA_LISTENERS</span>).foreach &#123; classNames =&gt;</span><br><span class="line">      <span class="comment">// 根据类名加载外部扩展的 SparkListener</span></span><br><span class="line">      <span class="keyword">val</span> listeners = <span class="type">Utils</span>.loadExtensions(classOf[<span class="type">SparkListenerInterface</span>], classNames, conf)</span><br><span class="line">      <span class="comment">// 将 SparkListener 添加到 Shared-AsyncEventQueue 总线上</span></span><br><span class="line">      listeners.foreach &#123; listener =&gt;</span><br><span class="line">        listenerBus.addToSharedQueue(listener)</span><br><span class="line">        logInfo(<span class="string">s&quot;Registered listener <span class="subst">$&#123;listener.getClass().getName()&#125;</span>&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// An asynchronous listener bus for Spark events</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">listenerBus</span></span>: <span class="type">LiveListenerBus</span> = _listenerBus</span><br></pre></td></tr></table></figure>

<p>addToQueue() 方法内部的 <code>queue.addListener(listener)</code> 操作是真正将具体 SparkListener 关联到对应 SparkListenerBus 上。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala</span></span><br><span class="line">  <span class="comment">/** Add a listener to queue shared by all non-internal listeners. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addToSharedQueue</span></span>(listener: <span class="type">SparkListenerInterface</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    addToQueue(listener, <span class="type">SHARED_QUEUE</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">addToQueue</span></span>(</span><br><span class="line">      listener: <span class="type">SparkListenerInterface</span>,</span><br><span class="line">      queue: <span class="type">String</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;LiveListenerBus is stopped.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断 queues 集合中是否存在名字为 queue 的总线（queue 表示 AsyncEventQueue 总线）</span></span><br><span class="line">    queues.asScala.find(_.name == queue) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// 如果 queue 总线存在，则直接将 SparkListener 添加到该总线上</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(queue) =&gt;</span><br><span class="line">        queue.addListener(listener)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 如果 queue 总线不存在，则创建对应的 queue 总线，并将 SparkListener 添加到总线上</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">// 根据 queue 名字创建对应的总线，AsyncEventQueue 就是这个总线</span></span><br><span class="line">        <span class="keyword">val</span> newQueue = <span class="keyword">new</span> <span class="type">AsyncEventQueue</span>(queue, conf, metrics, <span class="keyword">this</span>)</span><br><span class="line">        newQueue.addListener(listener)</span><br><span class="line">        <span class="keyword">if</span> (started.get()) &#123;</span><br><span class="line">          newQueue.start(sparkContext)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 四个总线的集合</span></span><br><span class="line">        queues.add(newQueue)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-2-2-注册到ExecutorManagement总线"><a href="#2-2-2-注册到ExecutorManagement总线" class="headerlink" title="2.2.2 注册到ExecutorManagement总线"></a>2.2.2 注册到ExecutorManagement总线</h3><p>addToManagementQueue 方式是 SparkListener 注册到 ExecutorManagement-AsyncEventQueue 总线的入口。</p>
<ul>
<li><strong>ExecutorAllocationListener 注册。</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    listenerBus.addToManagementQueue(listener)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// listener定义</span></span><br><span class="line">  <span class="keyword">val</span> listener = <span class="keyword">new</span> <span class="type">ExecutorAllocationListener</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>HeartbeatReceiver 注册。</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Lives in the driver to receive heartbeats from executors..</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">HeartbeatReceiver</span>(<span class="params">sc: <span class="type">SparkContext</span>, clock: <span class="type">Clock</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">SparkListener</span> <span class="keyword">with</span> <span class="type">ThreadSafeRpcEndpoint</span> <span class="keyword">with</span> <span class="type">Logging</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// this 表示 HeartbeatReceiver 类，也是一个 SparkListener</span></span><br><span class="line">  sc.listenerBus.addToManagementQueue(<span class="keyword">this</span>)</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-2-3-注册到AppStatus总线"><a href="#2-2-3-注册到AppStatus总线" class="headerlink" title="2.2.3 注册到AppStatus总线"></a>2.2.3 注册到AppStatus总线</h3><p>addToStatusQueue 方式是 SparkListener 注册到 AppStatus-AsyncEventQueue 总线的入口。</p>
<ul>
<li><strong>AppStatusListener 注册。</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/SparkContext.scala</span></span><br><span class="line">    <span class="comment">// Initialize the app status store and listener before SparkEnv is created so that it gets all events.</span></span><br><span class="line">    <span class="comment">// _statusStore.listener 为 AppStatusListener</span></span><br><span class="line">    _statusStore = <span class="type">AppStatusStore</span>.createLiveStore(conf)</span><br><span class="line">    listenerBus.addToStatusQueue(_statusStore.listener.get)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/status/AppStatusStore.scala</span></span><br><span class="line">	<span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">AppStatusStore</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    val store: <span class="type">KVStore</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val listener: <span class="type">Option</span>[<span class="type">AppStatusListener</span>] = <span class="type">None</span></span>)</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>BarrierCoordinator 内部 SparkListener 注册。</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/BarrierCoordinator.scala</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">super</span>.onStart()</span><br><span class="line">    listenerBus.addToStatusQueue(listener)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 内部直接创建一个 SparkListener</span></span><br><span class="line">  <span class="comment">// Listen to StageCompleted event, clear corresponding ContextBarrierState.</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> listener = <span class="keyword">new</span> <span class="type">SparkListener</span> &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageCompleted</span></span>(stageCompleted: <span class="type">SparkListenerStageCompleted</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> stageInfo = stageCompleted.stageInfo</span><br><span class="line">      <span class="keyword">val</span> barrierId = <span class="type">ContextBarrierId</span>(stageInfo.stageId, stageInfo.attemptNumber)</span><br><span class="line">      <span class="comment">// Clear ContextBarrierState from a finished stage attempt.</span></span><br><span class="line">      cleanupBarrierStage(barrierId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-2-4-注册到EventLog总线"><a href="#2-2-4-注册到EventLog总线" class="headerlink" title="2.2.4 注册到EventLog总线"></a>2.2.4 注册到EventLog总线</h3><p>addToEventLogQueue 方式是 SparkListener 注册到 EventLog-AsyncEventQueue 总线的入口。</p>
<ul>
<li><strong>EventLoggingListener 注册。</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/SparkContext.scala</span></span><br><span class="line">    _eventLogger =</span><br><span class="line">      <span class="keyword">if</span> (isEventLogEnabled) &#123;</span><br><span class="line">        <span class="keyword">val</span> logger =</span><br><span class="line">          <span class="keyword">new</span> <span class="type">EventLoggingListener</span>(_applicationId, _applicationAttemptId, _eventLogDir.get,</span><br><span class="line">            _conf, _hadoopConfiguration)</span><br><span class="line">        logger.start()</span><br><span class="line">        listenerBus.addToEventLogQueue(logger)</span><br><span class="line">        <span class="type">Some</span>(logger)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-3-Event处理逻辑"><a href="#2-3-Event处理逻辑" class="headerlink" title="2.3 Event处理逻辑"></a>2.3 Event处理逻辑</h2><p>Spark 的事件处理模型也是经典的<code>生产者-消费者模型</code>，对于 Event 的处理逻辑需要找到起对应的生产者和消费者。Spark 中有两大类 Event，一类是 SparkListenerEvent，作用域贯穿整个 SparkListener，一类是 DAGSchedulerEvent，作用域为 DAGScheduer 类。</p>
<h3 id="2-3-1-DAGSchedulerEvent处理逻辑"><a href="#2-3-1-DAGSchedulerEvent处理逻辑" class="headerlink" title="2.3.1 DAGSchedulerEvent处理逻辑"></a>2.3.1 DAGSchedulerEvent处理逻辑</h3><p>DAGSchedulerEvent 事件的处理逻辑基本都是通过 <code>eventProcessLoop.post(event)</code> 方式发送事件。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">	<span class="comment">// DAGSchedulerEvent 的提交方式，以 JobSubmitted 事件为例。</span></span><br><span class="line">	eventProcessLoop.post(<span class="type">JobSubmitted</span>())</span><br><span class="line"></span><br><span class="line">  <span class="comment">// eventProcessLoop 是一个 EventLoop 类</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> eventProcessLoop = <span class="keyword">new</span> <span class="type">DAGSchedulerEventProcessLoop</span>(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(<span class="params">dagScheduler: <span class="type">DAGScheduler</span></span>)</span></span><br><span class="line">    <span class="keyword">extends</span> <span class="type">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="string">&quot;dag-scheduler-event-loop&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>eventProcessLoop 是 EventLoop 类的唯一子类，eventProcessLoop 内部没有实现 post() 方法，直接调用 EventLoop 父类将事件添加到 eventQueue 阻塞队列中，此时事件生产过程已经结束。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/util/EventLoop.scala</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Put the event into the event queue. The event thread will process it later.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 阻塞队列添加事件</span></span><br><span class="line">    eventQueue.put(event)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> eventQueue: <span class="type">BlockingQueue</span>[<span class="type">E</span>] = <span class="keyword">new</span> <span class="type">LinkedBlockingDeque</span>[<span class="type">E</span>]()</span><br></pre></td></tr></table></figure>

<p>那消费者是如何来消费生产者生产的事件呢？在 DAGScheduler 初始化时有一段非常关键的代码。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">  eventProcessLoop.start()</span><br></pre></td></tr></table></figure>

<p>eventProcessLoop.start() 调用了 EventLoop 类的 start 方法，start() 方法内部启动一个 eventThread 独立线程，独立线程的作用就是死循环地消费前面 post 的 event 事件。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/util/EventLoop.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(name + <span class="string">&quot; has already been stopped&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Call onStart before starting the event thread to make sure it happens before onReceive</span></span><br><span class="line">    onStart()</span><br><span class="line">    <span class="comment">// start() 方法内部启动一个 eventThread 独立线程</span></span><br><span class="line">    eventThread.start()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> eventThread = <span class="keyword">new</span> <span class="type">Thread</span>(name) &#123;</span><br><span class="line">    setDaemon(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (!stopped.get) &#123;</span><br><span class="line">          <span class="comment">// 关键：从阻塞队列 eventQueue 中取出事件</span></span><br><span class="line">          <span class="keyword">val</span> event = eventQueue.take()</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 关键：具体处理事件的逻辑</span></span><br><span class="line">            onReceive(event)</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                onError(e)</span><br><span class="line">              &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// exit even if eventQueue is not empty</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>具体 evnet 的处理逻辑交给 onReceive(event) 方法，由于 EventLoop 内部没有实现该方法，并且只有一个子类 DAGSchedulerEventProcessLoop，那该类一定实现了对应的方法。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line"><span class="comment">// 内部类：DAGSchedulerEventProcessLoop</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// EventLoop 子类实现的 onReceive 方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> timerContext = timer.time()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      doOnReceive(event)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      timerContext.stop()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 根据 DAGSchedulerEvent 事件类型执行想要逻辑</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MapStageSubmitted</span>(jobId, dependency, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，DAGSchedulerEvent 事件的生产消费处理逻辑就介绍结束。</p>
<h3 id="2-3-2-SparkListenerEvent处理逻辑"><a href="#2-3-2-SparkListenerEvent处理逻辑" class="headerlink" title="2.3.2 SparkListenerEvent处理逻辑"></a>2.3.2 SparkListenerEvent处理逻辑</h3><p>SparkListenerEvent 贯穿 SparkListener 整个脉络，是 Spark 中非常重要的事件类型。它的实现于 DAGSchedulerEvent 实现方式不太一样，<strong>DAGSchedulerEvent 事件是一对一的调用，即一个事件交由一个 EventLoop 处理，而 SparkListenerEvent 事件是一对多的调用，即一个事件交由多个事件总线的多个 事件处理器处理。</strong>如下图，介绍了 SparkListenerEvent 事件的来源、LiveListenerBus 转发事件到四类事件总线，每类事件总线又将事件分发给总线上的所有 SparkListener，SparkListener 的注册逻辑在 2.2 小节中有介绍。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/28/e3ca4b2a6644c941bd6269f5026cb81d-1638101616428-c8329717-c941-43d5-8e6d-7ebbe3f832e5-ff7272.png" alt="img"></p>
<center>SparkListenerBus 工作流程图</center>

<p>前面提到 LiveListenerBus 类作为管理 SparkListenerBus 总线和分配事件到总线的枢纽，SparkListenerEvent 的生产当然绕不开它的存在。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line"> <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">     finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">     func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">     partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">     callSite: <span class="type">CallSite</span>,</span><br><span class="line">     listener: <span class="type">JobListener</span>,</span><br><span class="line">     properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">   ...</span><br><span class="line">   <span class="comment">// SparkListenerEvent 事件的生产，以 SparkListenerJobStart 事件为例</span></span><br><span class="line">   listenerBus.post(</span><br><span class="line">     <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</span><br><span class="line">   submitStage(finalStage)</span><br><span class="line"> &#125;   </span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">DAGScheduler</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">   private[scheduler] val sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">   private[scheduler] val taskScheduler: <span class="type">TaskScheduler</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">   // listenerBus 属于 <span class="type">LiveListenerBus</span></span></span></span><br><span class="line"><span class="params"><span class="class">   listenerBus: <span class="type">LiveListenerBus</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">   ...</span></span></span><br><span class="line"><span class="params"><span class="class">   &#125;</span></span></span><br></pre></td></tr></table></figure>

<p>经过 LiveListenerBus#post 事件后，这里和之前的事件生产逻辑不太一样，它并不是将事件添加到一个 EventQueue 中，而是通过 postToQueues() 方法向 queues 数组中分别 post 该事件。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala</span></span><br><span class="line">  <span class="comment">/** Post an event to all queues. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    metrics.numEventsPosted.inc()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断 event buffer 数组是否为空，大多数情况都是调用这里发送事件</span></span><br><span class="line">    <span class="keyword">if</span> (queuedEvents == <span class="literal">null</span>) &#123;</span><br><span class="line">      postToQueues(event)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 判读LiveListenerBus 是否已经启动</span></span><br><span class="line">    synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span> (!started.get()) &#123;</span><br><span class="line">        queuedEvents += event</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If the bus was already started when the check above was made, just post directly to the</span></span><br><span class="line">    <span class="comment">// queues.</span></span><br><span class="line">    postToQueues(event)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>postToQueues 方法是和之前生产消费模型不同的关键，添加了一个中间阶段，postToQueues 方法收到事件后将其分别发送给 queues 队列中存放的四类事件总线。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">postToQueues</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> it = queues.iterator()</span><br><span class="line">    <span class="comment">// 关键：向 queues 中的四类事件总线分别发送 event 事件</span></span><br><span class="line">    <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">      it.next().post(event)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 关键数据结构，存放前面介绍的四类事件总线，线程安全的 ArrayList 集合。</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> queues = <span class="keyword">new</span> <span class="type">CopyOnWriteArrayList</span>[<span class="type">AsyncEventQueue</span>]()</span><br></pre></td></tr></table></figure>

<p>将事件分别发送给 queues 的各种事件总线时，在此时又看到了熟悉生产消费模型，将事件添加到容量有限的阻塞队列中。</p>
<blockquote>
<p>留一个问题思考：</p>
<ul>
<li>为什么在处理 DAGSchedulerEvent 时的阻塞队列是无限容量的，而这里处理 SparkListenerEvent 的阻塞队列要设计成有限容量？</li>
</ul>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    eventCount.incrementAndGet()</span><br><span class="line">    <span class="comment">// 关键：将事件添加到容量有限的阻塞队列中</span></span><br><span class="line">    <span class="keyword">if</span> (eventQueue.offer(event)) &#123;</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 容量有限的阻塞队列</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> eventQueue = <span class="keyword">new</span> <span class="type">LinkedBlockingQueue</span>[<span class="type">SparkListenerEvent</span>](</span><br><span class="line">    conf.get(<span class="type">LISTENER_BUS_EVENT_QUEUE_CAPACITY</span>))</span><br></pre></td></tr></table></figure>

<p>同样的，生产者为四类四类事件总线分别生产了同一个事件，那四类事件总线又是如何把这些事件转发到对应的 SparkListener 处理呢？在 AsyncEventQueue 启动时，会启动一个独立的异步线程，用于将接收的事件转发到对应的 SparkListener 处理。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> dispatchThread = <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">s&quot;spark-listener-group-<span class="subst">$name</span>&quot;</span>) &#123;</span><br><span class="line">    setDaemon(<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryOrStopSparkContext(sc) &#123;</span><br><span class="line">      <span class="comment">// 四类事件总线都有这样的异步独立线程去转发接收到的事件</span></span><br><span class="line">      dispatch()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dispatch</span></span>(): <span class="type">Unit</span> = <span class="type">LiveListenerBus</span>.withinListenerThread.withValue(<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="comment">// 关键：从阻塞队列中取出事件</span></span><br><span class="line">    <span class="keyword">var</span> next: <span class="type">SparkListenerEvent</span> = eventQueue.take()</span><br><span class="line">    <span class="keyword">while</span> (next != <span class="type">POISON_PILL</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> ctx = processingTime.time()</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 关键：每一类 ListenerBus 都会将取出来饿事件发送给总线上注册过的所有SparkListener</span></span><br><span class="line">        <span class="keyword">super</span>.postToAll(next)</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        ctx.stop()</span><br><span class="line">      &#125;</span><br><span class="line">      eventCount.decrementAndGet()</span><br><span class="line">      next = eventQueue.take()</span><br><span class="line">    &#125;</span><br><span class="line">    eventCount.decrementAndGet()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>从阻塞队列中取出事件开始消费后，获取到之前注册到每个 ListenerBus 总线上的所有 SparkListener，然后让所有的 SparkListener 处理该 event 事件。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/util/ListenerBus.scala</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Post the event to all registered listeners. The `postToAll` caller should guarantee calling</span></span><br><span class="line"><span class="comment">   * `postToAll` in the same thread for all events.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">postToAll</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// JavaConverters can create a JIterableWrapper if we use asScala.</span></span><br><span class="line">    <span class="comment">// However, this method will be called frequently. To avoid the wrapper cost, here we use</span></span><br><span class="line">    <span class="comment">// Java Iterator directly.</span></span><br><span class="line">    <span class="keyword">val</span> iter = listenersPlusTimers.iterator</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> listenerAndMaybeTimer = iter.next()</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 获取到之前注册到对应总线上到 SparkListener</span></span><br><span class="line">      <span class="keyword">val</span> listener = listenerAndMaybeTimer._1</span><br><span class="line">      <span class="keyword">val</span> maybeTimer = listenerAndMaybeTimer._2</span><br><span class="line">      <span class="keyword">val</span> maybeTimerContext = <span class="keyword">if</span> (maybeTimer.isDefined) &#123;</span><br><span class="line">        maybeTimer.get.time()</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="literal">null</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// SparkListener 处理具体 event 事件</span></span><br><span class="line">        doPostEvent(listener, event)</span><br><span class="line">        <span class="keyword">if</span> (<span class="type">Thread</span>.interrupted()) &#123;</span><br><span class="line">          <span class="comment">// We want to throw the InterruptedException right away so we can associate the interrupt</span></span><br><span class="line">          <span class="comment">// with this listener, as opposed to waiting for a queue.take() etc. to detect it.</span></span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InterruptedException</span>()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; </span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>doPostEvent 就是根据 event 和 SparkListener 信息来执行具体的任务状态流转逻辑。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">SparkListenerBus</span></span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">ListenerBus</span>[<span class="type">SparkListenerInterface</span>, <span class="type">SparkListenerEvent</span>] &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 不同 SparkListenerEvent 的处理</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doPostEvent</span></span>(</span><br><span class="line">      listener: <span class="type">SparkListenerInterface</span>,</span><br><span class="line">      event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    event <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stageSubmitted: <span class="type">SparkListenerStageSubmitted</span> =&gt;</span><br><span class="line">        listener.onStageSubmitted(stageSubmitted)</span><br><span class="line">      <span class="keyword">case</span> stageCompleted: <span class="type">SparkListenerStageCompleted</span> =&gt;</span><br><span class="line">        listener.onStageCompleted(stageCompleted)</span><br><span class="line">      <span class="keyword">case</span> jobStart: <span class="type">SparkListenerJobStart</span> =&gt;</span><br><span class="line">        listener.onJobStart(jobStart)</span><br><span class="line">      <span class="keyword">case</span> jobEnd: <span class="type">SparkListenerJobEnd</span> =&gt;</span><br><span class="line">        listener.onJobEnd(jobEnd)</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此，SparkListenerEvent 事件的处理逻辑也介绍完了，比常规的生产者-消费者设计模型要稍微绕一些弯，不过还是挺有意思的。</p>
<blockquote>
<p>小结</p>
<p>YARN 和 Spark 事件驱动模型的一点体会：</p>
<ul>
<li>YARN 的设计思想是向事件总线中注册 &lt;事件，事件处理器&gt; 的映射关系，而 Spark 则是向事件总线上注册事件处理器。</li>
<li>YARN 中事件注册是一对一或者一对多模型，1:1 或者 1:N 模型，即一个事件对应一个或多个事件处理器，而 Spark 则是一对多模型，即一个事件会发送给所有事件总线，每一条事件总线又会将事件转发给注册到对应总线上的所有 SparkListener，这里也是一对多模型，所以 Spark 是 1:N:N 模型。</li>
</ul>
</blockquote>
<h1 id="3-Spark事件流转源码引读"><a href="#3-Spark事件流转源码引读" class="headerlink" title="3. Spark事件流转源码引读"></a>3. Spark事件流转源码引读</h1><p>DAGScheduler 作为 Job 提交的入口，通过提交 Job 触发 Job 的状态变化事件，最终以 SparkListenerEvent 事件通过 SparkListenerBus 转发给不同的 SparkListener 处理。</p>
<p>Tips: 这部分从源码角度介绍事件的流转过程，内容和 <code>Event处理逻辑</code> 小节介绍的存在一些重复，如果对于 Event 处理逻辑比较清楚，可以跳过这部分内容。</p>
<h2 id="DAGSchedulerEvent源码引读"><a href="#DAGSchedulerEvent源码引读" class="headerlink" title="DAGSchedulerEvent源码引读"></a>DAGSchedulerEvent源码引读</h2><p>以 Spark 作业提交为例，所有 Spark 作业提交的入口最终都会调用到 DAGScheduler#submitJob() 方法，并发送 JobSubmitted 事件，JobSubmitted 继承自 DAGSchedulerEvent 事件接口。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">      callSite: <span class="type">CallSite</span>,</span><br><span class="line">      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">      properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">		...</span><br><span class="line">    <span class="comment">// 发送 JobSubmitted 事件提交作业</span></span><br><span class="line">    eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">      <span class="type">SerializationUtils</span>.clone(properties)))</span><br><span class="line">    waiter</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JobSubmitted</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    jobId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    finalRDD: <span class="type">RDD</span>[_],</span></span></span><br><span class="line"><span class="params"><span class="class">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]</span>) <span class="title">=&gt;</span> <span class="title">_</span>,</span></span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span> = <span class="literal">null</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">DAGSchedulerEvent</span></span><br></pre></td></tr></table></figure>

<p>eventProcessLoop 是 EventLoop 类的唯一子类，eventProcessLoop 内部没有实现 post() 方法，直接调用 EventLoop 父类将事件添加到 eventQueue 阻塞队列中，此时生产者已生产好数据。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/util/EventLoop.scala</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Put the event into the event queue. The event thread will process it later.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 阻塞队列添加事件</span></span><br><span class="line">    eventQueue.put(event)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> eventQueue: <span class="type">BlockingQueue</span>[<span class="type">E</span>] = <span class="keyword">new</span> <span class="type">LinkedBlockingDeque</span>[<span class="type">E</span>]()</span><br></pre></td></tr></table></figure>

<p>那消费者是如何来消费生产者生产的事件呢？在 DAGScheduler 类初始化时有一段非常关键的代码。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">  eventProcessLoop.start()</span><br></pre></td></tr></table></figure>

<p>eventProcessLoop.start() 调用了 EventLoop 类的 start 方法，start() 方法内部启动一个 eventThread 独立线程，独立线程的作用就是死循环地取出前面 post 到阻塞队列中的 event 事件，然后交给 onReceive() 方法执行具体逻辑。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/util/EventLoop.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(name + <span class="string">&quot; has already been stopped&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Call onStart before starting the event thread to make sure it happens before onReceive</span></span><br><span class="line">    onStart()</span><br><span class="line">    <span class="comment">// start() 方法内部启动一个 eventThread 独立线程</span></span><br><span class="line">    eventThread.start()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> eventThread = <span class="keyword">new</span> <span class="type">Thread</span>(name) &#123;</span><br><span class="line">    setDaemon(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (!stopped.get) &#123;</span><br><span class="line">          <span class="comment">// 关键：从阻塞队列 eventQueue 中取出事件</span></span><br><span class="line">          <span class="keyword">val</span> event = eventQueue.take()</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 关键：具体处理事件的逻辑</span></span><br><span class="line">            onReceive(event)</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                onError(e)</span><br><span class="line">              &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// exit even if eventQueue is not empty</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>具体 evnet 的处理逻辑交给 onReceive(event) 方法，由于 EventLoop 内部没有实现该方法，并且只有唯一的子类 DAGSchedulerEventProcessLoop，那该类一定实现了对应的方法。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line"><span class="comment">// 内部类：DAGSchedulerEventProcessLoop</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// EventLoop 子类实现的 onReceive 方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> timerContext = timer.time()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      doOnReceive(event)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      timerContext.stop()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 根据 DAGSchedulerEvent 事件类型执行想要逻辑</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="comment">// 关键：根据前面提交的 JobSubmitted 事件执行具体的逻辑</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MapStageSubmitted</span>(jobId, dependency, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，DAGSchedulerEvent 的 JobSubmitted 事件的生产和消费流程已比较清晰，相对而已还是比较简单的。</p>
<h2 id="SparkListenerEvent源码引读"><a href="#SparkListenerEvent源码引读" class="headerlink" title="SparkListenerEvent源码引读"></a>SparkListenerEvent源码引读</h2><p>这里还是把 2.3.2 小节的图放这里，方便理解 SparkListenerEvent 事件的流转方式。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/11/28/e3ca4b2a6644c941bd6269f5026cb81d-1638101616428-c8329717-c941-43d5-8e6d-7ebbe3f832e5-20211128204724899-9a9e93.png" alt="img"></p>
<p>接着上面 DAGScheduler 类处理完 JobSubmitted 事件，在 handleJobSubmitted 处理事件逻辑中，最后会发送 SparkListenerJobStart 事件。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line"> <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">     finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">     func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">     partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">     callSite: <span class="type">CallSite</span>,</span><br><span class="line">     listener: <span class="type">JobListener</span>,</span><br><span class="line">     properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">   ...</span><br><span class="line">   <span class="comment">// SparkListenerEvent 事件的生产，以 SparkListenerJobStart 事件为例</span></span><br><span class="line">   listenerBus.post(</span><br><span class="line">     <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</span><br><span class="line">   submitStage(finalStage)</span><br><span class="line"> &#125;   </span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">DAGScheduler</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">   private[scheduler] val sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">   private[scheduler] val taskScheduler: <span class="type">TaskScheduler</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">   // listenerBus 属于 <span class="type">LiveListenerBus</span></span></span></span><br><span class="line"><span class="params"><span class="class">   listenerBus: <span class="type">LiveListenerBus</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">   ...</span></span></span><br><span class="line"><span class="params"><span class="class">   &#125;</span></span></span><br></pre></td></tr></table></figure>

<p>经过 LiveListenerBus#post 事件后，这里和之前的事件生产者逻辑不太一样，它并不是将事件添加到一个 EventQueue 中，而是通过 postToQueues() 方法向 queues 数组中分别 post 该事件。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala</span></span><br><span class="line">  <span class="comment">/** Post an event to all queues. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    metrics.numEventsPosted.inc()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断 event buffer 数组是否为空，大多数情况都是调用这里发送事件</span></span><br><span class="line">    <span class="keyword">if</span> (queuedEvents == <span class="literal">null</span>) &#123;</span><br><span class="line">      postToQueues(event)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 判读LiveListenerBus 是否已经启动</span></span><br><span class="line">    synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span> (!started.get()) &#123;</span><br><span class="line">        queuedEvents += event</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If the bus was already started when the check above was made, just post directly to the</span></span><br><span class="line">    <span class="comment">// queues.</span></span><br><span class="line">    postToQueues(event)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>postToQueues 方法是和之前生产消费模型不同的关键，添加了一个中间阶段，postToQueues 方法收到事件后将其分别发送给 queues 队列中存放的四类事件总线。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">postToQueues</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> it = queues.iterator()</span><br><span class="line">    <span class="comment">// 关键：向 queues 中的四类事件总线分别发送 event 事件</span></span><br><span class="line">    <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">      it.next().post(event)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 关键数据结构，存放前面介绍的四类事件总线，线程安全的 ArrayList 集合。</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> queues = <span class="keyword">new</span> <span class="type">CopyOnWriteArrayList</span>[<span class="type">AsyncEventQueue</span>]()</span><br></pre></td></tr></table></figure>

<p>将事件分别发送给 queues 的各种事件总线时，在此时又看到了熟悉生产消费模型，将事件添加到容量有限的阻塞队列中。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    eventCount.incrementAndGet()</span><br><span class="line">    <span class="comment">// 关键：将事件添加到容量有限的阻塞队列中</span></span><br><span class="line">    <span class="keyword">if</span> (eventQueue.offer(event)) &#123;</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 容量有限的阻塞队列</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> eventQueue = <span class="keyword">new</span> <span class="type">LinkedBlockingQueue</span>[<span class="type">SparkListenerEvent</span>](</span><br><span class="line">    conf.get(<span class="type">LISTENER_BUS_EVENT_QUEUE_CAPACITY</span>))</span><br></pre></td></tr></table></figure>

<p>同样的，生产者为四类四类事件总线分别生产了同一个事件，那四类事件总线又是如何把这些事件转发到对应的 SparkListener 处理呢？在 AsyncEventQueue 启动时，会启动一个独立的异步线程，用于将接收的事件转发到对应的 SparkListener 处理。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> dispatchThread = <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">s&quot;spark-listener-group-<span class="subst">$name</span>&quot;</span>) &#123;</span><br><span class="line">    setDaemon(<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryOrStopSparkContext(sc) &#123;</span><br><span class="line">      <span class="comment">// 四类事件总线都有这样的异步独立线程去转发接收到的事件</span></span><br><span class="line">      dispatch()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dispatch</span></span>(): <span class="type">Unit</span> = <span class="type">LiveListenerBus</span>.withinListenerThread.withValue(<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="comment">// 关键：从阻塞队列中取出事件</span></span><br><span class="line">    <span class="keyword">var</span> next: <span class="type">SparkListenerEvent</span> = eventQueue.take()</span><br><span class="line">    <span class="keyword">while</span> (next != <span class="type">POISON_PILL</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> ctx = processingTime.time()</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 关键：每一类 ListenerBus 都会将取出来饿事件发送给总线上注册过的所有SparkListener</span></span><br><span class="line">        <span class="keyword">super</span>.postToAll(next)</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        ctx.stop()</span><br><span class="line">      &#125;</span><br><span class="line">      eventCount.decrementAndGet()</span><br><span class="line">      next = eventQueue.take()</span><br><span class="line">    &#125;</span><br><span class="line">    eventCount.decrementAndGet()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>从阻塞队列中取出事件开始消费后，获取到之前注册到每个 ListenerBus 总线上的所有 SparkListener，然后让所有的 SparkListener 处理该 event 事件。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/util/ListenerBus.scala</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Post the event to all registered listeners. The `postToAll` caller should guarantee calling</span></span><br><span class="line"><span class="comment">   * `postToAll` in the same thread for all events.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">postToAll</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// JavaConverters can create a JIterableWrapper if we use asScala.</span></span><br><span class="line">    <span class="comment">// However, this method will be called frequently. To avoid the wrapper cost, here we use</span></span><br><span class="line">    <span class="comment">// Java Iterator directly.</span></span><br><span class="line">    <span class="keyword">val</span> iter = listenersPlusTimers.iterator</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> listenerAndMaybeTimer = iter.next()</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 获取到之前注册到对应总线上到 SparkListener</span></span><br><span class="line">      <span class="keyword">val</span> listener = listenerAndMaybeTimer._1</span><br><span class="line">      <span class="keyword">val</span> maybeTimer = listenerAndMaybeTimer._2</span><br><span class="line">      <span class="keyword">val</span> maybeTimerContext = <span class="keyword">if</span> (maybeTimer.isDefined) &#123;</span><br><span class="line">        maybeTimer.get.time()</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="literal">null</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// SparkListener 处理具体 event 事件</span></span><br><span class="line">        doPostEvent(listener, event)</span><br><span class="line">        <span class="keyword">if</span> (<span class="type">Thread</span>.interrupted()) &#123;</span><br><span class="line">          <span class="comment">// We want to throw the InterruptedException right away so we can associate the interrupt</span></span><br><span class="line">          <span class="comment">// with this listener, as opposed to waiting for a queue.take() etc. to detect it.</span></span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InterruptedException</span>()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; </span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>doPostEvent 就是根据 event 和 SparkListener 信息来执行具体的任务状态流转逻辑，根据继承关系交给对应的 SparkListener 处理。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">SparkListenerBus</span></span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">ListenerBus</span>[<span class="type">SparkListenerInterface</span>, <span class="type">SparkListenerEvent</span>] &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 不同 SparkListenerEvent 的处理</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doPostEvent</span></span>(</span><br><span class="line">      listener: <span class="type">SparkListenerInterface</span>,</span><br><span class="line">      event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    event <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stageSubmitted: <span class="type">SparkListenerStageSubmitted</span> =&gt;</span><br><span class="line">        listener.onStageSubmitted(stageSubmitted)</span><br><span class="line">      <span class="keyword">case</span> stageCompleted: <span class="type">SparkListenerStageCompleted</span> =&gt;</span><br><span class="line">        listener.onStageCompleted(stageCompleted)</span><br><span class="line">      <span class="keyword">case</span> jobStart: <span class="type">SparkListenerJobStart</span> =&gt;</span><br><span class="line">        listener.onJobStart(jobStart)</span><br><span class="line">      <span class="keyword">case</span> jobEnd: <span class="type">SparkListenerJobEnd</span> =&gt;</span><br><span class="line">        listener.onJobEnd(jobEnd)</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此，SparkListenerEvent 事件的处理逻辑也介绍完了，比常规的生产者-消费者设计模型要稍微绕一些弯，不过还是挺有意思的。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li>耿嘉安，《Spark内核设计的艺术：架构设计与实现》</li>
<li><a href="https://blog.csdn.net/GYHYCX/article/details/118727960">死磕 Spark 事件总线——聊聊 Spark 中事件监听是如何实现的</a></li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>Spark源码</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark RDD持久化机制及源码分析</title>
    <url>/2021/12/05/Spark-RDD%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<blockquote>
<p>导读：Spark 中一个很重要的能力是将数据持久化（或称为缓存），缓存是迭代算法和快速的交互式使用的重要工具，在多个操作间都可以访问这些持久化的数据，当持久化一个 RDD 时，每个节点的其它分区都可以直接使用该缓存的 RDD， 这样可以加快其他操作的计算速度。</p>
</blockquote>
<h1 id="1-RDD-持久化"><a href="#1-RDD-持久化" class="headerlink" title="1. RDD 持久化"></a>1. RDD 持久化</h1><p>因为 Spark 程序执行的特性，即延迟执行和基于 Lineage 最大化的 pipeline，当 Spark 中由于对某个 RDD 的 Action 操作触发了作业时，会基于 Lineage 从后往前推，找到该 RDD 的源头 RDD，然后从前往后计算出结果。</p>
<p>很明显，如果对某个 RDD 执行了多次 Transformation 和 Action 操作，每次 Action 操作出发了作业时都会重新从源头 RDD 出计算一遍来获得 RDD，再对这个 RDD 执行相应的操作。当 RDD 本身计算特别复杂和耗时时，这种方式性能是非常差的，此时必须考虑对计算结果的数据进行持久化。</p>
<p>数据持久化（或称为缓存）就是将计算出来的 RDD 根据配置的持久化级别，保存在内存或磁盘中，以后每次对该 RDD 进行算子操作时，都会直接从内存或者磁盘中提取持久化的 RDD 数据，然后执行算子操作，而不会从源头处重新计算一遍该 RDD。</p>
<h1 id="2-RDD-持久化级别"><a href="#2-RDD-持久化级别" class="headerlink" title="2. RDD 持久化级别"></a>2. RDD 持久化级别</h1><p>Spark 的持久化级别有如下几种：</p>
<table>
<thead>
<tr>
<th>持久化级别</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>将 RDD 数据保存在内存中，如果内存不够存放则数据有可能有部分不会进行持久化，在瑕疵对该 RDD 执行算子操作时，没有被持久化的数据需要从源头重新计算一遍。这是默认的持久化策略，使用 cache() 方法使用的就是这种持久化策略。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>优先尝试将数据保存在内存中，如果内存不够存放所有数据，会将数据写入到磁盘文件中，下次对该 RDD 执行算子操作时，持久化在内存和磁盘文件中的数据会被读取出来使用。</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>基本含义同 MEMORY_ONLY。唯一的区别是会将 RDD 数据进行序列化，将 RDD 的每个 partition 序列化为一个字节数据，这种方式更加节省内存，避免持久化的数据占用过多内存导致频繁 GC。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>基本含义同 MEMORY_AND_DISK。唯一的区别是会将 RDD 数据进行序列化， RDD 的每个 partition 会被序列化成一个字节数据，种方式更加节省内存，避免持久化的数据占用过多内存导致频繁 GC。</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>使用未序列化的 Java 对象格式，将数据全部写入磁盘文件中。</td>
</tr>
<tr>
<td>OFF_HEAP</td>
<td>将 RDD 以序列化的方式存在在 Alluxio 中（曾用名 Tachyon），而不是 Executor 的内存中，减少垃圾回收的压力，当 RDD 存储在 Alluxio 上时，Executors 的崩溃不会造成 RDD 数据的丢失。</td>
</tr>
<tr>
<td>MEMORY_ONLY_2, MEMORY_ONLY_SER_2, MEMORY_AND_DISK_2, MEMORY_AND_DIST_SER_2, DISK_ONLY_2, 等等</td>
<td>后缀有_2的持久化策略，表示将每个持久化的数据都复制一个副本，并保存在其他节点上，这种基于副本的持久化机制主要用于容错，如果单个节点持久化的数据丢失了，还可以从其他节点的副本上获取该 RDD 数据。</td>
</tr>
</tbody></table>
<p>阅读 Spark 2.1.0 源码，RDD 持久化级别在 StorageLevel 类中有详细介绍，我们来详细看看。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：/org/apache/spark/storage/StorageLevel.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Various [[org.apache.spark.storage.StorageLevel]] defined and utility functions for creating</span></span><br><span class="line"><span class="comment">* new storage levels.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>这里列出了 12 种 RDD 缓存级别，每个缓存级别后面都 new 了一个 StorageLevel类的构造函数，什么意思呢？我么可以看看其构造函数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/storage/StorageLevel.scala</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">  private var _useDisk: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  private var _useMemory: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  private var _useOffHeap: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  private var _deserialized: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  private var _replication: <span class="type">Int</span> = 1</span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Externalizable</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// <span class="doctag">TODO:</span> Also add fields for caching priority, dataset ID, and flushing.</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(flags: <span class="type">Int</span>, replication: <span class="type">Int</span>) &#123;</span><br><span class="line">	<span class="keyword">this</span>((flags &amp; <span class="number">8</span>) != <span class="number">0</span>, (flags &amp; <span class="number">4</span>) != <span class="number">0</span>, (flags &amp; <span class="number">2</span>) != <span class="number">0</span>, (flags &amp; <span class="number">1</span>) != <span class="number">0</span>, replication)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>() = <span class="keyword">this</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>) <span class="comment">// For deserialization</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">useDisk</span></span>: <span class="type">Boolean</span> = _useDisk</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">useMemory</span></span>: <span class="type">Boolean</span> = _useMemory</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">useOffHeap</span></span>: <span class="type">Boolean</span> = _useOffHeap</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deserialized</span></span>: <span class="type">Boolean</span> = _deserialized</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replication</span></span>: <span class="type">Int</span> = _replication</span><br></pre></td></tr></table></figure>

<p>StorageLevel类的主构造器包含了5个参数：</p>
<ul>
<li><p>useDisk：使用硬盘（外存）。</p>
</li>
<li><p>useMemory：使用内存。</p>
</li>
<li><p>useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</p>
</li>
<li><p>deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象。</p>
</li>
<li><p>replication：备份数（在多个节点上备份）。</p>
</li>
</ul>
<p>理解了这 5 个参数，就不难理解不同缓存级别的含义了，比如 <code>val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</code> 缓存，表示将 RDD 的数据持久化在硬盘以及内存中，对数据进行序列化存储，并且将每个持久化的数据都复制一份副本保存到其他节点。</p>
<h1 id="3-持久化级别选择"><a href="#3-持久化级别选择" class="headerlink" title="3. 持久化级别选择"></a>3. 持久化级别选择</h1><p>Spark 提供了这么多中持久化策略，那在实际场景中应该如何使用呢？通常遵循的准则是，优先考虑内存，内存放不下就考虑序列化后放到内存中，尽量不要存储到磁盘中，因为一般 RDD 的重新计算要比从磁盘中读取更快，只有在需要更快的恢复时才使用备份级别（所有的存储级别都可以通过重新计算来提供全面的容错性，但是备份级别允许用于在 RDD 的备份上执行任务，而无须重新计算丢失的分区）。具体的选取方式如下：</p>
<ul>
<li><p>采用默认情况下性能最高的 MEMORY_ONLY。该持续化级别下，对 RDD 的后续算子操作，都是基于纯内存中的数据操作，不需要从磁盘文件中读取数据，性能很高，也不需要进行序列化与反序列化操作，避免了这部分开销。但要注意的是，如果 RDD 的数据比较多（比如几十亿条），直接用这种持久化级别可能会导致 JVM 的 OOM 内存溢出异常。</p>
</li>
<li><p>如果使用 MEMROY_ONLY 级别发生了内存溢出，建议尝试使用 MEMROY_ONLY_SER 级别。该级别会将 RDD 数据序列化后再保存到内存中，此时每个 partition 仅仅是一个字节数组，减少了对象数量，并降低内存占用，该级别比 MEMORY_ONLY 多出来的性能开销，主要就是序列化和反序列化的开销。同样，如果 RDD 数据过多仍然会导致 OOM 内存溢出异常。</p>
</li>
<li><p>如果纯内存级别都无法使用，则建议使用 MEMROY_AND_DISK_SER 策略，而不是 MEMORY_AND_DISK 策略。该策略会优先将数据缓存到内存中，内存缓存不下时才会写入磁盘。</p>
</li>
<li><p>通常不建议使用 DISK_ONLY 级别。因为完全基于磁盘文件进行数据的读写，会导致性能急剧下降，有时还不如重新计算一次该 RDD。</p>
</li>
<li><p>通常不建议使用后缀为_2的备份级别。因为该级别必须将所有的数据都复制一份副本，并发送到其他节点上，而数据复制和网络传输会导致较大的性能开销。除非是作业的高可用性能要求很高，否则不建议使用。</p>
</li>
<li><p>OFF_HEAP 级别一般使用得少，但优势也比较明显。该方式将 RDD 数据持久化到 Alluxio 而不是 Executor 内存中，可以避免 Executors 崩溃缓存数据丢失的情况。</p>
</li>
</ul>
<h1 id="4-删除持久化数据"><a href="#4-删除持久化数据" class="headerlink" title="4. 删除持久化数据"></a>4. 删除持久化数据</h1><p>Spark 的机制可以自动监控各个节点上的缓存使用率，并以 LRU (Least Recently Used，近期最少使用）算法删除过时的缓存数据。当然，如果想手动删除一个 RDD 数据的缓存，而不是等待该 RDD 被 Spark 自动移除，可以使用 <code>RDD.unpersist()</code>方法。</p>
<h1 id="5-RDD-cache和persist"><a href="#5-RDD-cache和persist" class="headerlink" title="5. RDD cache和persist"></a>5. RDD cache和persist</h1><p>我们先来看一个 persist 的示例。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// cache 使用示例：</span></span><br><span class="line">val rdd1 = sc.textFile(<span class="string">&quot;hdfs://nameservice/data/README.md&quot;</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist 使用示例</span></span><br><span class="line">val rdd1 = sc.textFile(<span class="string">&quot;hdfs://nameservice/data/README.md&quot;</span>).persist(StorageLevel.MEMORY_AND_DISK_SER)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure>

<p>唯一可以看出的是 persist() 持久化是可以手动指定持久化类型的，而 cache() 无须指定。那它们之间到底有什么区别呢？</p>
<p>通过阅读 Spark 2.1.0 源码，可以看到 cache() 方法调用了无参的 persist() 方法。想知道两者的区别，还需要进一步看看 persist() 方法逻辑。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/rdd/RDD.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Persist this RDD with the default storage level (`MEMORY_ONLY`).</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br></pre></td></tr></table></figure>

<p>可以看到 persist() 方法调用了 persist(StorageLevel.MEMORY_ONLY) 方法，即默认缓存方式采用 MEMORY_ONLY 级别。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/rdd/RDD.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Persist this RDD with the default storage level (`MEMORY_ONLY`).</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure>

<p>继续往下看，persist() 方法有一个 StorageLevel 类型的参数，该参数表示 RDD 的缓存级别。至此，也就能看出 cache 和 persist 的区别了：即 cache 只有一个默认的缓存级别 MEMORY_ONLY，而 persist 可以根据情况设置其他的缓存级别。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/rdd/RDD.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Mark this RDD for persisting using the specified level.</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* @param newLevel the target storage level</span></span><br><span class="line"><span class="comment">* @param allowOverride whether to override any existing level with the new one</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>, allowOverride: <span class="type">Boolean</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line"><span class="comment">// <span class="doctag">TODO:</span> Handle changes of StorageLevel</span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span> &amp;&amp; newLevel != storageLevel &amp;&amp; !allowOverride) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(</span><br><span class="line">    <span class="string">&quot;Cannot change storage level of an RDD after it was already assigned a level&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// If this is the first time this RDD is marked for persisting, register it</span></span><br><span class="line">  <span class="comment">// with the SparkContext for cleanups and accounting. Do this only once.</span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">    sc.cleaner.foreach(_.registerRDDForCleanup(<span class="keyword">this</span>))</span><br><span class="line">    sc.persistRDD(<span class="keyword">this</span>)</span><br><span class="line">  &#125;</span><br><span class="line">	storageLevel = newLevel</span><br><span class="line">	<span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="6-RDD-的-checkpoint"><a href="#6-RDD-的-checkpoint" class="headerlink" title="6. RDD 的 checkpoint"></a>6. RDD 的 checkpoint</h1><p>把数据通过 cache 或 persist 持久化到内存或磁盘中，虽然是快速的但却不是最可靠的，checkpoint 机制的产生就是为了更加可靠地持久化数据以复用 RDD 计算数据，通常针对整个 RDD 计算链路中特别需要数据持久化的缓解，启用 checkpoint 机制来确保高容错和高可用性。</p>
<p>可以通过调用 SparkContext.setCheckpointDir() 方法来指定 checkpoint 是持久化的 RDD 数据的存放位置，这里可以存在本地或 HDFS 中（生产环境通常是放在 HDFS 上，借助 HDFS 本身的高容错和高可靠的特性完成数据的持久化），同时为了提高效率，可以指定多个目录。</p>
<p>需要说明的是，<strong>checkpoint 和 persist 一样是惰性执行的，在对某个 RDD 标记了需要 checkpoint 后，并不会立即执行，只有在后续有 Action 触发 Job 从而导致该 RDD 的计算，且在这个 Job 执行完成后，才会从后往前回溯找到标记了 checkpoint 的 RDD，然后重新启动一个 Job 来执行具体的 checkpoint 操作，所以一般都会对需要进行 checkpoint 的 RDD 先进行 persist 标记，从而把该 RDD 的计算结果持久化到内存或者磁盘上，以备 checkpoint 复用</strong>。</p>
<p>下面是 checkpoint 使用的一个示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 配置 checkpointDir</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">&quot;hdfs://nameservice/spark/checkpoint&quot;</span>)</span><br><span class="line">val rdd1 = sc.textFile(<span class="string">&quot;hdfs://nameservice/data/README.md&quot;</span>).cache()</span><br><span class="line"><span class="comment">// 对 rdd1 标记 checkpoint</span></span><br><span class="line">rdd1.checkpoint()</span><br><span class="line"><span class="comment">// action 触发了 Job 才能导致 checkpoint 的真正执行</span></span><br><span class="line">rdd1.count()</span><br></pre></td></tr></table></figure>

<h1 id="7-DataSet-的cache和persist"><a href="#7-DataSet-的cache和persist" class="headerlink" title="7. DataSet 的cache和persist"></a>7. DataSet 的cache和persist</h1><p>阅读源码中无意中看到 DataSet 也支持 cache 和 persist 持久化方式，和 RDD 的持久化还是不太一样，我们来看看代码。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/Dataset.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* @group basic</span></span><br><span class="line"><span class="comment">* @since 1.6.0</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// DataSet 的 cache 持久化调用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* @group basic</span></span><br><span class="line"><span class="comment">* @since 1.6.0</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.cacheQuery(<span class="keyword">this</span>)</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 位置：org/apache/spark/sql/Dataset.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Persist this Dataset with the given storage level.</span></span><br><span class="line"><span class="comment">* @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,</span></span><br><span class="line"><span class="comment">* `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,</span></span><br><span class="line"><span class="comment">* `MEMORY_AND_DISK_2`, etc.</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* @group basic</span></span><br><span class="line"><span class="comment">* @since 1.6.0</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// DataSet 的 persist 持久化调用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.cacheQuery(<span class="keyword">this</span>, <span class="type">None</span>, newLevel)</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/CacheManager.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Caches the data produced by the logical representation of the given [[Dataset]].</span></span><br><span class="line"><span class="comment">* Unlike `RDD.cache()`, the default storage level is set to be `MEMORY_AND_DISK` because</span></span><br><span class="line"><span class="comment">* recomputing the in-memory columnar representation of the underlying table is expensive.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// cache 和 persist 持久化调用的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cacheQuery</span></span>(</span><br><span class="line">  query: <span class="type">Dataset</span>[_],</span><br><span class="line">  tableName: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span>,</span><br><span class="line">  storageLevel: <span class="type">StorageLevel</span> = <span class="type">MEMORY_AND_DISK</span>): <span class="type">Unit</span> = writeLock</span><br></pre></td></tr></table></figure>

<p>通过源码看到 cache() 调用的是无参的 persist() 方法，而 persist 调用 cacheQuery 方法，虽然 cache 和 persist 两者最终调用都是 cacheQuery 方法，但 cache 是采用默认的持久化级别 <code>MEMORY_ADN_DISK</code>，而 persist 则是用户自定义，这里默认的持久化持久和 RDD 是不一样的。</p>
<h1 id="参考连接"><a href="#参考连接" class="headerlink" title="参考连接"></a>参考连接</h1><ol>
<li><p><a href="https://blog.csdn.net/houmou/article/details/52491419">每次进步一点点——spark中cache和persist的区别</a></p>
</li>
<li><p><a href="https://dongkelun.com/2018/06/03/sparkCacheAndPersist/">Spark 持久化（cache和persist的区别）</a></p>
</li>
<li><p><a href="https://blog.csdn.net/qq_27639777/article/details/82319560">Spark Cache的几点思考</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark源码</tag>
        <tag>Spark原理</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark内存模型详解</title>
    <url>/2021/12/10/Spark%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<blockquote>
<p>导读：Spark 能够有效的利用内存并进行分布式计算，其内存管理模块在整个系统中扮演着非常重要的角色。为了更好地理解 Spark 作业的内存分布，下文主要介绍的内存模型指 Executor 端的内存模型，Driver 端的内存模型本文不做介绍，理解 Spark 内存使用有助于对问题内存使用进行分析，在出现各种内存问题时，能够找到哪块内存区域出现问题。</p>
</blockquote>
<h1 id="1-Spark-内存介绍"><a href="#1-Spark-内存介绍" class="headerlink" title="1. Spark 内存介绍"></a>1. Spark 内存介绍</h1><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种JVM进程。</p>
<p>Driver 程序主要负责：</p>
<ul>
<li><p>创建 Spark上下文；</p>
</li>
<li><p>提交 Spark作业（Job）并将 Job 转化为计算任务（Task）交给 Executor 计算；</p>
</li>
<li><p>协调各个 Executor 进程间任务调度。</p>
</li>
</ul>
<p>Executor 程序主要负责：</p>
<ul>
<li>在工作节点上执行具体的计算任务（Task），并将计算结果返回给 Driver；</li>
<li>为需要持久化的 RDD 提供存储功能。</li>
</ul>
<p>由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p>
<h1 id="2-Executor内存管理"><a href="#2-Executor内存管理" class="headerlink" title="2. Executor内存管理"></a><strong>2. Executor内存管理</strong></h1><p>Executor 进程作为一个 JVM 进程，其内存管理建立在 JVM 的内存管理之上，整个大致包含两种方式：堆内内存和堆外内存。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/0ceb92f23876fe378a6b9893fa2bf7c3-1630393018587-a690c541-f7ea-48e2-9fa7-caccae97db08-a2b138.png" alt="img"></p>
<center>堆内和堆外内存示意图</center>

<p><strong>一个 Executor 当中的所有 Task 是共享堆内内存的。一个 Work 中的多个 Executor 中的多个 Task 是共享堆外内存的。</strong>注意：默认的 Spark 是开启堆内内存的，配置参数：–executor-memory</p>
<p>但是默认的堆外内存是不开启的：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.memory.offHeap.enabled=true  # 开启堆外内存</span><br><span class="line">spark.memory.offHeap.size =1024    # 分配堆外内存的大小。</span><br></pre></td></tr></table></figure>

<h2 id="2-1-堆内内存"><a href="#2-1-堆内内存" class="headerlink" title="2.1 堆内内存"></a>2.1 堆内内存</h2><p>堆内内存大小，是指 JVM 堆的内存大小，由 Spark 程序启动时的 -executor-memory 或 spark.executor.memory 参数配置。那 Spark 是如何管理堆内内存呢？Spark 对堆内内存的管理是一种逻辑上的“规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存，我们来看看申请内存和释放内存的具体流程：</p>
<p><strong>申请内存：</strong></p>
<ol>
<li>Spark 在代码中 new 一个实例对象。</li>
<li>JVM 从堆内内存分配空间，创建对象并返回对象饮用。</li>
<li>Spark 保存该对象的引用，记录该对象占用的内存。</li>
</ol>
<p><strong>释放内存：</strong></p>
<ol>
<li>Spark 记录该对象释放的内存，删除该对象的引用。</li>
<li>等待 JVM 的垃圾回收机制释放该对象占用的堆内内存。</li>
</ol>
<p>Spark 内存管理分为静态内存管理和统一内存管理，在 Spark 1.6 之前是采用的静态内存，之后的版本都是采用统一内存管理，与静态内存管理的区别在于 Storeage 内存和 Execution 内存共享统一块空间，可以动态占用对方的空闲区域。</p>
<p>本文主要介绍统一内存管理。如图，可以看出堆内内存大致可以分为以下四个模块：<br><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/67dbf148dfcbac58f8c2387425926e06-1630393086200-dd0e1169-90ab-4144-aa6a-23a110f4dbe4-20211219185044179-c9d82b.png" alt="img"></p>
<center>统一内存管理图示—堆内</center>

<ul>
<li><p><strong>Storage 内存：</strong>主要用于存储 Spark 的 cache 数据，例如 RDD 的 cache，Broadcast 变量，Unroll 数据等。需要注意的是，unrolled 的数据如果内存不够，会存储在 driver 端。</p>
</li>
<li><p><strong>Execution 内存：</strong>用于存储 Spark task 执行过程中需要的对象，如 Shuffle、Join、Sort、Aggregation等计算过程中的临时数据。</p>
</li>
<li><p><strong>User 内存：</strong>分配 Spark Memory 剩余的内存，用户可以根据需要使用，可以存储 RDD transformations 需要的数据结构。</p>
</li>
<li><p><strong>Reserved 内存：</strong>这部分内存是预留给系统用的，固定不变。</p>
</li>
</ul>
<h2 id="2-2-堆外内存"><a href="#2-2-堆外内存" class="headerlink" title="2.2 堆外内存"></a>2.2 堆外内存</h2><p>在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同如下图3所示(以统一内存管理机制为例)，所有运行中的并发任务共享存储内存和执行内存。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/e4d5ae88116aa71d3485c29ea320803e-1630393132830-bdb0abd2-53f1-4fe4-8f85-cf5345243e7f-c20b06.png" alt="img"></p>
<center>统一内存管理图示—堆外</center>

<h1 id="3-统一内存动态占用机制"><a href="#3-统一内存动态占用机制" class="headerlink" title="3. 统一内存动态占用机制"></a>3. 统一内存动态占用机制</h1><p>Spark 1.6 之后引入的统一内存管理机制，存储内存（Storeage Memory）和执行内存（Execution Memory）可以动态占用对方的空闲区域（如下图），在设定基本的的存储内存和执行内存区域（由 spark.memory.storageFraction 参数控制）后，便确定了双方各自拥有的空间容量。统一内存管理的核心在内存区域的动态占用机制，其占用规则如下：</p>
<ol>
<li><p>双方空间都不足时，则存储到硬盘；如己方空间不足而对方空余时，可借用对方的空间；（存储空间不足是指不足以放下一个完整的 Block）。</p>
</li>
<li><p>执行内存的空间被对方占用后，可让对方将占用的部分存储转存到硬盘，然后“归还”借用的空间。</p>
</li>
<li><p>存储内存的空间被对方占用后，无法让对方“归还”，因为需要考虑到 Shuffle 过程中很多因素，实现起来较为复杂。</p>
</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/dcd02d6e382b8a11f227f3e3c7323846-1630393153185-c1fa0f82-ec1c-469c-8c76-e607ece26961-20211219185121310-e4684b.png" alt="img"></p>
<center>动态占用机制示意图</center>

<h1 id="4-Spark内存参数"><a href="#4-Spark内存参数" class="headerlink" title="4. Spark内存参数"></a>4. Spark内存参数</h1><h2 id="4-1-统一内存管理参数"><a href="#4-1-统一内存管理参数" class="headerlink" title="4.1 统一内存管理参数"></a>4.1 统一内存管理参数</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">参数：spark.memory.fraction</span><br><span class="line">含义：用于执行和存储的部分（堆空间-300MB）。值越低，溢出和缓存数据逐出的频率就越高。此配置的目的是为稀疏的，异常大的记录留出用于内部元数据，用户数据结构和大小估计不精确的内存。建议将其保留为默认值。</span><br><span class="line">默认值：0.6 (Spark 1.6 默认为 0.75，Spark 2.0+ 默认为 0.6)</span><br><span class="line"></span><br><span class="line">参数：spark.memory.storageFraction</span><br><span class="line">含义：可以逐出的存储内存量，以spark.memory.fraction预留的区域大小的一部分表示。数值越高，则可用于执行的工作内存就越少，任务可能会更频繁地溢出到磁盘上. 建议将其保留为默认值。</span><br><span class="line">默认值：0.5</span><br><span class="line"></span><br><span class="line">参数：spark.memory.offHeap.enabled</span><br><span class="line">含义：如果为true，Spark将尝试将堆外内存用于某些操作。如果启用了堆外内存使用，则spark.memory.offHeap.size必须为正。</span><br><span class="line">默认值：false</span><br><span class="line"></span><br><span class="line">参数：spark.memory.offHeap.size</span><br><span class="line">含义：可用于堆外分配的绝对内存量（以字节为单位）。此设置对堆内存使用没有影响，因此，如果执行者的总内存消耗必须在某个硬限制内，那么请确保相应地缩小JVM堆大小。当spark.memory.offHeap.enabled=true时，必须将此值设置为正值.</span><br><span class="line">默认值：0</span><br><span class="line"></span><br><span class="line">参数：spark.memory.useLegacyMode </span><br><span class="line">含义：是否启用Spark 1.5及之前版本中使用的旧版内存管理模式。传统模式将堆空间严格划分为固定大小的区域，如果未调整应用程序，则可能导致过多的溢出. 除非已启用，否则不会读取以下不推荐使用的内存分数配置： spark.shuffle.memoryFraction、spark.storage.memoryFraction、spark.storage.unrollFraction</span><br><span class="line">默认值：false</span><br><span class="line"></span><br><span class="line">参数：spark.shuffle.memoryFraction</span><br><span class="line">含义：（不建议使用）仅在启用spark.memory.useLegacyMode读。在 shuffle 期间用于聚合和协同分组的Java堆的分数。在任何给定时间，用于随机播放的所有内存映射的集合大小都受到此限制的限制，超出此限制，内容将开始溢出到磁盘。如果经常发生泄漏，请考虑以spark.storage.memoryFraction为代价增加此值.</span><br><span class="line">默认值：0.2</span><br><span class="line"></span><br><span class="line">参数：spark.storage.memoryFraction</span><br><span class="line">含义：（不建议使用）仅在启用spark.memory.useLegacyMode读。用于Spark的内存缓存的Java堆的分数。这不应大于JVM中对象的&quot;旧&quot;代，默认情况下，该对象的堆大小为0.6，但是如果您配置自己的旧代大小，则可以增加它。</span><br><span class="line">默认值：0.6</span><br><span class="line"></span><br><span class="line">参数：spark.storage.unrollFraction</span><br><span class="line">含义：（不建议使用）仅在启用spark.memory.useLegacyMode读。spark.storage.memoryFraction分数，用于展开内存中的块。当没有足够的可用存储空间来完全展开新块时，通过删除现有块来动态分配该块。</span><br><span class="line">默认值：0.2</span><br></pre></td></tr></table></figure>

<h2 id="4-2-内存参数示例"><a href="#4-2-内存参数示例" class="headerlink" title="4.2 内存参数示例"></a>4.2 内存参数示例</h2><p>为了方便理解堆内内存，可以通过一个简单的例子来看看。假设我们提交的 Spark 作业内存设置为 –executor-memory 18g。提交命令为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--executor-memory 18g \</span><br><span class="line">--queue root.exquery \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">/opt/cloudera/parcels/CDH/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.14.4-hadoop2.6.0-cdh5.14.4.jar \</span><br><span class="line">100  </span><br></pre></td></tr></table></figure>

<p>Spark UI 页面显示的内存情况：</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/6bcf4cfc9a71d27cb91407220c2beeed-image-20211219184847518-0da8d2.png" alt="image-20211219184847518"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemMemory = spark.executor.memory</span><br><span class="line">reservedMemory = 300MB</span><br><span class="line">usableMemory = systemMemory - reservedMemory</span><br><span class="line">StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction</span><br></pre></td></tr></table></figure>

<p>代入数据，可以得出以下结果：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemMemory = 18Gb = 19327352832 字节</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 19327352832 - 314572800 = 19012780032</span><br><span class="line">StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction</span><br><span class="line">             = 19012780032 * 0.6 * 0.5 = 5703834009.6 = 5.312109375GB</span><br></pre></td></tr></table></figure>

<p>结果和 Spark UI 界面展示的 10GB 差不多有一般的差距，这是因为 Spark UI 上显示的 Storage Memory 可用内存其实等于 Storage 内存和 Execution 内存之和，也就是 usableMemory * spark.memory.fraction。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction</span><br><span class="line">             = 19012780032 * 0.6 / （1024 * 1024 * 1024) = 10.62421 GB</span><br></pre></td></tr></table></figure>

<p>这个结果和 Spark UI 上看到的结果还是有点出入，为什么呢？这是因为虽然我们设置了 –executor-memory 18g，但是 Spark 的 Executor 端通过 Runtime.getRuntime.maxMemory 拿到的内存其实没这么大，只有 17179869184 字节，所以 systemMemory = 17179869184，然后计算的数据如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemMemory = 17179869184 字节</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384</span><br><span class="line">StorageMemory= usableMemory * spark.memory.fraction</span><br><span class="line">             = 16865296384 * 0.6 /（1024 * 1024 * 1024)  = 9.42421 GB</span><br></pre></td></tr></table></figure>

<p>我们通过将上面的 16865296384 * 0.6 字节除于 1024 * 1024 * 1024 转换成 9.42421875 GB，和 UI 上显示的还是对不上，这是因为 Spark UI 是通过除于 1000 * 1000 * 1000 将字节转换成 GB，如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemMemory = 17179869184 字节</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384</span><br><span class="line">StorageMemory= usableMemory * spark.memory.fraction</span><br><span class="line">             = 16865296384 * 0.6 字节 =  16865296384 * 0.6 / (1000 * 1000 * 1000) = 10.1GB</span><br></pre></td></tr></table></figure>

<p>systemMemory 内存之所以为 17179869184（可以通过方法 Runtime.getRuntime.maxMemory( ) 获得该值），是因为内存分配池的堆部分分为 Eden，Survivor 和 Tenured 三部分空间，而这里面一共包含了两个 Survivor 区域，而这两个 Survivor 区域在任何时候我们只能用到其中一个，所以我们可以使用下面的公式进行描述：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ExecutorMemory = Eden + 2 * Survivor + Tenured</span><br><span class="line">Runtime.getRuntime.maxMemory =  Eden + Survivor + Tenured</span><br></pre></td></tr></table></figure>

<p>上面的 17179869184 字节可能因为你的 GC 配置不一样得到的数据不一样，但是上面的计算公式是一样的。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://developer.ibm.com/zh/articles/ba-cn-apache-spark-memory-management/">https://developer.ibm.com/zh/articles/ba-cn-apache-spark-memory-management/</a></li>
<li><a href="https://www.iteblog.com/archives/2342.html">Apache Spark 统一内存管理模型详解</a></li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark原理</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark ContextCleaner源码分析</title>
    <url>/2021/12/12/Spark-ContextCleaner%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<blockquote>
<p>导读：ContextCleaner 是 Spark 应用中的垃圾收集器，负责应用级别的shuffles，RDDs，broadcasts，accumulators 及 checkpointedRDD文件的清理，用于减少内存及磁盘存储的压力</p>
</blockquote>
<h1 id="1-ContextCleaner-简介"><a href="#1-ContextCleaner-简介" class="headerlink" title="1. ContextCleaner 简介"></a>1. ContextCleaner 简介</h1><p>ContextCleaner 是 Spark 应用中的垃圾收集器，负责应用级别的shuffles，RDDs，broadcasts，accumulators 及 checkpointedRDD文件的清理，用于减少内存及磁盘存储的压力。</p>
<h1 id="2-ContextCleaner-源码实现"><a href="#2-ContextCleaner-源码实现" class="headerlink" title="2. ContextCleaner 源码实现"></a>2. ContextCleaner 源码实现</h1><h2 id="2-1-ContextCleaner-的创建"><a href="#2-1-ContextCleaner-的创建" class="headerlink" title="2.1 ContextCleaner 的创建"></a>2.1 ContextCleaner 的创建</h2><p>ContextCleaner 的创建是在 SparkContext 中，创建 ContextCleaner 的代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/SparkContext.scala	</span></span><br><span class="line">	_cleaner =</span><br><span class="line">      <span class="keyword">if</span> (_conf.getBoolean(<span class="string">&quot;spark.cleaner.referenceTracking&quot;</span>, <span class="keyword">true</span>)) &#123;</span><br><span class="line">        Some(<span class="keyword">new</span> ContextCleaner(<span class="keyword">this</span>))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        None</span><br><span class="line">      &#125;</span><br><span class="line">    _cleaner.foreach(_.start())</span><br></pre></td></tr></table></figure>

<p>根据上述代码，我们知道可以通过配置属性 <code>spark.cleaner.referenceTracking</code>（默认是 true）来决定是否启用 ContextCleaner。</p>
<p>ContextCleaner 类的组成如下：</p>
<ul>
<li><p>referenceQueue：缓存顶级的 AnyRef 引用。</p>
</li>
<li><p>referenceBuffer：缓存 AnyRef 的弱引用。</p>
</li>
<li><p>listeners：缓存清理工作的监听器数组。</p>
</li>
<li><p>cleaningThread：用于具体清理工作的线程。此线程为守护线程，名称为 Spark Context Cleaner。</p>
</li>
<li><p>periodicGCService：类型为 ScheduledExecutorService，用于执行 GC（garbage collection，即垃圾收集）的调度线程池，此线程池只包含一个线程，启动的线程名称以 <code>context-cleaner-periodic-gc</code>开头。</p>
</li>
<li><p>periodicGCInterval：执行 GC 的时间间隔。可通过 <code> spark.cleaner.periodicGC.interval</code>属性进行配置，默认是 30 分钟。</p>
</li>
<li><p>blockOnCleanupTasks：清理非Shuffle的其它数据是否是阻塞式的。可通过 <code>spark.cleaner.referenceTracking.blocking</code>属性进行配置，默认是true。</p>
</li>
<li><p>blockOnShuffleCleanupTasks：清理Shuffle数据是否是阻塞式的。可通过 <code>spark.cleaner.referenceTracking.blocking.shuffle</code>属性进行配置，默认是 false。清理Shuffle数据包括：清理 MapOutputTracker 中指定 ShuffleId 对应的 map 任务状态和 ShuffleManager 中注册的 ShuffleId 对应的 Shuffle 元数据。</p>
</li>
<li><p>stopped：ContextCleaner 是否停止的状态标记。</p>
</li>
</ul>
<h2 id="2-2-ContextCleaner-的启动"><a href="#2-2-ContextCleaner-的启动" class="headerlink" title="2.2 ContextCleaner 的启动"></a>2.2 ContextCleaner 的启动</h2><p>SparkContext 在初始化的过程中会启动 ContextCleaner，只有这样 ContextCleaner 才能够清理那些超出应用范围的 RDD、Shuffle 对应的 map 任务状态、Shuffle 元数据、Broadcast 对象以及 RDD 的 Checkpoint 数据。启动 ContextCleaner 的代码如下；</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/ContextCleaner.scala</span></span><br><span class="line">  <span class="comment">/** Start the cleaner. */</span></span><br><span class="line">  <span class="function">def <span class="title">start</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="comment">// 将 cleaningThread 线程设置为守护线程，命名后启动线程</span></span><br><span class="line">    cleaningThread.setDaemon(<span class="keyword">true</span>)</span><br><span class="line">    cleaningThread.setName(<span class="string">&quot;Spark Context Cleaner&quot;</span>)</span><br><span class="line">    cleaningThread.start()</span><br><span class="line">    <span class="comment">// 定时GC线程</span></span><br><span class="line">    periodicGCService.scheduleAtFixedRate(<span class="keyword">new</span> Runnable &#123;</span><br><span class="line">      <span class="function">override def <span class="title">run</span><span class="params">()</span>: Unit </span>= System.gc()</span><br><span class="line">    &#125;, periodicGCInterval, periodicGCInterval, TimeUnit.SECONDS)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>根据上述代码，启动ContextCleaner的步骤如下：</p>
<ol>
<li>将cleaningThread设置为守护线程，并指定名称为Spark Context Cleaner。</li>
<li>启动cleaningThread。</li>
<li>给 periodicGCService 设置以 periodicGCInterval 作为时间间隔定时进行 GC 操作的任务。</li>
</ol>
<h2 id="2-3-资源的的注册"><a href="#2-3-资源的的注册" class="headerlink" title="2.3 资源的的注册"></a>2.3 资源的的注册</h2><p>ContextCleaner 提供了支持清理的多种对象类型和注册方式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/ContextCleaner.scala</span></span><br><span class="line">  <span class="comment">/** Register an RDD for cleanup when it is garbage collected. */</span></span><br><span class="line">  <span class="function">def <span class="title">registerRDDForCleanup</span><span class="params">(rdd: RDD[_])</span>: Unit </span>= &#123;</span><br><span class="line">    registerForCleanup(rdd, CleanRDD(rdd.id))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">registerAccumulatorForCleanup</span><span class="params">(a: AccumulatorV2[_, _])</span>: Unit </span>= &#123;</span><br><span class="line">    registerForCleanup(a, CleanAccum(a.id))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Register a ShuffleDependency for cleanup when it is garbage collected. */</span></span><br><span class="line">  <span class="function">def <span class="title">registerShuffleForCleanup</span><span class="params">(shuffleDependency: ShuffleDependency[_, _, _])</span>: Unit </span>= &#123;</span><br><span class="line">    registerForCleanup(shuffleDependency, CleanShuffle(shuffleDependency.shuffleId))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Register a Broadcast for cleanup when it is garbage collected. */</span></span><br><span class="line">  def registerBroadcastForCleanup[T](broadcast: Broadcast[T]): Unit = &#123;</span><br><span class="line">    registerForCleanup(broadcast, CleanBroadcast(broadcast.id))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Register a RDDCheckpointData for cleanup when it is garbage collected. */</span></span><br><span class="line">  def registerRDDCheckpointDataForCleanup[T](rdd: RDD[_], parentId: Int): Unit = &#123;</span><br><span class="line">    registerForCleanup(rdd, CleanCheckpoint(parentId))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Register an object for cleanup. */</span></span><br><span class="line">  <span class="comment">// 不同类型对象清理的注册都是以弱引用的方式添加到 referenceBuffer 中</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">registerForCleanup</span><span class="params">(objectForCleanup: AnyRef, task: CleanupTask)</span>: Unit </span>= &#123;</span><br><span class="line">    referenceBuffer.add(<span class="keyword">new</span> CleanupTaskWeakReference(task, objectForCleanup, referenceQueue))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>以 RDD 对象为例，在 registerRDDForCleanup 注册清理时调用  registerForCleanup，后者会将要清理的 RDD 封装为 CleanupTaskWeakReference 对象，并添加到 referenceBuffer 中，task 类型为 CleanRDD，CleanupTask 的一种。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/ContextCleaner.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A WeakReference associated with a CleanupTask.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * When the referent object becomes only weakly reachable, the corresponding</span></span><br><span class="line"><span class="comment"> * CleanupTaskWeakReference is automatically added to the given reference queue.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> class <span class="title">CleanupTaskWeakReference</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    val task: CleanupTask,</span></span></span><br><span class="line"><span class="params"><span class="function">    referent: AnyRef,</span></span></span><br><span class="line"><span class="params"><span class="function">    referenceQueue: ReferenceQueue[AnyRef])</span></span></span><br><span class="line"><span class="function">  extends <span class="title">WeakReference</span><span class="params">(referent, referenceQueue)</span></span></span><br></pre></td></tr></table></figure>

<p>这里封装的 CleanupTaskWeakReference 弱引用并不会被马上清理，而是要等到变为弱可达时才会被加入到 referenceQueue 队列中去被清理。</p>
<h2 id="2-4-资源的清理"><a href="#2-4-资源的清理" class="headerlink" title="2.4 资源的清理"></a>2.4 资源的清理</h2><p>在前文 ContextCleaner 启动中，我们知道会启动 cleaningThread 线程，该线程主要负责各种资源的清理操作，其中 keepCleaning 方法会死循环的跑，并会通过阻塞队列 referenceQueue 实现，当发现有对象变为弱可达时，就会从 referenceQueue 队列中取出对象，并从 referenceBuffer 中移除该弱引用，执行具体的清理逻辑。。代码如下：    </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/ContextCleaner.scala</span></span><br><span class="line">  <span class="keyword">private</span> val cleaningThread = <span class="keyword">new</span> Thread() &#123; <span class="function">override def <span class="title">run</span><span class="params">()</span> </span>&#123; keepCleaning() &#125;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Keep cleaning RDD, shuffle, and broadcast state. */</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">keepCleaning</span><span class="params">()</span>: Unit </span>= Utils.tryOrStopSparkContext(sc) &#123;</span><br><span class="line">    <span class="keyword">while</span> (!stopped) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        val reference = Option(referenceQueue.remove(ContextCleaner.REF_QUEUE_POLL_TIMEOUT))</span><br><span class="line">          .map(_.asInstanceOf[CleanupTaskWeakReference])</span><br><span class="line">        <span class="comment">// Synchronize here to avoid being interrupted on stop()</span></span><br><span class="line">        <span class="keyword">synchronized</span> &#123;</span><br><span class="line">          reference.foreach &#123; ref =&gt;</span><br><span class="line">            logDebug(<span class="string">&quot;Got cleaning task &quot;</span> + ref.task)</span><br><span class="line">            referenceBuffer.remove(ref)</span><br><span class="line">            ref.task match &#123;</span><br><span class="line">              <span class="function"><span class="keyword">case</span> <span class="title">CleanRDD</span><span class="params">(rddId)</span> </span>=&gt;</span><br><span class="line">                doCleanupRDD(rddId, blocking = blockOnCleanupTasks)</span><br><span class="line">              <span class="function"><span class="keyword">case</span> <span class="title">CleanShuffle</span><span class="params">(shuffleId)</span> </span>=&gt;</span><br><span class="line">                doCleanupShuffle(shuffleId, blocking = blockOnShuffleCleanupTasks)</span><br><span class="line">              <span class="function"><span class="keyword">case</span> <span class="title">CleanBroadcast</span><span class="params">(broadcastId)</span> </span>=&gt;</span><br><span class="line">                doCleanupBroadcast(broadcastId, blocking = blockOnCleanupTasks)</span><br><span class="line">              <span class="function"><span class="keyword">case</span> <span class="title">CleanAccum</span><span class="params">(accId)</span> </span>=&gt;</span><br><span class="line">                doCleanupAccum(accId, blocking = blockOnCleanupTasks)</span><br><span class="line">              <span class="function"><span class="keyword">case</span> <span class="title">CleanCheckpoint</span><span class="params">(rddId)</span> </span>=&gt;</span><br><span class="line">                doCleanCheckpoint(rddId)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ie: InterruptedException <span class="keyword">if</span> stopped =&gt; <span class="comment">// ignore</span></span><br><span class="line">        <span class="keyword">case</span> e: Exception =&gt; logError(<span class="string">&quot;Error in cleaning thread&quot;</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>以 doCleanupRDD 为例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/ContextCleaner.scala</span></span><br><span class="line">  <span class="comment">/** Perform RDD cleanup. */</span></span><br><span class="line">  <span class="function">def <span class="title">doCleanupRDD</span><span class="params">(rddId: Int, blocking: Boolean)</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      logDebug(<span class="string">&quot;Cleaning RDD &quot;</span> + rddId)</span><br><span class="line">      sc.unpersistRDD(rddId, blocking)</span><br><span class="line">      listeners.asScala.foreach(_.rddCleaned(rddId))</span><br><span class="line">      logInfo(<span class="string">&quot;Cleaned RDD &quot;</span> + rddId)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: Exception =&gt; logError(<span class="string">&quot;Error cleaning RDD &quot;</span> + rddId, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>doCleanupRDD的执行步骤如下。</p>
<ol>
<li><p>调用 SparkContext 的 unpersistRDD 方法从内存或磁盘中移除 RDD。</p>
</li>
<li><p>从 persistentRdds 中移除对 RDD 的跟踪。</p>
</li>
<li><p>调用所有监听器的 rddCleaned 方法。</p>
</li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><p>耿嘉安. 《Spark内核设计的艺术之架构设计与实现》</p>
</li>
<li><p><a href="https://www.huaweicloud.com/articles/13248730.html">spark源码分析-ContextCleaner缓存清理</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/87258505">Spark ContextCleaner及checkpoint的clean机制分析</a></p>
</li>
<li><p><a href="https://www.processon.com/view/link/6007a4e31e08534bec2adf7e">spark原理之一张图搞定ContextCleaner</a></p>
</li>
<li><p><a href="https://blog.csdn.net/beliefer/article/details/84998806">Spark2.1.0——ContextCleaner的工作原理分析</a></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/1797903">spark刷爆磁盘与java弱引用的关系</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark源码</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark HistoryServer日志解析异常源码分析</title>
    <url>/2021/12/18/Spark-HistoryServer%E6%97%A5%E5%BF%97%E8%A7%A3%E6%9E%90%E5%BC%82%E5%B8%B8%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<blockquote>
<p>源码版本：Cloudera Spark 2.1.0</p>
<p>导读：线上集群在使用 Spark HistoryServer 曾出现过两类问题，一类问题是用户提交 Spark 任务后 HistoryServer 服务不能正常解析日志，导致任务完成后无法在 HistoryServer 中找到作业的执行记录，严重影响到用户查看日志的体验，另一类问题是每个 Spark 作业都会向 HDFS 中写入一个 eventlog 日志文件，而 HDFS 单个目录不含递归的最大目录数/文件数是有限制的，由参数 <code>dfs.namenode.fs-limits.max-directory-items</code> 控制，默认上限是 1048576，HistoryServer 没能正常清理日志导致 HDFS 目录写满，Spark 任务无法正常提交。针对这两类问题，我们特地对 <strong>Spark 2.1.0 HistoryServer</strong> 源码进行了研究，找出根本问题出在内部的两个核心数据结构的使用存在异常导致的。</p>
</blockquote>
<h1 id="1-eventLog-日志文件及相关参数"><a href="#1-eventLog-日志文件及相关参数" class="headerlink" title="1. eventLog 日志文件及相关参数"></a>1. eventLog 日志文件及相关参数</h1><h2 id="1-1-eventLog-日志文件介绍"><a href="#1-1-eventLog-日志文件介绍" class="headerlink" title="1.1 eventLog 日志文件介绍"></a>1.1 eventLog 日志文件介绍</h2><p>eventLog 是 Spark 任务在运行过程中，调用 EventLoggingListener#logEvent() 方法来输出 eventLog 内容，Spark 中定义各种类型的事件，一旦某个事件被触发，就会构造一个类型的 Event，然后获取相应的运行信息并设置进去，最终将该 event 对象序列化成 json 字符串，追加到 eventLog 日志文件中。</p>
<p>Spark 中 eventLog 默认是不开启的，由参数 <code>spark.history.fs.cleaner.enabled</code> 来控制，开启这个配置后，任务运行的信息就会写到 eventLog 日志文件中，日志文件具体保存在参数 <code>spark.eventLog.dir</code> 配置的目录下。 </p>
<h2 id="1-2-相关配置参数"><a href="#1-2-相关配置参数" class="headerlink" title="1.2 相关配置参数"></a>1.2 相关配置参数</h2><p>一般这些配置放在 <code>/etc/spark2/conf/spark-defaults.conf</code> 中。但在实际自定义修改 Spark HistoryServer 配置时，spark-defaults.conf 中并没有写入（具体原因待看）。但可以通过查看 HistoryServer 进程使用的 spark-history-server.conf 配置查看，在 Spark HistoryServer 所在机器上，通过 <code>ps -ef |grep HistoryServer</code> 查看具体配置 <code>--properties-file /run/cloudera-scm-agent/process/136253-spark2_on_yarn-SPARK2_YARN_HISTORY_SERVER/spark2-conf/spark-history-server.conf</code>，这里会使用自定义更新的 HistoryServer 参数。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>spark.history.retainedApplications</td>
<td>50</td>
<td>在内存中保存 Application 历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，当再次访问已被删除的应用信息时需要重新构建页面。</td>
</tr>
<tr>
<td>spark.history.fs.update.interval</td>
<td>10s</td>
<td>指定刷新日志的时间，更短的时间可以更快检测到新的任务以及任务执行情况，但过快会加重服务器负载。</td>
</tr>
<tr>
<td>spark.history.ui.maxApplication</td>
<td>Int.MaxValue</td>
<td>显示在总历史页面中的程序的数量。如果总历史页面未显示，程序 UI 仍可通过访问其 URL 来显示。</td>
</tr>
<tr>
<td>spark.history.ui.port</td>
<td>18089（Spark2.1）</td>
<td>指定history-server的网页UI端口号</td>
</tr>
<tr>
<td>spark.history.fs.cleaner.enabled</td>
<td>false</td>
<td>指定history-server的日志是否定时清除，true为定时清除，false为不清除。这个值一定设置成true啊，不然日志文件会越来越大。</td>
</tr>
<tr>
<td>spark.history.fs.cleaner.interval</td>
<td>1d</td>
<td>定history-server的日志检查间隔，默认每一天会检查一下日志文件</td>
</tr>
<tr>
<td>spark.history.fs.cleaner.maxAge</td>
<td>7d</td>
<td>指定history-server日志生命周期，当检查到某个日志文件的生命周期为7d时，则会删除该日志文件</td>
</tr>
<tr>
<td>spark.eventLog.compress</td>
<td>false</td>
<td>设置history-server产生的日志文件是否使用压缩，true为使用，false为不使用。这个参数务可以成压缩哦，不然日志文件岁时间积累会过大</td>
</tr>
<tr>
<td>spark.history.retainedApplications</td>
<td>50</td>
<td>在内存中保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，当再次访问已被删除的应用信息时需要重新构建页面。</td>
</tr>
<tr>
<td>spark.history.fs.numReplayThreads</td>
<td>ceil(cpu核数/4)</td>
<td>解析 eventLog 的线程数量</td>
</tr>
</tbody></table>
<h1 id="2-eventLog-日志解析及日志清理原理"><a href="#2-eventLog-日志解析及日志清理原理" class="headerlink" title="2. eventLog 日志解析及日志清理原理"></a>2. eventLog 日志解析及日志清理原理</h1><h2 id="2-1-两个定时任务"><a href="#2-1-两个定时任务" class="headerlink" title="2.1 两个定时任务"></a>2.1 两个定时任务</h2><p>FsHistoryProvider 类在初始化时，会调用 startPolling() 方法，来启动两个定时任务，即日志文件解析任务和日志文件清理任务，两个任务均是由独立线程执行。当然，日志文件清理任务是否开启是由参数 <code>spark.history.fs.cleaner.enabled</code> 控制（默认为 false，线上环境为 true，即开启了日志文件清理任务）。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/deploy/history/FsHistoryProvider.scala</span></span><br><span class="line">  <span class="keyword">private</span>[history] <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(): <span class="type">Thread</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!isFsInSafeMode()) &#123;</span><br><span class="line">      <span class="comment">// 两个定时任务启动入口</span></span><br><span class="line">      startPolling()</span><br><span class="line">      <span class="literal">null</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      startSafeModeCheckThread(<span class="type">None</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startPolling</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Validate the log directory.</span></span><br><span class="line">    <span class="keyword">val</span> path = <span class="keyword">new</span> <span class="type">Path</span>(logDir)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Disable the background thread during tests.</span></span><br><span class="line">    <span class="keyword">if</span> (!conf.contains(<span class="string">&quot;spark.testing&quot;</span>)) &#123;</span><br><span class="line">      <span class="comment">// A task that periodically checks for event log updates on disk.</span></span><br><span class="line">      logDebug(<span class="string">s&quot;Scheduling update thread every <span class="subst">$UPDATE_INTERVAL_S</span> seconds&quot;</span>)</span><br><span class="line">      <span class="comment">// 日志文件解析线程</span></span><br><span class="line">      pool.scheduleWithFixedDelay(getRunner(checkForLogs), <span class="number">0</span>, <span class="type">UPDATE_INTERVAL_S</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (conf.getBoolean(<span class="string">&quot;spark.history.fs.cleaner.enabled&quot;</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">        <span class="comment">// A task that periodically cleans event logs on disk.</span></span><br><span class="line">        <span class="comment">// 日志文件清理线程</span></span><br><span class="line">        pool.scheduleWithFixedDelay(getRunner(cleanLogs), <span class="number">0</span>, <span class="type">CLEAN_INTERVAL_S</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      logDebug(<span class="string">&quot;Background update thread disabled for testing&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-2-eventLog-日志文件解析原理"><a href="#2-2-eventLog-日志文件解析原理" class="headerlink" title="2.2 eventLog 日志文件解析原理"></a>2.2 eventLog 日志文件解析原理</h2><h3 id="2-2-1-关键数据结构"><a href="#2-2-1-关键数据结构" class="headerlink" title="2.2.1 关键数据结构"></a>2.2.1 关键数据结构</h3><p>在介绍日志解析前，先来看看两个关键的数据结构。fileToAppInfo 和 applications。</p>
<p>fileToAppInfo 结构用于保存日志目录 <code>/user/spark/spark2ApplicationHistory/</code> 下每一条 eventLog 日志文件。每次 HDFS 目录下新生成的文件都会更新到该数据结构。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> fileToAppInfo = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Path</span>, <span class="type">FsApplicationAttemptInfo</span>]()</span><br></pre></td></tr></table></figure>

<p>applications 结构用于保存每个 App 对应的所有 AppAttempt 运行或完成的日志信息。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> applications: mutable.<span class="type">LinkedHashMap</span>[<span class="type">String</span>, <span class="type">FsApplicationHistoryInfo</span>] = <span class="keyword">new</span> mutable.<span class="type">LinkedHashMap</span>()</span><br></pre></td></tr></table></figure>

<p>举个例子：HDFS 日志目录下有同一个 App 的两个 eventLog 文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/user/spark/spark2ApplicationHistory/application_1599034722009_10003548_1</span><br><span class="line">/user/spark/spark2ApplicationHistory/application_1599034722009_10003548_2</span><br></pre></td></tr></table></figure>

<p>此时，fileToAppInfo 保存的数据格式为：（两条记录）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;&#x27;/user/spark/spark2ApplicationHistory/application_1599034722009_10003548_1&#x27;, AttemptInfo&gt;</span><br><span class="line">&lt;&#x27;/user/spark/spark2ApplicationHistory/application_1599034722009_10003548_2&#x27;, AttemptInfo&gt;</span><br></pre></td></tr></table></figure>

<p>而 applications 保存的数据格式为：（一条记录）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;&#x27;application_1599034722009_10003548&#x27;, HistoryInfo&lt;Attemp1, Attempt2&gt;&gt;</span><br></pre></td></tr></table></figure>

<h3 id="2-2-2-日志文件解析流程"><a href="#2-2-2-日志文件解析流程" class="headerlink" title="2.2.2 日志文件解析流程"></a>2.2.2 日志文件解析流程</h3><p><strong>eventLog 日志文件一次完整解析的流程大概分为以下几个步骤：</strong></p>
<ol>
<li><p>扫描 <code>/user/spark/spark2ApplicationHistory/</code> 目录下日志文件是否有更新。（更新有两个情况：一种是已有的日志文件大小增加，一种是生成了新的日志文件）。</p>
</li>
<li><p>若有更新，则从线程池中启动一个线程对日志进行初步解析。<strong>（解析环节是关键，UI 界面无法查看是因为解析出现异常）</strong></p>
</li>
<li><p>将解析后的日志同时更新到 fileToAppInfo 和 applications 结构中，保证数据维持最新状态。</p>
</li>
<li><p>等待解析线程执行完成，更新 HDFS 目录的扫描时间。（线程池启动的多个线程会阻塞执行，直到所有解析线程完成才更新扫描时间）。</p>
</li>
</ol>
<p><strong>源码分析如下：</strong></p>
<p>这段代码主要是前两个步骤的介绍，定期扫描日志目录（定期时间由参数 spark.history.fs.update.interval  控制，线上环境为 30s），将文件大小有增加和新生成的文件保存在 logInfos 对象中。然后将新文件放到 </p>
<p>replayExecutor 线程池中执行，该线程池大小默认为 机器cpu核数/4，由参数 <code>spark.history.fs.numReplayThreads</code> 控制，线上环境为 50。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/deploy/history/FsHistoryProvider.scala</span></span><br><span class="line">  <span class="keyword">private</span>[history] <span class="function"><span class="keyword">def</span> <span class="title">checkForLogs</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> newLastScanTime = getNewLastScanTime()</span><br><span class="line">      logDebug(<span class="string">s&quot;Scanning <span class="subst">$logDir</span> with lastScanTime==<span class="subst">$lastScanTime</span>&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> statusList = <span class="type">Option</span>(fs.listStatus(<span class="keyword">new</span> <span class="type">Path</span>(logDir))).map(_.toSeq)</span><br><span class="line">        .getOrElse(<span class="type">Seq</span>[<span class="type">FileStatus</span>]())</span><br><span class="line">      <span class="comment">// logInfos 保存所有新的 eventLog 文件（包括大小增加的和新生成的文件）</span></span><br><span class="line">      <span class="comment">// filter：过滤出新的日志文件</span></span><br><span class="line">      <span class="comment">// flatMap：过滤空的entry对象</span></span><br><span class="line">      <span class="comment">// sortWith：根据日志文件更新时间降序排序</span></span><br><span class="line">      <span class="keyword">val</span> logInfos: <span class="type">Seq</span>[<span class="type">FileStatus</span>] = statusList</span><br><span class="line">        .filter &#123; entry =&gt;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">val</span> prevFileSize = fileToAppInfo.get(entry.getPath()).map&#123;_.fileSize&#125;.getOrElse(<span class="number">0</span>L)</span><br><span class="line">            !entry.isDirectory() &amp;&amp;</span><br><span class="line">              !entry.getPath().getName().startsWith(<span class="string">&quot;.&quot;</span>) &amp;&amp;</span><br><span class="line">              prevFileSize &lt; entry.getLen()</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">AccessControlException</span> =&gt;</span><br><span class="line">              logDebug(<span class="string">s&quot;No permission to read <span class="subst">$entry</span>, ignoring.&quot;</span>)</span><br><span class="line">              <span class="literal">false</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        .flatMap &#123; entry =&gt; <span class="type">Some</span>(entry) &#125;</span><br><span class="line">        .sortWith &#123; <span class="keyword">case</span> (entry1, entry2) =&gt;</span><br><span class="line">          entry1.getModificationTime() &gt;= entry2.getModificationTime()</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (logInfos.nonEmpty) &#123;</span><br><span class="line">        logDebug(<span class="string">s&quot;New/updated attempts found: <span class="subst">$&#123;logInfos.size&#125;</span> <span class="subst">$&#123;logInfos.map(_.getPath)&#125;</span>&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">var</span> tasks = mutable.<span class="type">ListBuffer</span>[<span class="type">Future</span>[_]]()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (file &lt;- logInfos) &#123;</span><br><span class="line">          <span class="comment">// 对扫描出来的文件进行解析</span></span><br><span class="line">          tasks += replayExecutor.submit(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = mergeApplicationListing(file)</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">          logError(<span class="string">s&quot;Exception while submitting event log for replay&quot;</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line">  	 ... <span class="comment">//省略</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>第三步流程主要在 mergeApplicationListing() 方法中处理。先来看看 fileToAppInfo 结构如何更新，这里的关键是 replay() 方法，这里会对 eventLog 进行初步解析，然后将解析后的内容更新到 fileToAppInfo 中。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/deploy/history/FsHistoryProvider.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">mergeApplicationListing</span></span>(fileStatus: <span class="type">FileStatus</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  	<span class="comment">// 函数监听两个事件：作业开始和作业结束</span></span><br><span class="line">    <span class="keyword">val</span> newAttempts = <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> eventsFilter: <span class="type">ReplayEventsFilter</span> = &#123; eventString =&gt;</span><br><span class="line">        eventString.startsWith(<span class="type">APPL_START_EVENT_PREFIX</span>) ||</span><br><span class="line">          eventString.startsWith(<span class="type">APPL_END_EVENT_PREFIX</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> logPath = fileStatus.getPath()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> appCompleted = isApplicationCompleted(fileStatus)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// UI 查看的关键，对 eventLog 日志文件进行解析回放</span></span><br><span class="line">      <span class="keyword">val</span> appListener = replay(fileStatus, appCompleted, <span class="keyword">new</span> <span class="type">ReplayListenerBus</span>(), eventsFilter)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 根据解析的结果构建 FsApplicationAttemptInfo 对象</span></span><br><span class="line">      <span class="keyword">if</span> (appListener.appId.isDefined) &#123;</span><br><span class="line">        <span class="keyword">val</span> attemptInfo = <span class="keyword">new</span> <span class="type">FsApplicationAttemptInfo</span>(</span><br><span class="line">          logPath.getName(),</span><br><span class="line">          appListener.appName.getOrElse(<span class="type">NOT_STARTED</span>),</span><br><span class="line">          appListener.appId.getOrElse(logPath.getName()),</span><br><span class="line">          appListener.appAttemptId,</span><br><span class="line">          appListener.startTime.getOrElse(<span class="number">-1</span>L),</span><br><span class="line">          appListener.endTime.getOrElse(<span class="number">-1</span>L),</span><br><span class="line">          fileStatus.getModificationTime(),</span><br><span class="line">          appListener.sparkUser.getOrElse(<span class="type">NOT_STARTED</span>),</span><br><span class="line">          appCompleted,</span><br><span class="line">          fileStatus.getLen()</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">// 更新 fileToAppInfo 结构</span></span><br><span class="line">        fileToAppInfo(logPath) = attemptInfo</span><br><span class="line">        logDebug(<span class="string">s&quot;Application log <span class="subst">$&#123;attemptInfo.logPath&#125;</span> loaded successfully: <span class="subst">$attemptInfo</span>&quot;</span>)</span><br><span class="line">        <span class="type">Some</span>(attemptInfo)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logWarning(<span class="string">s&quot;Failed to load application log <span class="subst">$&#123;fileStatus.getPath&#125;</span>. &quot;</span> +</span><br><span class="line">          <span class="string">&quot;The application may have not started.&quot;</span>)</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    ... <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>那 applications 结构又是如何更新的呢？主要是先找出新的 App 对象，将旧的 App 列表和新的 App 列表进行合并，生成新的对象，并更新到 applications 中。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/deploy/history/FsHistoryProvider.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">mergeApplicationListing</span></span>(fileStatus: <span class="type">FileStatus</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">  	... <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">  	<span class="keyword">val</span> newAppMap = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">FsApplicationHistoryInfo</span>]()</span><br><span class="line"></span><br><span class="line">  	<span class="comment">// 多线程同时更新 applications 对象，这里用 synchronized 实现同步访问该对象</span></span><br><span class="line">    applications.synchronized &#123;</span><br><span class="line">      <span class="comment">// newAttempts 对象是刚才解析 eventLog 构造的 FsApplicationAttemptInfo 对象列表</span></span><br><span class="line">      <span class="comment">// 这一步的目的就是要过滤出刚才新生成的App对象，并更新已存在但大小有增加的App对象</span></span><br><span class="line">      newAttempts.foreach &#123; attempt =&gt;</span><br><span class="line">        <span class="keyword">val</span> appInfo = newAppMap.get(attempt.appId)</span><br><span class="line">          .orElse(applications.get(attempt.appId))</span><br><span class="line">          .map &#123; app =&gt;</span><br><span class="line">            <span class="keyword">val</span> attempts =</span><br><span class="line">              app.attempts.filter(_.attemptId != attempt.attemptId) ++ <span class="type">List</span>(attempt)</span><br><span class="line">            <span class="keyword">new</span> <span class="type">FsApplicationHistoryInfo</span>(attempt.appId, attempt.name,</span><br><span class="line">              attempts.sortWith(compareAttemptInfo))</span><br><span class="line">          &#125;</span><br><span class="line">          .getOrElse(<span class="keyword">new</span> <span class="type">FsApplicationHistoryInfo</span>(attempt.appId, attempt.name, <span class="type">List</span>(attempt)))</span><br><span class="line">        newAppMap(attempt.appId) = appInfo</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> newApps = newAppMap.values.toSeq.sortWith(compareAppInfo)</span><br><span class="line">      <span class="keyword">val</span> mergedApps = <span class="keyword">new</span> mutable.<span class="type">LinkedHashMap</span>[<span class="type">String</span>, <span class="type">FsApplicationHistoryInfo</span>]()</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">addIfAbsent</span></span>(info: <span class="type">FsApplicationHistoryInfo</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (!mergedApps.contains(info.id)) &#123;</span><br><span class="line">          mergedApps += (info.id -&gt; info)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// mergedApps 对象用于保存已有App对象和新生成的App对象进行合并后结果，产生最新的 applications 对象</span></span><br><span class="line">      <span class="keyword">val</span> newIterator = newApps.iterator.buffered</span><br><span class="line">      <span class="keyword">val</span> oldIterator = applications.values.iterator.buffered</span><br><span class="line">      <span class="keyword">while</span> (newIterator.hasNext &amp;&amp; oldIterator.hasNext) &#123;</span><br><span class="line">        <span class="keyword">if</span> (newAppMap.contains(oldIterator.head.id)) &#123;</span><br><span class="line">          oldIterator.next()</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (compareAppInfo(newIterator.head, oldIterator.head)) &#123;</span><br><span class="line">          addIfAbsent(newIterator.next())</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          addIfAbsent(oldIterator.next())</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      newIterator.foreach(addIfAbsent)</span><br><span class="line">      oldIterator.foreach(addIfAbsent)</span><br><span class="line"></span><br><span class="line">      applications = mergedApps</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-3-eventLog-日志清理原理"><a href="#2-3-eventLog-日志清理原理" class="headerlink" title="2.3 eventLog 日志清理原理"></a>2.3 eventLog 日志清理原理</h2><p>了解了前面 fileToAppInfo 和 applications 数据结构，日志清理的原理相对而言就简单很多，主要是对 applications 对象进行处理。</p>
<p><strong>日志清理大致流程如下：</strong></p>
<ol>
<li>获取 eventLog 日志保留的生命周期事件，由参数 spark.history.fs.cleaner.maxAge 控制，默认 7d，线上 5d。</li>
<li>扫描 applications 对象，将待清理的日志对象保存在 attemptsToClean 对象，保留的对象保存在 appsToRetain。（一个文件是否可以删除由函数 shouldClean() 控制）</li>
<li>更新 applications 对象。</li>
<li>调用 HDFS api 执行真正的删除操作。</li>
</ol>
<p><strong>源码分析：</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/deploy/history/FsHistoryProvider.scala</span></span><br><span class="line">   <span class="keyword">private</span>[history] <span class="function"><span class="keyword">def</span> <span class="title">cleanLogs</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、获取 eventLog 保存的生命周期时间</span></span><br><span class="line">      <span class="keyword">val</span> maxAge = conf.getTimeAsSeconds(<span class="string">&quot;spark.history.fs.cleaner.maxAge&quot;</span>, <span class="string">&quot;7d&quot;</span>) * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> now = clock.getTimeMillis()</span><br><span class="line">      <span class="keyword">val</span> appsToRetain = <span class="keyword">new</span> mutable.<span class="type">LinkedHashMap</span>[<span class="type">String</span>, <span class="type">FsApplicationHistoryInfo</span>]()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 判断函数：超过生命周期并完成（后缀不是 .inprogress 结束）的任务可以正常清理</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">shouldClean</span></span>(attempt: <span class="type">FsApplicationAttemptInfo</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">        now - attempt.lastUpdated &gt; maxAge &amp;&amp; attempt.completed</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 2、扫描 applications 对象，将超过生命周期待清理的 eventLog 保存在 attemptsToClean 对象中，未超过的保存在 appsToRetain 对象中</span></span><br><span class="line">      applications.values.foreach &#123; app =&gt;</span><br><span class="line">        <span class="keyword">val</span> (toClean, toRetain) = app.attempts.partition(shouldClean)</span><br><span class="line">        attemptsToClean ++= toClean</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (toClean.isEmpty) &#123;</span><br><span class="line">          appsToRetain += (app.id -&gt; app)</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (toRetain.nonEmpty) &#123;</span><br><span class="line">          appsToRetain += (app.id -&gt;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">FsApplicationHistoryInfo</span>(app.id, app.name, toRetain.toList))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 3、更新 applications 对象</span></span><br><span class="line">      applications = appsToRetain</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> leftToClean = <span class="keyword">new</span> mutable.<span class="type">ListBuffer</span>[<span class="type">FsApplicationAttemptInfo</span>]</span><br><span class="line">      <span class="comment">// 4、调用 HDFS api 执行真正的清理操作</span></span><br><span class="line">      attemptsToClean.foreach &#123; attempt =&gt;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          fs.delete(<span class="keyword">new</span> <span class="type">Path</span>(logDir, attempt.logPath), <span class="literal">true</span>)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">AccessControlException</span> =&gt;</span><br><span class="line">            logInfo(<span class="string">s&quot;No permission to delete <span class="subst">$&#123;attempt.logPath&#125;</span>, ignoring.&quot;</span>)</span><br><span class="line">          <span class="keyword">case</span> t: <span class="type">IOException</span> =&gt;</span><br><span class="line">            logError(<span class="string">s&quot;IOException in cleaning <span class="subst">$&#123;attempt.logPath&#125;</span>&quot;</span>, t)</span><br><span class="line">            leftToClean += attempt</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 没有正常清理的对象重新更新到 attemptsToClean 中</span></span><br><span class="line">      attemptsToClean = leftToClean</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Exception</span> =&gt; logError(<span class="string">&quot;Exception in cleaning logs&quot;</span>, t)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="3-原因分析-amp-解决方案"><a href="#3-原因分析-amp-解决方案" class="headerlink" title="3. 原因分析&amp;解决方案"></a>3. 原因分析&amp;解决方案</h1><p>上面日志解析和日志清理的逻辑都依赖 fileToAppInfo 和 applications 对象，Spark HistoryServer UI 界面展示的内容也是依赖这两个对象，所以，UI 无法加载任务信息也是由于这里的数据结构出现了多线程访问的线程安全问题。</p>
<h2 id="3-1-HashMap-线程同步问题-amp-解决方案"><a href="#3-1-HashMap-线程同步问题-amp-解决方案" class="headerlink" title="3.1 HashMap 线程同步问题&amp;解决方案"></a>3.1 HashMap 线程同步问题&amp;解决方案</h2><h3 id="3-1-1-原因分析"><a href="#3-1-1-原因分析" class="headerlink" title="3.1.1 原因分析"></a>3.1.1 原因分析</h3><p>fileToAppInfo 对象是 FsHistoryProvider 类的一个对象，数据结构采用 HashMap，是线程不安全的对象，但在多线程调用 mergeApplicationListing() 方法操作 fileToAppInfo 对象并不是同步访问，导致每次载入所有 eventLog 日志文件，会出现不能保证所有文件都能被正常加载。那为什么会出现这种情况呢？其实就是多线程访问同一个对象时经常出现的一个问题。</p>
<p>下图是多线程访问同一对象带来的线程安全问题的一个简单例子：</p>
<ul>
<li><p>当线程 1 执行 x++ 后将结果更新到内存中，内存中此时 x=1，没有问题。</p>
</li>
<li><p>但由于线程 1 在读内存数据时线程 2 同时也读取内存中 x 的值，当线程 2 执行 x++ 后，将结果更新到内存中，此时内存中 x 的值还是 1。</p>
</li>
<li><p>而预期的结果是 x = 2，这种情况便是多线程访问同一对象的线程安全问题。</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/d7b0cb45c1276e53109bed343a2227c8-1630396093098-d032d4d4-cda2-4ba8-81f0-bb35935a473b-5c3168.png" alt="img"></p>
<center>多线程访问同一对象带来的线程安全问题</center>

<h3 id="3-1-2-解决方案"><a href="#3-1-2-解决方案" class="headerlink" title="3.1.2 解决方案"></a>3.1.2 解决方案</h3><p>HashMap 对象带来的线程安全问题，解决方法比较简单，用 ConcurrentHashMap 替代即可。参考 patch：<a href="https://issues.apache.org/jira/browse/SPARK-21223">SPARK-21223</a>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> fileToAppInfo = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">Path</span>, <span class="type">FsApplicationAttemptInfo</span>]()</span><br></pre></td></tr></table></figure>

<h2 id="3-2-Synchronized-锁同步问题"><a href="#3-2-Synchronized-锁同步问题" class="headerlink" title="3.2 Synchronized 锁同步问题"></a>3.2 Synchronized 锁同步问题</h2><h3 id="3-2-1-原因分析"><a href="#3-2-1-原因分析" class="headerlink" title="3.2.1 原因分析"></a>3.2.1 原因分析</h3><p>在 Spark HistoryServer 中，applications 更新的玩法是这样的：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/deploy/history/FsHistoryProvider.scala</span></span><br><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> applications: mutable.<span class="type">LinkedHashMap</span>[<span class="type">String</span>, <span class="type">FsApplicationHistoryInfo</span>]</span><br><span class="line">    = <span class="keyword">new</span> mutable.<span class="type">LinkedHashMap</span>()</span><br><span class="line"></span><br><span class="line">applications.synchronized &#123;</span><br><span class="line">  ... <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> mergedApps = <span class="keyword">new</span> mutable.<span class="type">LinkedHashMap</span>[<span class="type">String</span>, <span class="type">FsApplicationHistoryInfo</span>]()</span><br><span class="line"></span><br><span class="line">  ... <span class="comment">// 省略更新 mergedApps 的值</span></span><br><span class="line"></span><br><span class="line">  applications = mergedApps</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>咋一看，这样使用 synchronized 锁住 applications 对象似乎没什么问题。但其实是有问题的，我们先来看一个例子。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Synchronized</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> List aList = <span class="keyword">new</span> ArrayList();</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">anyObject1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="comment">// 和 HistoryServer 玩法一致，锁住 aList 对象，代码块中用 aList2 更新 aList 对象值</span></span><br><span class="line">		<span class="keyword">synchronized</span> (aList) &#123;</span><br><span class="line">			List aList2 = <span class="keyword">new</span> ArrayList();</span><br><span class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">				System.out.println(<span class="string">&quot;anyObject&quot;</span>  + <span class="string">&quot;-&quot;</span> + Thread.currentThread());</span><br><span class="line">				aList2.add(<span class="number">1</span>);</span><br><span class="line">			&#125;</span><br><span class="line">			aList = aList2;</span><br><span class="line">			System.out.println(<span class="string">&quot;aList =&quot;</span> + aList.size());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SynchronizedDemo01</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		SynchronizedDemo01 syn = <span class="keyword">new</span> SynchronizedDemo01();</span><br><span class="line">		syn.anyObjTest();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">anyObjTest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">final</span> Synchronized syn = <span class="keyword">new</span> Synchronized();</span><br><span class="line">		<span class="comment">// 启动5个线程去操作aList对象，每次打印10条记录</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">			<span class="keyword">new</span> Thread() &#123;</span><br><span class="line">				<span class="meta">@Override</span></span><br><span class="line">				<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">					syn.anyObject1();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;.start();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">运行结果：（随机多运行几次）</span><br><span class="line">anyObject-Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main]</span><br><span class="line">anyObject-Thread[Thread-<span class="number">2</span>,<span class="number">5</span>,main]</span><br><span class="line">anyObject-Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main]</span><br><span class="line">anyObject-Thread[Thread-<span class="number">2</span>,<span class="number">5</span>,main]</span><br><span class="line">anyObject-Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main]</span><br><span class="line">aList =<span class="number">10</span></span><br><span class="line">anyObject-Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main]</span><br><span class="line">anyObject-Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main]</span><br><span class="line">anyObject-Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main]</span><br><span class="line">anyObject-Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main]</span><br><span class="line">anyObject-Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main]</span><br><span class="line">aList =<span class="number">10</span></span><br><span class="line">anyObject-Thread[Thread-<span class="number">4</span>,<span class="number">5</span>,main]</span><br><span class="line">anyObject-Thread[Thread-<span class="number">4</span>,<span class="number">5</span>,main]</span><br><span class="line">anyObject-Thread[Thread-<span class="number">4</span>,<span class="number">5</span>,main]</span><br></pre></td></tr></table></figure>

<p>通过这个例子，可以看出 Thread-3 在 Thread-2 线程中打印了信息，也就是说通过这种方式锁住 synchronized(aList 对象)（非 this 对象）是有问题的，线程并没有真正的锁住 aList 对象。那为什么会出现这种情况呢？我们接着看。</p>
<p><a href="https://blog.csdn.net/weixin_42762133/article/details/103241439">https://blog.csdn.net/weixin_42762133/article/details/103241439</a> 这篇文章给出了 Synchronized 锁几种使用场景。</p>
<table>
<thead>
<tr>
<th>修饰目标</th>
<th>锁</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>方法</td>
<td>实例方法</td>
<td>当前对象实例（即方法调用者）</td>
</tr>
<tr>
<td>静态方法</td>
<td>类对象</td>
<td></td>
</tr>
<tr>
<td>代码块</td>
<td>this</td>
<td>当前对象实例（即方法调用者）</td>
</tr>
<tr>
<td>class 对象</td>
<td>类对象</td>
<td></td>
</tr>
<tr>
<td>任意 Object 对象</td>
<td>当前对象实例（即方法调用者）</td>
<td></td>
</tr>
</tbody></table>
<p>这里重点介绍下 synchronized 修饰目标为 this 和任意 Object 对象这两种情况。要理解他们之间的区别，就需要搞清楚 synchronized 到底锁住的是什么？在 <a href="https://juejin.im/post/6844903872431915022">https://juejin.im/post/6844903872431915022</a>  这篇文章中，介绍了 synchronized 锁住的内容有两种，一种是类，另一种是<strong>对象实例</strong>。这里的关键就在于第二种情况，当使用 synchronized 锁住的是对象实例时，HistoryServer 和上面 aList 的例子那就有问题了，怎么说呢？我们来看看下面这张图。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/69f0faa17e283e35287939a943ee4ff3-1630396243880-f5c22b27-08d5-4799-ad30-b44aed94249b-a3675b.png" alt="img"></p>
<center>Synchronized锁对象示意图</center>

<p>通过这张图就一目了然，synchronized(aList) 代码块锁住的是 aList 对象指向的堆中的对象实例，当在代码块中通过 aList = aList2 赋值后，aList 便指向的新的对象实例，导致原来的对象实例变成了无主状态，synchronized(aList) 代码块的锁其实也就失去了意义。所以才会出现线程安全的问题。</p>
<p>上面那段测试代码如果采用 synchronized(this) 则不会出现多线程错乱打印的情况，为什么呢？通过上表中我们知道 synchronized(this) 的锁是当前对象实例，即方法的调用者，在测试代码中也就是 <code>SynchronizedDemo01 syn = new SynchronizedDemo01();</code> 这里创建 syn 对象实例，在内存中的表现为：</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/472e6cbccd97f63a0650398500c45f75-1630396288906-0eeee454-f92d-47ef-b7eb-c8ab665772a0-805061.png" alt="img"></p>
<center>Synchronized锁无效对象示意图</center>

<p>使用 synchronized(this) 之所以不会出问题，是由于不管 aList 指向哪个对象实例，this 对象（即 syn 对象）指向的对象实例始终没有变，所以多线程访问 aList 不会出现线程安全问题。</p>
<p>至此，HistoryServer 中的那段代码块是有问题的，并不能实现 applications 对象的多线程安全访问。</p>
<h3 id="3-2-2-解决方案"><a href="#3-2-2-解决方案" class="headerlink" title="3.2.2 解决方案"></a>3.2.2 解决方案</h3><p>分析清楚了具体原因后，解决方法就比较容易了，将那段代码的 synchronized 锁住的对象从 applications 对象改成 this 对象即可。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/deploy/history/FsHistoryProvider.scala</span></span><br><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> applications: mutable.<span class="type">LinkedHashMap</span>[<span class="type">String</span>, <span class="type">FsApplicationHistoryInfo</span>]</span><br><span class="line">    = <span class="keyword">new</span> mutable.<span class="type">LinkedHashMap</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">  ... <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> mergedApps = <span class="keyword">new</span> mutable.<span class="type">LinkedHashMap</span>[<span class="type">String</span>, <span class="type">FsApplicationHistoryInfo</span>]()</span><br><span class="line"></span><br><span class="line">  ... <span class="comment">// 省略更新 mergedApps 的值</span></span><br><span class="line"></span><br><span class="line">  applications = mergedApps</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-3-3-一点小扩展（为什么锁-object-对象）"><a href="#3-3-3-一点小扩展（为什么锁-object-对象）" class="headerlink" title="3.3.3 一点小扩展（为什么锁 object 对象）"></a>3.3.3 一点小扩展（为什么锁 object 对象）</h3><p>上面解决了 synchronized 锁住 applications 非 this 对象的问题，那 Spark 中为什么不直接用 this 对象呢？这里还是有一点小窍门的。那就是 synchronzied(this) 比 Synchronized(非this) 的效率要低一些，为什么这么说呢？来看两个例子。</p>
<p><strong>例子1：两个线程使用同一个对象分别访问 synchronized 方法和 synchronized(str) 代码块。</strong></p>
<p>结论：两个线程是异步执行的，Thread1 锁住的 ‘str’ Object 对象实例，而 Thread2 锁住的是 service 对象实例，互不影响。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SynchronizedDemo02</span> </span>&#123;</span><br><span class="line">  <span class="keyword">static</span> Service service = <span class="keyword">new</span> Service();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">new</span> Thread () &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     service.method1();</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;.start();</span><br><span class="line">   <span class="keyword">new</span> Thread () &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     service.method2();</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;.start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Service</span> </span>&#123;</span><br><span class="line"> String str = <span class="string">&quot;test&quot;</span>;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">method1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">synchronized</span> (str) &#123;</span><br><span class="line">   System.out.println(<span class="string">&quot;method1 begin&quot;</span>);</span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">    Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">   &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">   &#125;</span><br><span class="line">   System.out.println(<span class="string">&quot;method1 end&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">method2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  System.out.println(<span class="string">&quot;method2 begin&quot;</span>);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">   Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">  &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">   e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">  System.out.println(<span class="string">&quot;method2 end&quot;</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果输出：</span><br><span class="line">method1 begin</span><br><span class="line">method2 begin</span><br><span class="line">method1 end</span><br><span class="line">method2 end</span><br></pre></td></tr></table></figure>

<p><strong>例子2：两个线程使用同一个对象分别访问 synchronized 方法和 synchronized(this) 代码块。</strong></p>
<p>结论：两个线程同步执行，锁住的是同一个 this 对象（即 service 对象），必须一个线程执行完才能执行另一个线程。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SynchronizedDemo02</span> </span>&#123;</span><br><span class="line">  <span class="keyword">static</span> Service service = <span class="keyword">new</span> Service();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span>  <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">new</span> Thread () &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     service.method1();</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;.start();</span><br><span class="line">   <span class="keyword">new</span> Thread () &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     service.method2();</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;.start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Service</span> </span>&#123;</span><br><span class="line"> String str = <span class="string">&quot;test&quot;</span>;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">method1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">   System.out.println(<span class="string">&quot;method1 begin&quot;</span>);</span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">    Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">   &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">   &#125;</span><br><span class="line">   System.out.println(<span class="string">&quot;method1 end&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span>  <span class="keyword">void</span> <span class="title">method2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  System.out.println(<span class="string">&quot;method2 begin&quot;</span>);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">   Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">  &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">   e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">  System.out.println(<span class="string">&quot;method2 end&quot;</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果输出：</span><br><span class="line">method1 begin</span><br><span class="line">method1 end</span><br><span class="line">method2 begin</span><br><span class="line">method2 end</span><br></pre></td></tr></table></figure>

<p>所以，采用 synchronized(非 this 对象) 会减少当前对象锁与其他 synchorinzed(this) 代码块或 synchronized 方法之间的锁竞争，与其他 synchronized 代码异步执行，互不影响，会提高代码的执行效率。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><p><a href="https://blog.csdn.net/u013332124/article/details/88350345">Spark History Server 架构原理介绍</a> </p>
</li>
<li><p><a href="https://issues.apache.org/jira/browse/SPARK-21223">https://issues.apache.org/jira/browse/SPARK-21223</a></p>
</li>
<li><p><a href="https://blog.csdn.net/winterking3/article/details/83858782">synchronized(非this对象)同步语句块</a></p>
</li>
<li><p><a href="https://juejin.im/post/6844903872431915022">Synchronized到底锁住的是谁？</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark源码</tag>
        <tag>bugfix</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkSQL建表请求sentry鉴权失败</title>
    <url>/2021/12/18/SparkSQL%E5%BB%BA%E8%A1%A8%E8%AF%B7%E6%B1%82sentry%E9%89%B4%E6%9D%83%E5%A4%B1%E8%B4%A5/</url>
    <content><![CDATA[<blockquote>
<p>源码版本：Apache Spark 2.4.7</p>
<p>导读：使用 Spark SQL 客户端建表时 Hive MetaStore 端提示没有建表权限，相同的用户和语句在 Hive 客户端是正常执行的，主要原因是开启了 sentry 鉴权，需要将建立的表 role 角色和 HDFS 路径都当作鉴权对象，而 HDFS 路径实际是没有 role 角色控制，在 Hive 中建表的 schema 信息中是没有 location 信息所以 sentry 鉴权会通过，Spark SQL 需要进行和 Hive 类似的处理兼容 SQL 端建表。</p>
</blockquote>
<h1 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h1><p>使用 spark2.4.7-sql 建表发现的异常，用户已经申请 test_db 库权限，并且在 Hive 中建表逻辑是正常的，在 spark-sql 中提示用户没有建表权限。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 常规 spark-sql 建表</span></span><br><span class="line">spark-sql (default)&gt; create external test_db.table tmp_1(id int);</span><br><span class="line"> </span><br><span class="line">Error in query: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:User 11085244 does not have privileges for CREATETABLE);</span><br></pre></td></tr></table></figure>

<h1 id="2-原因分析"><a href="#2-原因分析" class="headerlink" title="2. 原因分析"></a>2. 原因分析</h1><h2 id="2-1-调试分析"><a href="#2-1-调试分析" class="headerlink" title="2.1 调试分析"></a>2.1 调试分析</h2><p>spark-sql 本身并不提供安全认证机制，当前集群的安全认证主要包括 sentry 和 Ranger 两大块，根据 spark-sql 建表抛出的异常 Error 信息，追溯到异常是从 sentry 代码中抛出的，由于 sentry 中抛异常的位置包含大量 DEBUG 信息，于是开启 sentry 组件的 DEBUG 日志，再次使用 spark-sql 建表，发现 sentry 端抛出如下信息：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2021-09-15 11:30:09,964 DEBUG org.apache.hadoop.security.UserGroupInformation: [pool-5-thread-3]: PrivilegedAction as:kwang (auth:SIMPLE) from:org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)</span><br><span class="line">2021-09-15 11:30:09,965 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [pool-5-thread-3]: &lt;PERFLOG method=create_table_with_environment_context from=org.apache.hadoop.hive.metastore.RetryingHMSHandler&gt;</span><br><span class="line">2021-09-15 11:30:09,965 INFO  org.apache.hadoop.hive.metastore.HiveMetaStore: [pool-5-thread-3]: 3: source:10.101.3.17 create_table: Table(tableName:tmp_1, dbName:test_db, owner:kwang, createTime:1631676574, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null)], location:hdfs://nameservice/hive/warehouse/test_db.db/tmp_1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:&#123;serialization.format=1&#125;), bucketCols:[], sortCols:[], parameters:&#123;&#125;, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:&#123;&#125;)), partitionKeys:[], parameters:&#123;spark.sql.sources.schema.numParts=1, spark.sql.sources.schema.part.0=&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;id&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;]&#125;, spark.sql.create.version=2.4.7&#125;, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:&#123;&#125;, groupPrivileges:null, rolePrivileges:null))</span><br><span class="line">2021-09-15 11:30:09,965 INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit: [pool-5-thread-3]: ugi=kwang     ip=10.101.3.17  cmd=source:10.101.3.17 create_table: Table(tableName:tmp_1, dbName:test_db, owner:kwang, createTime:1631676574, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null)], location:hdfs://nameservice/hive/warehouse/test_db.db/tmp_1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:&#123;serialization.format=1&#125;), bucketCols:[], sortCols:[], parameters:&#123;&#125;, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:&#123;&#125;)), partitionKeys:[], parameters:&#123;spark.sql.sources.schema.numParts=1, spark.sql.sources.schema.part.0=&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;id&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;]&#125;, spark.sql.create.version=2.4.7&#125;, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:&#123;&#125;, groupPrivileges:null, rolePrivileges:null))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2021-09-15 11:30:09,965 DEBUG org.apache.sentry.binding.hive.authz.HiveAuthzBinding: [pool-5-thread-3]: Going to authorize statement CREATETABLE for subject kwang</span><br><span class="line"></span><br><span class="line">2021-09-15 11:30:09,965 DEBUG org.apache.sentry.binding.hive.authz.HiveAuthzBinding: [pool-5-thread-3]: requiredInputPrivileges = &#123;URI=[ALL]&#125;</span><br><span class="line">2021-09-15 11:30:09,965 DEBUG org.apache.sentry.binding.hive.authz.HiveAuthzBinding: [pool-5-thread-3]: </span><br><span class="line">inputHierarchyList = [</span><br><span class="line">	[Server [name=server1]],</span><br><span class="line">	[Server [name=server1], Database [name=test_db]],</span><br><span class="line">	[Server [name=server1]],</span><br><span class="line">	[Server [name=server1], URI [name=hdfs://nameservice/hive/warehouse/test_db.db/tmp_1]]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">2021-09-15 11:30:09,965 DEBUG org.apache.sentry.binding.hive.authz.HiveAuthzBinding: [pool-5-thread-3]: requiredOuputPrivileges = &#123;Db=[ALL]&#125;</span><br><span class="line">2021-09-15 11:30:09,965 DEBUG org.apache.sentry.binding.hive.authz.HiveAuthzBinding: [pool-5-thread-3]: </span><br><span class="line">outputHierarchyList = [</span><br><span class="line">	[Server [name=server1]],</span><br><span class="line">	[Server [name=server1], Database [name=test_db]]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">2021-09-15 11:30:09,965 DEBUG org.apache.sentry.provider.common.ResourceAuthorizationProvider: [pool-5-thread-3]: Authorization Request for Subject [name=kwang] [Server [name=server1], URI [name=hdfs://nameservice/hive/warehouse/test_db.db/tmp_1]] and [ALL]</span><br><span class="line"></span><br><span class="line">2021-09-18 11:49:03,081 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-5-thread-195]: MetaException(message:User kwang does not have privileges for CREATETABLE)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.firePreEvent(HiveMetaStore.java:2136)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1459)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1557)</span><br><span class="line">        at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)</span><br><span class="line">        at com.sun.proxy.$Proxy11.create_table_with_environment_context(Unknown Source)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_table_with_environment_context.getResult(ThriftHiveMetastore.java:9974)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_table_with_environment_context.getResult(ThriftHiveMetastore.java:9958)</span><br><span class="line">        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)</span><br><span class="line">        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:745)</span><br></pre></td></tr></table></figure>

<p>可以看到 sentry 端在对鉴权对象 <code>URI [name=hdfs://nameservice/hive/warehouse/test_db.db/tmp_1]</code> 路径鉴权时没有权限。而通过 Hive 建表进行鉴权时并没有对 URI 路径进行鉴权，Hive 端传递过来的 location 为 null。这也解释得通为什么 spark-sql 建表权限不通过，因为在 sentry 鉴权过程中用户有对应的 DB role 权限，而 URI role 在 sentry 中并不存在，鉴权也就无法通过。</p>
<h2 id="2-2-鉴权逻辑源码分析"><a href="#2-2-鉴权逻辑源码分析" class="headerlink" title="2.2 鉴权逻辑源码分析"></a>2.2 鉴权逻辑源码分析</h2><p>通过对 sentry 日志的分析，定位到 sentry 鉴权的代码逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/sentry/binding/hive/authz/HiveAuthzBinding.java</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Validate the privilege for the given operation for the given subject</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">authorize</span><span class="params">(HiveOperation hiveOp, HiveAuthzPrivileges stmtAuthPrivileges,</span></span></span><br><span class="line"><span class="params"><span class="function">      Subject subject, List&lt;List&lt;DBModelAuthorizable&gt;&gt; inputHierarchyList,</span></span></span><br><span class="line"><span class="params"><span class="function">      List&lt;List&lt;DBModelAuthorizable&gt;&gt; outputHierarchyList)</span></span></span><br><span class="line"><span class="function">          <span class="keyword">throws</span> AuthorizationException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!open) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Binding has been closed&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">boolean</span> isDebug = LOG.isDebugEnabled();</span><br><span class="line">    <span class="keyword">if</span>(isDebug) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Going to authorize statement &quot;</span> + hiveOp.name() +</span><br><span class="line">          <span class="string">&quot; for subject &quot;</span> + subject.getName());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* for each read and write entity captured by the compiler -</span></span><br><span class="line"><span class="comment">     *    check if that object type is part of the input/output privilege list</span></span><br><span class="line"><span class="comment">     *    If it is, then validate the access.</span></span><br><span class="line"><span class="comment">     * Note the hive compiler gathers information on additional entities like partitions,</span></span><br><span class="line"><span class="comment">     * etc which are not of our interest at this point. Hence its very</span></span><br><span class="line"><span class="comment">     * much possible that the we won&#x27;t be validating all the entities in the given list</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Check read entities</span></span><br><span class="line">    Map&lt;AuthorizableType, EnumSet&lt;DBModelAction&gt;&gt; requiredInputPrivileges =</span><br><span class="line">        stmtAuthPrivileges.getInputPrivileges();</span><br><span class="line">    <span class="keyword">if</span>(isDebug) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;requiredInputPrivileges = &quot;</span> + requiredInputPrivileges);</span><br><span class="line">      LOG.debug(<span class="string">&quot;inputHierarchyList = &quot;</span> + inputHierarchyList);</span><br><span class="line">    &#125;</span><br><span class="line">    Map&lt;AuthorizableType, EnumSet&lt;DBModelAction&gt;&gt; requiredOutputPrivileges =</span><br><span class="line">        stmtAuthPrivileges.getOutputPrivileges();</span><br><span class="line">    <span class="keyword">if</span>(isDebug) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;requiredOuputPrivileges = &quot;</span> + requiredOutputPrivileges);</span><br><span class="line">      LOG.debug(<span class="string">&quot;outputHierarchyList = &quot;</span> + outputHierarchyList);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> found = <span class="keyword">false</span>;</span><br><span class="line">    <span class="comment">// 对 input 对象鉴权</span></span><br><span class="line">    <span class="keyword">for</span>(AuthorizableType key: requiredInputPrivileges.keySet()) &#123;</span><br><span class="line">      <span class="keyword">for</span> (List&lt;DBModelAuthorizable&gt; inputHierarchy : inputHierarchyList) &#123;</span><br><span class="line">        <span class="keyword">if</span> (getAuthzType(inputHierarchy).equals(key)) &#123;</span><br><span class="line">          found = <span class="keyword">true</span>;</span><br><span class="line">          <span class="comment">// 真正鉴权逻辑</span></span><br><span class="line">          <span class="keyword">if</span> (!authProvider.hasAccess(subject, inputHierarchy, requiredInputPrivileges.get(key), activeRoleSet)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> AuthorizationException(<span class="string">&quot;User &quot;</span> + subject.getName() +</span><br><span class="line">                <span class="string">&quot; does not have privileges for &quot;</span> + hiveOp.name());</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span>(!found &amp;&amp; !(key.equals(AuthorizableType.URI)) &amp;&amp;  !(hiveOp.equals(HiveOperation.QUERY))</span><br><span class="line">          &amp;&amp; !(hiveOp.equals(HiveOperation.CREATETABLE_AS_SELECT))) &#123;</span><br><span class="line">        <span class="comment">//URI privileges are optional for some privileges: anyPrivilege, tableDDLAndOptionalUriPrivilege</span></span><br><span class="line">        <span class="comment">//Query can mean select/insert/analyze where all of them have different required privileges.</span></span><br><span class="line">        <span class="comment">//CreateAsSelect can has table/columns privileges with select.</span></span><br><span class="line">        <span class="comment">//For these alone we skip if there is no equivalent input privilege</span></span><br><span class="line">        <span class="comment">//<span class="doctag">TODO:</span> Even this case should be handled to make sure we do not skip the privilege check if we did not build</span></span><br><span class="line">        <span class="comment">//the input privileges correctly</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> AuthorizationException(<span class="string">&quot;Required privilege( &quot;</span> + key.name() + <span class="string">&quot;) not available in input privileges&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      found = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 对 output 对象鉴权</span></span><br><span class="line">    <span class="keyword">for</span>(AuthorizableType key: requiredOutputPrivileges.keySet()) &#123;</span><br><span class="line">      <span class="keyword">for</span> (List&lt;DBModelAuthorizable&gt; outputHierarchy : outputHierarchyList) &#123;</span><br><span class="line">        <span class="keyword">if</span> (getAuthzType(outputHierarchy).equals(key)) &#123;</span><br><span class="line">          found = <span class="keyword">true</span>;</span><br><span class="line">          <span class="keyword">if</span> (!authProvider.hasAccess(subject, outputHierarchy, requiredOutputPrivileges.get(key), activeRoleSet)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> AuthorizationException(<span class="string">&quot;User &quot;</span> + subject.getName() +</span><br><span class="line">                <span class="string">&quot; does not have privileges for &quot;</span> + hiveOp.name());</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span>(!found &amp;&amp; !(key.equals(AuthorizableType.URI)) &amp;&amp;  !(hiveOp.equals(HiveOperation.QUERY))) &#123;</span><br><span class="line">        <span class="comment">//URI privileges are optional for some privileges: tableInsertPrivilege</span></span><br><span class="line">        <span class="comment">//Query can mean select/insert/analyze where all of them have different required privileges.</span></span><br><span class="line">        <span class="comment">//For these alone we skip if there is no equivalent output privilege</span></span><br><span class="line">        <span class="comment">//<span class="doctag">TODO:</span> Even this case should be handled to make sure we do not skip the privilege check if we did not build</span></span><br><span class="line">        <span class="comment">//the output privileges correctly</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> AuthorizationException(<span class="string">&quot;Required privilege( &quot;</span> + key.name() + <span class="string">&quot;) not available in output privileges&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      found = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>该方法的传入参数包括：</p>
<ul>
<li><p>hiveOp：当前 sql 的操作类型。</p>
</li>
<li><p>stmtAuthPrivileges：本次操作所需的权限集合。</p>
</li>
<li><p>subject：表示当前操作的用户</p>
</li>
<li><p>inputHierarchyList：sql 请求的输入对象。</p>
</li>
<li><p>outputHierarchyList：sql 请求的输出对象。</p>
</li>
</ul>
<p>用户的鉴权分为两步：</p>
<ol>
<li>用户是否拥有对 input 对象列表的该 operation 对应的访问权限。</li>
<li>用户是否拥有对 output 对象列表的该 operation 对应的访问权限。</li>
</ol>
<p>stmtAuthPrivileges 包含了 input 对象权限 map 和 output 对象权限 map，map 的 key 值为一个 AuthorizableType 枚举对象，取值为 Server/Db/Table/Column/View/URI 中的一种，对于每一个 AuthorizableType，至少有一个 inputList 或 outputList 与其 AuthorizableType 相同，此时通过 Provider 的 hasAccess 方法判断该用户是否对该对象列表拥有相应的权限。</p>
<p>真正校验权限的逻辑在 ResourceAuthorizationProvider 的 doHasAccess 方法中：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/sentry/provider/common/ResourceAuthorizationProvider.java</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">doHasAccess</span><span class="params">(Subject subject,</span></span></span><br><span class="line"><span class="params"><span class="function">      List&lt;? extends Authorizable&gt; authorizables, Set&lt;? extends Action&gt; actions,</span></span></span><br><span class="line"><span class="params"><span class="function">      ActiveRoleSet roleSet)</span> </span>&#123;</span><br><span class="line">    Set&lt;String&gt; groups =  getGroups(subject);</span><br><span class="line">    Set&lt;String&gt; hierarchy = <span class="keyword">new</span> HashSet&lt;String&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Authorizable authorizable : authorizables) &#123;</span><br><span class="line">      hierarchy.add(KV_JOINER.join(authorizable.getTypeName(), authorizable.getName()));</span><br><span class="line">    &#125;</span><br><span class="line">    List&lt;String&gt; requestPrivileges = buildPermissions(authorizables, actions);</span><br><span class="line">    Iterable&lt;Privilege&gt; privileges = getPrivileges(groups, roleSet, authorizables.toArray(<span class="keyword">new</span> Authorizable[<span class="number">0</span>]));</span><br><span class="line">    lastFailedPrivileges.get().clear();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (String requestPrivilege : requestPrivileges) &#123;</span><br><span class="line">      Privilege priv = privilegeFactory.createPrivilege(requestPrivilege);</span><br><span class="line">      <span class="keyword">for</span> (Privilege permission : privileges) &#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Does the permission granted in the policy file imply the requested action?</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">boolean</span> result = permission.implies(priv);</span><br><span class="line">        <span class="keyword">if</span>(LOGGER.isDebugEnabled()) &#123;</span><br><span class="line">          LOGGER.debug(<span class="string">&quot;ProviderPrivilege &#123;&#125;, RequestPrivilege &#123;&#125;, RoleSet, &#123;&#125;, Result &#123;&#125;&quot;</span>,</span><br><span class="line">              <span class="keyword">new</span> Object[]&#123; permission, requestPrivilege, roleSet, result&#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (result) &#123;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    lastFailedPrivileges.get().addAll(requestPrivileges);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>sentry 根据用户的组、角色从数据库中读取其拥有的权限，并与需要的权限进行对比，只有当 inputHierarchyList 中的所有权限都符合时，才能通过认证。</p>
<p>通过对 spark-sql 执行时 sentry 日志和 hive-cli 执行时 sentry 日志对比发现，spark-sql 执行传入的 inputHierarchyList 中包含了欲创建表的 location 信息，而该 location URI 在 sentry 中并没有对应的 role 角色，因此对该表的权限认证不能通过。</p>
<h1 id="3-解决方案"><a href="#3-解决方案" class="headerlink" title="3. 解决方案"></a>3. 解决方案</h1><p>通过分析 spark-sql 的源码，spark 创建表的逻辑在 HiveClientImpl 的 createTable 方法中：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/client/HiveClientImpl.scala</span></span><br><span class="line">  <span class="function">override def <span class="title">createTable</span><span class="params">(table: CatalogTable, ignoreIfExists: Boolean)</span>: Unit </span>= withHiveState &#123;</span><br><span class="line">    verifyColumnDataType(table.dataSchema)</span><br><span class="line">    client.createTable(toHiveTable(table, Some(userName)), ignoreIfExists)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>spark 在这里对 Table 类型进行了转换，将 CatalogTable 转化成 HiveTable，而 CatalogTable 中包含了 location 信息，参照 Hive 建表的方式这里的 location 是多余的。因此，对这块逻辑进行了修改，spark-sql 在执行 createTable 时，去掉 Table 属性的 location 信息，这里只针对内部表进行修改，外部表的创建一般都会指定 location 信息，这里还是保留不变。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/client/HiveClientImpl.scala</span></span><br><span class="line">  <span class="function">override def <span class="title">createTable</span><span class="params">(table: CatalogTable, ignoreIfExists: Boolean)</span>: Unit </span>= withHiveState &#123;</span><br><span class="line">    verifyColumnDataType(table.dataSchema)</span><br><span class="line">    val hiveTable = toHiveTable(table, Some(userName))</span><br><span class="line">    <span class="keyword">if</span> (hiveTable.getTableType.equals(HiveTableType.MANAGED_TABLE)</span><br><span class="line">      &amp;&amp; sparkConf.getBoolean(<span class="string">&quot;spark.sql.enable.sentry&quot;</span>, defaultValue = <span class="keyword">true</span>)) &#123;</span><br><span class="line">      hiveTable.getTTable.getSd.setLocation(<span class="keyword">null</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    client.createTable(hiveTable, ignoreIfExists)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><p><a href="https://www.huaweicloud.com/articles/12456466.html">sparksql集成sentry遇到的问题</a></p>
</li>
<li><p><a href="https://github.com/shenh062326/document/blob/master/Sentry%E9%89%B4%E6%9D%83%E5%8E%9F%E7%90%86/Sentry%E9%89%B4%E6%9D%83%E5%8E%9F%E7%90%86.md">Sentry鉴权原理</a></p>
</li>
<li><p><a href="https://blog.csdn.net/zhanyuanlin/article/details/95898018">Spark对HiveMetastore客户端的多版本管理、兼容性探究以及栅栏实现</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark源码</tag>
        <tag>bugfix</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark作业提交流程及源码分析</title>
    <url>/2021/12/15/Spark%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<blockquote>
<p>源码版本：Apache Spark 2.4.7</p>
<p>导读：本文主要从源码角度介绍 Spark 作业的提交流程，一个应用程序从提交，到执行流程图的构建，到任务在 Executor 端执行，Spark 是如何执行这一系列的逻辑呢，本文将结合源码展开介绍。</p>
</blockquote>
<h1 id="1-作业执行流程简介"><a href="#1-作业执行流程简介" class="headerlink" title="1. 作业执行流程简介"></a>1. 作业执行流程简介</h1><p>下图是 Spark 作业提交到执行的流程，主要划分为四个阶段：</p>
<ul>
<li><p>编写 Driver 程序，定义 RDD 的 Action 和 Transformation 操作，Driver 端启动时根据算子依赖关系形成 DAG 有向无环图。</p>
</li>
<li><p>根据形成的 DAG 图，DAGScheduler 将其划分为不同的 Stage。</p>
</li>
<li><p>每个 Stage 中有一个 TaskSet（一系列 Task 的集合），DAGScheduler 将 TaskSet 交给 TaskScheduler 去执行，TaskScheduler 将任务执行完毕后将结果返回给 DAGScheduler。</p>
</li>
<li><p>TaskScheduler 会真正将 Task 分发到不同的 Worker 节点去执行，并将执行结果返回给 TaskScheduler。</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/fa8bbf2d26c01e17c90e93e9f8ed5592-1639723948405-ad5b4876-d332-4f40-823c-d914cd9c8c92-e9a44a.png" alt="img"></p>
<center>Spark作业提交逻辑图</center>

<h1 id="2-任务执行源码分析"><a href="#2-任务执行源码分析" class="headerlink" title="2. 任务执行源码分析"></a>2. 任务执行源码分析</h1><p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/bd630cd4ad8c5a2241090e04c547029b-1630392633594-8f32c89c-f5cd-46b0-8eef-5c750676ff7d-53919e.png" alt="img"></p>
<center>Spark作业提交源码流程图</center>

<p>Spark 的作业调度主要是基于 RDD 的一系列操作构成一个 Job，然后在 Executor 中执行，这些操作算子分为转换操作（Transform）和行动操作（Action)，只有触发了 Action 操作才会触发 Job 的提交。本文以 RDD 的 count Action 算子为入口，介绍 Spark 作业的提交过程。</p>
<h2 id="2-1-初始化操作"><a href="#2-1-初始化操作" class="headerlink" title="2.1 初始化操作"></a>2.1 初始化操作</h2><p>Driver 端作为 Spark 作业执行的核心组件，管控着整个 Spark 作业的生命周期，而所谓的 Driver 端最核心的对象就在于 SparkContext，在 Job 提交之前，都需要对 SparkContext 进行初始化。SparkContext 初始化时会构建特别多的关键对象，目的是管控 Spark 作业的生命周期。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/SparkContext.scala</span></span><br><span class="line">  <span class="comment">/* ------------------------------------------------------------------------------------- *</span></span><br><span class="line"><span class="comment">   | Private variables. These variables keep the internal state of the context, and are    |</span></span><br><span class="line"><span class="comment">   | not accessible by the outside world. They&#x27;re mutable since we want to initialize all  |</span></span><br><span class="line"><span class="comment">   | of them to some neutral value ahead of time, so that calling &quot;stop()&quot; while the       |</span></span><br><span class="line"><span class="comment">   | constructor is still running is safe.                                                 |</span></span><br><span class="line"><span class="comment">   * ------------------------------------------------------------------------------------- */</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _conf: <span class="type">SparkConf</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _eventLogDir: <span class="type">Option</span>[<span class="type">URI</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _eventLogCodec: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _listenerBus: <span class="type">LiveListenerBus</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _env: <span class="type">SparkEnv</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _statusTracker: <span class="type">SparkStatusTracker</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _progressBar: <span class="type">Option</span>[<span class="type">ConsoleProgressBar</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _ui: <span class="type">Option</span>[<span class="type">SparkUI</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _hadoopConfiguration: <span class="type">Configuration</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _executorMemory: <span class="type">Int</span> = _</span><br><span class="line">	<span class="comment">// SchedulerBackend对象，TaskScheduler 的调度后端接口，真正负责 Task 的资源分配</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _schedulerBackend: <span class="type">SchedulerBackend</span> = _</span><br><span class="line">	<span class="comment">// TaskScheduler对象，负责将 Task 分配给 Executor 执行（通过 SchedulerBackend 实现）</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _taskScheduler: <span class="type">TaskScheduler</span> = _</span><br><span class="line">	<span class="comment">// 心跳接收器，负责定时接收 Executor 的心跳</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _heartbeatReceiver: <span class="type">RpcEndpointRef</span> = _</span><br><span class="line">	<span class="comment">// DAGScheduler对象，负责 Job 的创建，Stage 划分，并将 Stage 交给 TaskScheduler</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> _dagScheduler: <span class="type">DAGScheduler</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _applicationId: <span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _applicationAttemptId: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line">  <span class="comment">// 事件监听器（可选服务），负责将事件持久化到存储系统</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _eventLogger: <span class="type">Option</span>[<span class="type">EventLoggingListener</span>] = <span class="type">None</span></span><br><span class="line">  <span class="comment">// 资源管理器：负责Executor资源的动态申请和分配</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _executorAllocationManager: <span class="type">Option</span>[<span class="type">ExecutorAllocationManager</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _cleaner: <span class="type">Option</span>[<span class="type">ContextCleaner</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _listenerBusStarted: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _jars: <span class="type">Seq</span>[<span class="type">String</span>] = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _files: <span class="type">Seq</span>[<span class="type">String</span>] = _</span><br><span class="line">  <span class="comment">// Hook函数，处理程序退出时的逻辑</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _shutdownHookRef: <span class="type">AnyRef</span> = _</span><br><span class="line">  <span class="comment">// 状态存储，负责收集和展示作业的运行状态信息</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _statusStore: <span class="type">AppStatusStore</span> = _</span><br><span class="line">  <span class="comment">// 心跳服务，负责构建心跳对象</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _heartbeater: <span class="type">Heartbeater</span> = _</span><br></pre></td></tr></table></figure>

<p>根据前面 Spark 作业的执行流程中，最重要的对象包括 DAGScheduler、TaskScheduler 和 SchedulerBackend。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/SparkContext.scala</span></span><br><span class="line">    <span class="comment">// Create and start the scheduler</span></span><br><span class="line">    <span class="keyword">val</span> (sched, ts) = <span class="type">SparkContext</span>.createTaskScheduler(<span class="keyword">this</span>, master, deployMode)</span><br><span class="line">    _schedulerBackend = sched</span><br><span class="line">    _taskScheduler = ts</span><br><span class="line">    _dagScheduler = <span class="keyword">new</span> <span class="type">DAGScheduler</span>(<span class="keyword">this</span>)</span><br></pre></td></tr></table></figure>

<h2 id="2-2-提交Job"><a href="#2-2-提交Job" class="headerlink" title="2.2 提交Job"></a>2.2 提交Job</h2><p>当我们执行 RDD 的 count()，产生了一个 Action 操作，每个 Action 操作在 Spark 里都会生成一个 Job。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/rdd/RDD.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>

<p>count() 方法底层是执行 SparkContext#runJob() 方法，可以看到在 SparkContext 内部重载了多个 runJob()，最后会调用到 DAGScheduler#runJob() 执行真正的作业提交。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/SparkContext.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">    runJob(rdd, func, <span class="number">0</span> until rdd.partitions.length)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">    runJob(rdd, (ctx: <span class="type">TaskContext</span>, it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedFunc(it), partitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> results = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">U</span>](partitions.size)</span><br><span class="line">    runJob[<span class="type">T</span>, <span class="type">U</span>](rdd, func, partitions, (index, res) =&gt; results(index) = res)</span><br><span class="line">    results</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;SparkContext has been shutdown&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> callSite = getCallSite</span><br><span class="line">    <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">    logInfo(<span class="string">&quot;Starting job: &quot;</span> + callSite.shortForm)</span><br><span class="line">    <span class="keyword">if</span> (conf.getBoolean(<span class="string">&quot;spark.logLineage&quot;</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">      logInfo(<span class="string">&quot;RDD&#x27;s recursive dependencies:\n&quot;</span> + rdd.toDebugString)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 调用 dagScheduler 的 runJob 方法来提交作业</span></span><br><span class="line">    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">    progressBar.foreach(_.finishAll())</span><br><span class="line">    rdd.doCheckpoint()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>DAGScheduler#runJob() 内部调用 submitJob() 提交作业，并对作业的执行结果（成功或失败）进行判断处理。因此，可以知道 submitJob() 是 Spark Job 提交最关键的入口，内部会执行具体的 Job 操作，最后返回 Job 的结果状态。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">      callSite: <span class="type">CallSite</span>,</span><br><span class="line">      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">      properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime</span><br><span class="line">    <span class="comment">// 调用 submitJob 方法提交作业，返回成功或者失败</span></span><br><span class="line">    <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br><span class="line">    <span class="type">ThreadUtils</span>.awaitReady(waiter.completionFuture, <span class="type">Duration</span>.<span class="type">Inf</span>)</span><br><span class="line">    waiter.completionFuture.value.get <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> scala.util.<span class="type">Success</span>(_) =&gt;</span><br><span class="line">        logInfo(<span class="string">&quot;Job %d finished: %s, took %f s&quot;</span>.format</span><br><span class="line">          (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">      <span class="keyword">case</span> scala.util.<span class="type">Failure</span>(exception) =&gt;</span><br><span class="line">        logInfo(<span class="string">&quot;Job %d failed: %s, took %f s&quot;</span>.format</span><br><span class="line">          (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">        <span class="comment">// SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.</span></span><br><span class="line">        <span class="keyword">val</span> callerStackTrace = <span class="type">Thread</span>.currentThread().getStackTrace.tail</span><br><span class="line">        exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)</span><br><span class="line">        <span class="keyword">throw</span> exception</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>DAGScheduler#submitJob() 内部会发送 JobSubmitted 事件，该事件继承自 DAGSchedulerEvent 事件接口类。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">      callSite: <span class="type">CallSite</span>,</span><br><span class="line">      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">      properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">    <span class="comment">// Check to make sure we are not launching a task on a partition that does not exist.</span></span><br><span class="line">    <span class="keyword">val</span> maxPartitions = rdd.partitions.length</span><br><span class="line">    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach &#123; p =&gt;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</span><br><span class="line">        <span class="string">&quot;Attempting to access a non-existent partition: &quot;</span> + p + <span class="string">&quot;. &quot;</span> +</span><br><span class="line">          <span class="string">&quot;Total number of partitions: &quot;</span> + maxPartitions)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()</span><br><span class="line">    <span class="keyword">if</span> (partitions.size == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// Return immediately if the job is running 0 tasks</span></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    assert(partitions.size &gt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span><br><span class="line">    <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span><br><span class="line">    <span class="comment">// 关键：DAGSchedulerEventProcessLoop 是 DAGScheduler 的内部类</span></span><br><span class="line">    <span class="comment">// 负责接收和转发 DAGSchedulerEvent 事件 </span></span><br><span class="line">    eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">      <span class="type">SerializationUtils</span>.clone(properties)))</span><br><span class="line">    waiter</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// JobSubmitted 事件定义</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JobSubmitted</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    jobId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    finalRDD: <span class="type">RDD</span>[_],</span></span></span><br><span class="line"><span class="params"><span class="class">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]</span>) <span class="title">=&gt;</span> <span class="title">_</span>,</span></span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span> = <span class="literal">null</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">DAGSchedulerEvent</span></span><br></pre></td></tr></table></figure>

<h2 id="2-3-DAGScheduler事件转发"><a href="#2-3-DAGScheduler事件转发" class="headerlink" title="2.3 DAGScheduler事件转发"></a>2.3 DAGScheduler事件转发</h2><p>上面 submitJob() 内部通过 eventProcessLoop.post() 方法发送 JobSubmited 事件，那该事件如何处理呢？那就需要看看 DAGSchedulerEventProcessLoop 类的具体实现。</p>
<p>DAGSchedulerEventProcessLoop 类继承自 EventLoop 类，可以看到 post() 方法在 DAGSchedulerEventProcessLoop 并没有定义，因此一定是其唯一父类 EventLoop 定义的。EventLoop 的实现是典型的<code>生产者-消费者模型</code>，通过 post() 生产数据到阻塞队列 eventQueue，然后创建异步线程从队列中取出事件，并交给对应的处理器处理。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/util/EventLoop.scala</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">EventLoop</span>[<span class="type">E</span>](<span class="params">name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义事件阻塞队列，接收事件</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> eventQueue: <span class="type">BlockingQueue</span>[<span class="type">E</span>] = <span class="keyword">new</span> <span class="type">LinkedBlockingDeque</span>[<span class="type">E</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 内部独立线程通过 eventQueue.take() 消费事件，并通过 onReceive() 方法转发事件</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> eventThread = <span class="keyword">new</span> <span class="type">Thread</span>(name) &#123;</span><br><span class="line">    setDaemon(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (!stopped.get) &#123;</span><br><span class="line">          <span class="comment">// 从队列中取出事件</span></span><br><span class="line">          <span class="keyword">val</span> event = eventQueue.take()</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 转发事件</span></span><br><span class="line">            onReceive(event)</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                onError(e)</span><br><span class="line">              &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// exit even if eventQueue is not empty</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// DAGScheduler 内部调用 eventProcessLoop.start() 启动 EventLoop 服务</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(name + <span class="string">&quot; has already been stopped&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Call onStart before starting the event thread to make sure it happens before onReceive</span></span><br><span class="line">    onStart()</span><br><span class="line">    <span class="comment">// 启动独立线程消费事件</span></span><br><span class="line">    eventThread.start()</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 向队列中添加事件</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    eventQueue.put(event)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// EventLoop 抽象类的方法，内部没有具体实现</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>前面通过 eventProcessLoop.post() 发送 JobSubmitted 事件，添加到阻塞队列，异步线程边阻塞地从队列中取出事件并转发给对应的处理器处理，真正的处理方法为 onReceive()，但 EventLoop 抽象类中的 onReceive() 方法并没有具体实现，因此一定是唯一子类 DAGSchedulerEventProcessLoop 去处理 onReceive() 方法。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(<span class="params">dagScheduler: <span class="type">DAGScheduler</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="string">&quot;dag-scheduler-event-loop&quot;</span>) <span class="keyword">with</span> <span class="type">Logging</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> timer = dagScheduler.metricsSource.messageProcessingTimer</span><br><span class="line"></span><br><span class="line">  <span class="comment">// EventLoop 异步线程取出事件后真正的处理位置</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> timerContext = timer.time()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      doOnReceive(event)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      timerContext.stop()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 根据不同的事件类型执行相应逻辑</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MapStageSubmitted</span>(jobId, dependency, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">StageCancelled</span>(stageId, reason) =&gt;</span><br><span class="line">      dagScheduler.handleStageCancellation(stageId, reason)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobCancelled</span>(jobId, reason) =&gt;</span><br><span class="line">      dagScheduler.handleJobCancellation(jobId, reason)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobGroupCancelled</span>(groupId) =&gt;</span><br><span class="line">      dagScheduler.handleJobGroupCancelled(groupId)</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-4-Stage执行调度"><a href="#2-4-Stage执行调度" class="headerlink" title="2.4 Stage执行调度"></a>2.4 Stage执行调度</h2><p>DAGScheduler#handleJobSubmitted() 方法真正处理 JobSubmitted 事件，负责 finalStage 的创建和 stage 的划分与提交工作。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">      finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">      partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">      callSite: <span class="type">CallSite</span>,</span><br><span class="line">      listener: <span class="type">JobListener</span>,</span><br><span class="line">      properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">			<span class="comment">// 关键：根据最后一个RDD，回溯创建 finalStage</span></span><br><span class="line">      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">BarrierJobSlotsNumberCheckFailed</span> =&gt;</span><br><span class="line">        logWarning(<span class="string">s&quot;The job <span class="subst">$jobId</span> requires to run a barrier stage that requires more slots &quot;</span> +</span><br><span class="line">          <span class="string">&quot;than the total number of slots in the cluster currently.&quot;</span>)</span><br><span class="line">        <span class="comment">// If jobId doesn&#x27;t exist in the map, Scala coverts its value null to 0: Int automatically.</span></span><br><span class="line">        <span class="keyword">val</span> numCheckFailures = barrierJobIdToNumTasksCheckFailures.compute(jobId,</span><br><span class="line">          <span class="keyword">new</span> <span class="type">BiFunction</span>[<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>] &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Int</span>, value: <span class="type">Int</span>): <span class="type">Int</span> = value + <span class="number">1</span></span><br><span class="line">          &#125;)</span><br><span class="line">        <span class="keyword">if</span> (numCheckFailures &lt;= maxFailureNumTasksCheck) &#123;</span><br><span class="line">          messageScheduler.schedule(</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = eventProcessLoop.post(<span class="type">JobSubmitted</span>(jobId, finalRDD, func,</span><br><span class="line">                partitions, callSite, listener, properties))</span><br><span class="line">            &#125;,</span><br><span class="line">            timeIntervalNumTasksCheck,</span><br><span class="line">            <span class="type">TimeUnit</span>.<span class="type">SECONDS</span></span><br><span class="line">          )</span><br><span class="line">          <span class="keyword">return</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// Job failed, clear internal data.</span></span><br><span class="line">          barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line">          listener.jobFailed(e)</span><br><span class="line">          <span class="keyword">return</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">        logWarning(<span class="string">&quot;Creating new stage failed due to exception - job: &quot;</span> + jobId, e)</span><br><span class="line">        listener.jobFailed(e)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Job 已提交，清理内部数据</span></span><br><span class="line">    barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据最后一个调度阶段 finalStage 创建 ActiveJob</span></span><br><span class="line">    <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">    clearCacheLocs()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobSubmissionTime = clock.getTimeMillis()</span><br><span class="line">    jobIdToActiveJob(jobId) = job</span><br><span class="line">    activeJobs += job</span><br><span class="line">    finalStage.setActiveJob(job)</span><br><span class="line">    <span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">    <span class="keyword">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">    <span class="comment">// 向不同 Listener 监听器 发送 SparkListenerJobStart 事件</span></span><br><span class="line">    listenerBus.post(</span><br><span class="line">      <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</span><br><span class="line">    <span class="comment">// 关键：提交 stage，从最后一个 stage 开始提交</span></span><br><span class="line">    submitStage(finalStage)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>从最后一个 Stage 开始提交，不停地递归向前寻找没有父 Stage 的 Stage，当没有某个 Stage 没有父 Stage 依赖，则通过 submitMissingTasks 正式提交该 Stage。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">	<span class="comment">// 提交stage，从没有父 stage 的开始提交</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">    <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">      logDebug(<span class="string">s&quot;submitStage(<span class="subst">$stage</span> (name=<span class="subst">$&#123;stage.name&#125;</span>;&quot;</span> +</span><br><span class="line">        <span class="string">s&quot;jobs=<span class="subst">$&#123;stage.jobIds.toSeq.sorted.mkString(&quot;,&quot;)&#125;</span>))&quot;</span>)</span><br><span class="line">      <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">        <span class="comment">// 获取没有父stage的stage</span></span><br><span class="line">        <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">        logDebug(<span class="string">&quot;missing: &quot;</span> + missing)</span><br><span class="line">        <span class="comment">// 如果没有父 stage</span></span><br><span class="line">        <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">          logInfo(<span class="string">&quot;Submitting &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;), which has no missing parents&quot;</span>)</span><br><span class="line">          <span class="comment">// 关键：提交 Task 任务</span></span><br><span class="line">          submitMissingTasks(stage, jobId.get)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">            <span class="comment">// 递归调用，直到获取没有父stage的stage</span></span><br><span class="line">            submitStage(parent)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// 当前stage有父stage依赖，则加入waitingStages等待父stage提交完</span></span><br><span class="line">          waitingStages += stage</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      abortStage(stage, <span class="string">&quot;No active job for stage &quot;</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>获取当前 Stage 缺失的父 Stage。有点类似树的深度遍历，从 finalStage 开始回溯查找没有父依赖的 Stage 集合。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">	<span class="comment">// 类似树的深度遍历，找到缺失父stage的stage</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMissingParentStages</span></span>(stage: <span class="type">Stage</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> missing = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>]</span><br><span class="line">    <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    <span class="comment">// 存放等待访问的窄依赖RDD</span></span><br><span class="line">    <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">ArrayStack</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(rdd: <span class="type">RDD</span>[_]) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!visited(rdd)) &#123;</span><br><span class="line">        <span class="comment">// 标记访问过的RDD</span></span><br><span class="line">        visited += rdd</span><br><span class="line">        <span class="keyword">val</span> rddHasUncachedPartitions = getCacheLocs(rdd).contains(<span class="type">Nil</span>)</span><br><span class="line">        <span class="keyword">if</span> (rddHasUncachedPartitions) &#123;</span><br><span class="line">          <span class="keyword">for</span> (dep &lt;- rdd.dependencies) &#123;</span><br><span class="line">            dep <span class="keyword">match</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">                <span class="comment">// 通过宽依赖进行stage的划分</span></span><br><span class="line">                <span class="keyword">val</span> mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)</span><br><span class="line">                <span class="comment">// 如果stage是未激活的，则加入missing集合中</span></span><br><span class="line">                <span class="keyword">if</span> (!mapStage.isAvailable) &#123;</span><br><span class="line">                  missing += mapStage</span><br><span class="line">                &#125;</span><br><span class="line">              <span class="keyword">case</span> narrowDep: <span class="type">NarrowDependency</span>[_] =&gt;</span><br><span class="line">                <span class="comment">// 窄依赖直接加入waitingForVisit集合</span></span><br><span class="line">                waitingForVisit.push(narrowDep.rdd)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将当前stage的RDD放入waitingForVisit数组，表示从最后一个RDD开始向前遍历递归树</span></span><br><span class="line">    waitingForVisit.push(stage.rdd)</span><br><span class="line">    <span class="comment">// 深度遍历递归调用</span></span><br><span class="line">    <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">      visit(waitingForVisit.pop())</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 返回父stage</span></span><br><span class="line">    missing.toList</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>如果当前 Stage 没有父依赖，则提交该 Stage。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>) &#123;</span><br><span class="line">    logDebug(<span class="string">&quot;submitMissingTasks(&quot;</span> + stage + <span class="string">&quot;)&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// First figure out the indexes of partition ids to compute.</span></span><br><span class="line">    <span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Use the scheduling pool, job group, description, etc. from an ActiveJob associated</span></span><br><span class="line">    <span class="comment">// with this Stage</span></span><br><span class="line">    <span class="keyword">val</span> properties = jobIdToActiveJob(jobId).properties</span><br><span class="line">		<span class="comment">// 将当前stage加入到运行中stage集合</span></span><br><span class="line">    runningStages += stage</span><br><span class="line"></span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - <span class="number">1</span>)</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        outputCommitCoordinator.stageStart(</span><br><span class="line">          stage = s.id, maxPartitionId = s.rdd.partitions.length - <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> taskIdToLocations: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">TaskLocation</span>]] = <span class="keyword">try</span> &#123;</span><br><span class="line">      stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt; (id, getPreferredLocs(stage.rdd, id))&#125;.toMap</span><br><span class="line">        <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> p = s.partitions(id)</span><br><span class="line">            (id, getPreferredLocs(stage.rdd, p))</span><br><span class="line">          &#125;.toMap</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">        stage.makeNewStageAttempt(partitionsToCompute.size)</span><br><span class="line">        listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo, properties))</span><br><span class="line">        abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">        runningStages -= stage</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (partitionsToCompute.nonEmpty) &#123;</span><br><span class="line">      stage.latestInfo.submissionTime = <span class="type">Some</span>(clock.getTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 向 ListenerBus 发送 SparkListenerStageSubmitted 事件</span></span><br><span class="line">    listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo, properties))</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()</span><br><span class="line">      stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          stage.pendingPartitions.clear()</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">            <span class="keyword">val</span> part = partitions(id)</span><br><span class="line">            stage.pendingPartitions += id</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">              taskBinary, part, locs, properties, serializedTaskMetrics, <span class="type">Option</span>(jobId),</span><br><span class="line">              <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">            <span class="keyword">val</span> part = partitions(p)</span><br><span class="line">            <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">              taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">              <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId,</span><br><span class="line">              stage.rdd.isBarrier())</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">        abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">        runningStages -= stage</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      logInfo(<span class="string">s&quot;Submitting <span class="subst">$&#123;tasks.size&#125;</span> missing tasks from <span class="subst">$stage</span> (<span class="subst">$&#123;stage.rdd&#125;</span>) (first 15 &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;tasks are for partitions <span class="subst">$&#123;tasks.take(15).map(_.partitionId)&#125;</span>)&quot;</span>)</span><br><span class="line">      <span class="comment">// 关键：调用TaskScheduler提交Task任务</span></span><br><span class="line">      taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>(</span><br><span class="line">        tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">// 标记调度阶段已经结束</span></span><br><span class="line">      markStageAsFinished(stage, <span class="type">None</span>)</span><br><span class="line"></span><br><span class="line">      stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          logDebug(<span class="string">s&quot;Stage <span class="subst">$&#123;stage&#125;</span> is actually done; &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;(available: <span class="subst">$&#123;stage.isAvailable&#125;</span>,&quot;</span> +</span><br><span class="line">              <span class="string">s&quot;available outputs: <span class="subst">$&#123;stage.numAvailableOutputs&#125;</span>,&quot;</span> +</span><br><span class="line">              <span class="string">s&quot;partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)&quot;</span>)</span><br><span class="line">          markMapStageJobsAsFinished(stage)</span><br><span class="line">        <span class="keyword">case</span> stage : <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          logDebug(<span class="string">s&quot;Stage <span class="subst">$&#123;stage&#125;</span> is actually done; (partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 提交子Stage</span></span><br><span class="line">      submitWaitingChildStages(stage)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>提交完当前 Stage 后，选择出当前 Stage 的子依赖 Stage，然后依次提交。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitWaitingChildStages</span></span>(parent: <span class="type">Stage</span>) &#123;</span><br><span class="line">		<span class="comment">// 从waitingStages中拿出当前stage的子stages</span></span><br><span class="line">    <span class="keyword">val</span> childStages = waitingStages.filter(_.parents.contains(parent)).toArray</span><br><span class="line">    <span class="comment">// 从waitingStages移除（表示子stages不再等待，即将提交）</span></span><br><span class="line">    waitingStages --= childStages</span><br><span class="line">    <span class="comment">// 遍历提交子stages</span></span><br><span class="line">    <span class="keyword">for</span> (stage &lt;- childStages.sortBy(_.firstJobId)) &#123;</span><br><span class="line">      submitStage(stage)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-5-提交执行任务"><a href="#2-5-提交执行任务" class="headerlink" title="2.5 提交执行任务"></a>2.5 提交执行任务</h2><p>选择了没有父依赖的 Stage 开始提交后，则开始提交该 Stage 的 TaskSet 集合。提交 Task 的关键在于 CoarseGrainedSchedulerBackend 调度后端接口，通过该接口向 Executor 端发送 RPC 请求，</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">submitTasks</span></span>(taskSet: <span class="type">TaskSet</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> tasks = taskSet.tasks</span><br><span class="line">    logInfo(<span class="string">&quot;Adding task set &quot;</span> + taskSet.id + <span class="string">&quot; with &quot;</span> + tasks.length + <span class="string">&quot; tasks&quot;</span>)</span><br><span class="line">    <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">      <span class="keyword">val</span> manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">      <span class="keyword">val</span> stage = taskSet.stageId</span><br><span class="line">      <span class="keyword">val</span> stageTaskSets =</span><br><span class="line">        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">TaskSetManager</span>])</span><br><span class="line"></span><br><span class="line">      stageTaskSets.foreach &#123; <span class="keyword">case</span> (_, ts) =&gt;</span><br><span class="line">        ts.isZombie = <span class="literal">true</span></span><br><span class="line">      &#125;</span><br><span class="line">      stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (!isLocal &amp;&amp; !hasReceivedTask) &#123;</span><br><span class="line">        <span class="comment">// 定时检测是否有足够资源运行Task</span></span><br><span class="line">        starvationTimer.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">TimerTask</span>() &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">            <span class="keyword">if</span> (!hasLaunchedTask) &#123;</span><br><span class="line">              logWarning(<span class="string">&quot;Initial job has not accepted any resources; &quot;</span> +</span><br><span class="line">                <span class="string">&quot;check your cluster UI to ensure that workers are registered &quot;</span> +</span><br><span class="line">                <span class="string">&quot;and have sufficient resources&quot;</span>)</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="keyword">this</span>.cancel()</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;, <span class="type">STARVATION_TIMEOUT_MS</span>, <span class="type">STARVATION_TIMEOUT_MS</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      hasReceivedTask = <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 关键：后台进程调用reviveOffers方法分配资源并且运行</span></span><br><span class="line">    <span class="comment">// backend类型为CoarseGrainedSchedulerBackend</span></span><br><span class="line">    backend.reviveOffers()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>调用 driverEndpoint 发送 ReviveOffers 消息.</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reviveOffers</span></span>() &#123;</span><br><span class="line">    <span class="comment">// driverEndpoint是DriverEndpoint的RpcEndpointRef引用，发送RPC请求ReviveOffers</span></span><br><span class="line">    driverEndpoint.send(<span class="type">ReviveOffers</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>RPC Endpoint 根据消息类型处理 ReviveOffers 消息。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala</span></span><br><span class="line"> <span class="class"><span class="keyword">class</span> <span class="title">DriverEndpoint</span>(<span class="params">override val rpcEnv: <span class="type">RpcEnv</span>, sparkProperties: <span class="type">Seq</span>[(<span class="type">String</span>, <span class="type">String</span></span>)])</span></span><br><span class="line">    <span class="keyword">extends</span> <span class="type">ThreadSafeRpcEndpoint</span> <span class="keyword">with</span> <span class="type">Logging</span> &#123;</span><br><span class="line">   <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">StatusUpdate</span>(executorId, taskId, state, data) =&gt;</span><br><span class="line">				...</span><br><span class="line">      <span class="comment">// 处理ReviveOffers请求</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">ReviveOffers</span> =&gt;</span><br><span class="line">        makeOffers()</span><br><span class="line">			...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>处理消息的具体逻辑，这里可以看到关键的 launchTasks() 方法。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala</span></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeOffers</span></span>() &#123;</span><br><span class="line">      <span class="comment">// Make sure no executor is killed while some task is launching on it</span></span><br><span class="line">      <span class="keyword">val</span> taskDescs = withLock &#123;</span><br><span class="line">        <span class="comment">// Filter out executors under killing</span></span><br><span class="line">        <span class="keyword">val</span> activeExecutors = executorDataMap.filterKeys(executorIsAlive)</span><br><span class="line">        <span class="keyword">val</span> workOffers = activeExecutors.map &#123;</span><br><span class="line">          <span class="keyword">case</span> (id, executorData) =&gt;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">WorkerOffer</span>(id, executorData.executorHost, executorData.freeCores,</span><br><span class="line">              <span class="type">Some</span>(executorData.executorAddress.hostPort))</span><br><span class="line">        &#125;.toIndexedSeq</span><br><span class="line">        <span class="comment">// </span></span><br><span class="line">        scheduler.resourceOffers(workOffers)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (!taskDescs.isEmpty) &#123;</span><br><span class="line">        <span class="comment">// 关键：运行Task</span></span><br><span class="line">        launchTasks(taskDescs)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>launchTasks() 方法是 CoarseGrainedSchedulerBackend 类发出的，属于 Driver 端发出运行 Task 的指令，指令通过调用 Executor 端的 RpcEndpointRef 对象向 Executor 端发送 LaunchTask 消息。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala</span></span><br><span class="line">    <span class="comment">// Launch tasks returned by a set of resource offers</span></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchTasks</span></span>(tasks: <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]]) &#123;</span><br><span class="line">      <span class="keyword">for</span> (task &lt;- tasks.flatten) &#123;</span><br><span class="line">        <span class="keyword">val</span> serializedTask = <span class="type">TaskDescription</span>.encode(task)</span><br><span class="line">        <span class="keyword">if</span> (serializedTask.limit() &gt;= maxRpcMessageSize) &#123;</span><br><span class="line">          <span class="type">Option</span>(scheduler.taskIdToTaskSetManager.get(task.taskId)).foreach &#123; taskSetMgr =&gt;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              <span class="keyword">var</span> msg = <span class="string">&quot;Serialized task %s:%d was %d bytes, which exceeds max allowed: &quot;</span> +</span><br><span class="line">                <span class="string">&quot;spark.rpc.message.maxSize (%d bytes). Consider increasing &quot;</span> +</span><br><span class="line">                <span class="string">&quot;spark.rpc.message.maxSize or using broadcast variables for large values.&quot;</span></span><br><span class="line">              msg = msg.format(task.taskId, task.index, serializedTask.limit(), maxRpcMessageSize)</span><br><span class="line">              taskSetMgr.abort(msg)</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logError(<span class="string">&quot;Exception in error callback&quot;</span>, e)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> executorData = executorDataMap(task.executorId)</span><br><span class="line">          executorData.freeCores -= scheduler.<span class="type">CPUS_PER_TASK</span></span><br><span class="line"></span><br><span class="line">          logDebug(<span class="string">s&quot;Launching task <span class="subst">$&#123;task.taskId&#125;</span> on executor id: <span class="subst">$&#123;task.executorId&#125;</span> hostname: &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;<span class="subst">$&#123;executorData.executorHost&#125;</span>.&quot;</span>)</span><br><span class="line">					<span class="comment">// 关键：通过RPC向Executor发送LaunchTask请求</span></span><br><span class="line">          <span class="comment">// executorData.executorEndpoint类型为CoarseGrainedExecutorBackend</span></span><br><span class="line">          executorData.executorEndpoint.send(<span class="type">LaunchTask</span>(<span class="keyword">new</span> <span class="type">SerializableBuffer</span>(serializedTask)))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>Executor 端的调度后端接口 CoarseGrainedExecutorBackend 根据 LaunchTask 消息类型执行 executor.launchTask() 逻辑。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">CoarseGrainedExecutorBackend</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    override val rpcEnv: <span class="type">RpcEnv</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    driverUrl: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    executorId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    hostname: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    cores: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    userClassPath: <span class="type">Seq</span>[<span class="type">URL</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">    env: <span class="type">SparkEnv</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">ThreadSafeRpcEndpoint</span> <span class="keyword">with</span> <span class="type">ExecutorBackend</span> <span class="keyword">with</span> <span class="type">Logging</span> &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">		...</span><br><span class="line">    <span class="comment">// 根据LaunchTask请求运行任务</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">LaunchTask</span>(data) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (executor == <span class="literal">null</span>) &#123;</span><br><span class="line">        exitExecutor(<span class="number">1</span>, <span class="string">&quot;Received LaunchTask command but executor was null&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> taskDesc = <span class="type">TaskDescription</span>.decode(data.value)</span><br><span class="line">        logInfo(<span class="string">&quot;Got assigned task &quot;</span> + taskDesc.taskId)</span><br><span class="line">        <span class="comment">// 调用Executor对象运行Task</span></span><br><span class="line">        executor.launchTask(<span class="keyword">this</span>, taskDesc)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>Executor 端运行 Task 任务，运行的方式是通过预先创建好的线程池，通过线程池运行 TaskRunner 线程。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：core/src/main/scala/org/apache/spark/executor/Executor.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">launchTask</span></span>(context: <span class="type">ExecutorBackend</span>, taskDescription: <span class="type">TaskDescription</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> tr = <span class="keyword">new</span> <span class="type">TaskRunner</span>(context, taskDescription)</span><br><span class="line">    <span class="comment">// Executor执行Task是通过预先创建好的线程池去执行</span></span><br><span class="line">    runningTasks.put(taskDescription.taskId, tr)</span><br><span class="line">    threadPool.execute(tr)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Executor端预先创建好的线程池，用于执行具体的Task</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> threadPool = &#123;</span><br><span class="line">    <span class="keyword">val</span> threadFactory = <span class="keyword">new</span> <span class="type">ThreadFactoryBuilder</span>()</span><br><span class="line">      .setDaemon(<span class="literal">true</span>)</span><br><span class="line">      .setNameFormat(<span class="string">&quot;Executor task launch worker-%d&quot;</span>)</span><br><span class="line">      .setThreadFactory(<span class="keyword">new</span> <span class="type">ThreadFactory</span> &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">newThread</span></span>(r: <span class="type">Runnable</span>): <span class="type">Thread</span> =</span><br><span class="line">          <span class="keyword">new</span> <span class="type">UninterruptibleThread</span>(r, <span class="string">&quot;unused&quot;</span>) <span class="comment">// thread name will be set by ThreadFactoryBuilder</span></span><br><span class="line">      &#125;)</span><br><span class="line">      .build()</span><br><span class="line">    <span class="type">Executors</span>.newCachedThreadPool(threadFactory).asInstanceOf[<span class="type">ThreadPoolExecutor</span>]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，Task 在 Executor 端便真正启动执行了。</p>
<h1 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h1><ul>
<li><p><a href="https://smartcxr.github.io/2019/05/01/Spark%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86/">Spark作业执行原理</a></p>
</li>
<li><p><a href="https://blog.csdn.net/u011239443/article/details/53911902">深入理解Spark 2.1 Core （二）：DAG调度器的原理与源码分析</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/johnny666888/p/11233982.html">DAG的生成和Stage的划分</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark源码</tag>
        <tag>Spark原理</tag>
      </tags>
  </entry>
  <entry>
    <title>Java如何设计自定义排序器</title>
    <url>/2021/12/19/Java%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%92%E5%BA%8F%E5%99%A8/</url>
    <content><![CDATA[<blockquote>
<p>导读：排序器在 Java 中使用比较频繁，主要用于对进行进行排序或者二次排序，实现用户的自定义排序需求。</p>
</blockquote>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>在使用 Java 无序集合时，经常需要对集合进行排序，此时需要我们自己去实现排序逻辑。接下来就以TreeSet 为例来看看如何对集合进行排序。</p>
<p>TreeSet 对元素排序有两种方式：</p>
<ul>
<li>第一种：复写 Comparable 接口的 compareTo 方法。</li>
<li>第二种：采用自定义Comparator比较器</li>
</ul>
<p>需求：对象Person包含姓名name和年龄age两个属性，按照年龄进行升序排序，如果年龄一致，则按照姓名升序排序。</p>
<h1 id="12-重写Comparable接口"><a href="#12-重写Comparable接口" class="headerlink" title="12. 重写Comparable接口"></a>12. 重写Comparable接口</h1><p>让元素自身具备比较功能，元素就需要实现 Comparable 接口，覆盖 compareTo 方法。</p>
<p>代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Comparable</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重写toString()方法，输出对象时输出格式为：name:age</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name+ <span class="string">&quot;:&quot;</span> + age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">        Person p = (Person) o;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据年龄升序排序</span></span><br><span class="line">      <span class="keyword">int</span> temp = <span class="keyword">this</span>.age - p.age;</span><br><span class="line">      <span class="keyword">return</span> temp == <span class="number">0</span> ? <span class="keyword">this</span>.name.compareTo(p.name):temp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试用例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeSetDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        TreeSet&lt;Person&gt; ts = <span class="keyword">new</span> TreeSet();</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">20</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;lisi&quot;</span>, <span class="number">20</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;wangwu&quot;</span>, <span class="number">21</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhouqi&quot;</span>, <span class="number">29</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhaoliu&quot;</span>, <span class="number">28</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Person person : ts) &#123;</span><br><span class="line">            System.out.println(person);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 结果输出</span></span><br><span class="line">lisi:<span class="number">20</span></span><br><span class="line">zhangsan:<span class="number">20</span></span><br><span class="line">wangwu:<span class="number">21</span></span><br><span class="line">zhaoliu:<span class="number">28</span></span><br><span class="line">zhouqi:<span class="number">29</span></span><br></pre></td></tr></table></figure>

<p>但是如果不要按照对象中具备的自然顺序进行排序，即对象中不具备自然顺序该如何处理呢？就需要采用下面的第二种方式。</p>
<h1 id="3-自定义Comparator比较器"><a href="#3-自定义Comparator比较器" class="headerlink" title="3. 自定义Comparator比较器"></a>3. 自定义Comparator比较器</h1><p>让集合自身具备比较功能，即在集合对象创建时，由此想到TreeSet的构造方法: <code>new TreeSet(Comparator&lt;? super E&gt; comparator)</code>。为此需要定义一个类实现Comparator接口，覆盖 compare 方法，将该类对象作为参数传递给TreeSet集合的构造函数。</p>
<p>代码如下：</p>
<p>PersonNoComparator 类不用复写 Comparable 接口的 compareTo 方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PersonNoComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name+<span class="string">&quot;:&quot;</span> + age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PersonNoComparator</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>设计自定义排序器，并复写compare方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ComparatorByAge</span> <span class="keyword">implements</span> <span class="title">Comparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据年龄升序排序</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Object o1, Object o2)</span> </span>&#123;</span><br><span class="line">        Person p1 = (Person) o1;</span><br><span class="line">        Person p2 = (Person) o2;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> tmp = p1.getAge() - p2.getAge();</span><br><span class="line">        <span class="keyword">return</span> tmp == <span class="number">0</span> ? p1.getName().compareTo(p2.getName()) : tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试用例：</p>
<p>与第一种方式测试不同的地方是，在构造TreeSet方法时直接按照ComparatorByAge排序器排序。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeSetDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ts = <span class="keyword">new</span> TreeSet(<span class="keyword">new</span> ComparatorByAge());</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">20</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;lisi&quot;</span>, <span class="number">20</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;wangwu&quot;</span>, <span class="number">21</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhouqi&quot;</span>, <span class="number">29</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhaoliu&quot;</span>, <span class="number">28</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Person person : ts) &#123;</span><br><span class="line">            System.out.println(person);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 结果输出</span></span><br><span class="line">lisi:<span class="number">20</span></span><br><span class="line">zhangsan:<span class="number">20</span></span><br><span class="line">wangwu:<span class="number">21</span></span><br><span class="line">zhaoliu:<span class="number">28</span></span><br><span class="line">zhouqi:<span class="number">29</span></span><br></pre></td></tr></table></figure>

<p>可以发现，两种方式实现的排序结果是一致的。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol>
<li><a href="https://www.jianshu.com/p/9334567d0912">TreeSet排序compareTo、Comparator</a></li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Java阻塞队列LinkedBlockingQueue</title>
    <url>/2021/12/19/Java%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97LinkedBlockingQueue/</url>
    <content><![CDATA[<blockquote>
<p>导读：最近在阅读 YARN 和 Spark 源码过程中，发现内部消息的处理机制（生产者-阻塞队列-消费者）都是通过阻塞队列来实现的，为弄清阻塞队列中生产事件和消费事件的不同操作，本文单独介绍 Java 中的阻塞队列 LinkedBlockingQueue 的使用。</p>
</blockquote>
<h2 id="1-什么是阻塞队列"><a href="#1-什么是阻塞队列" class="headerlink" title="1. 什么是阻塞队列"></a>1. 什么是阻塞队列</h2><p>阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作支持阻塞的插入和移除方法。</p>
<ul>
<li>支持阻塞的插入方法：当队列满时，队列会阻塞插入元素的线程，直到队列不满。</li>
<li>支持阻塞的移除方法：当队列为空时，获取元素的线程会等待队列变为非空。</li>
</ul>
<p>阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。阻塞队列就是生产者用来存放元素、消费者用来获取元素的容器。</p>
<p>在阻塞队列不可用时，这两个附加操作提供了4种处理方式，如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2021/12/19/db1bd5041f5de31c285a88c4c6ca7c05-1635924725139-70fd0f74-fab6-4498-874d-5ff54ca0f44f-41a6e1.png" alt="img"></p>
<ul>
<li><p>抛出异常：当队列满时，如果再往队列里插入元素，会抛出 IllegalArgumentException 异常。当队列空时，从队列里获取元素会抛出 NoSuchElementException 异常。</p>
</li>
<li><p>返回特殊值：当往队列插入元素时，会返回元素是否插入成功，成功返回true。如果是移除方法，则是从队列里取出一个元素，如果没有则返回null。</p>
</li>
<li><p>一直阻塞：当阻塞队列满时，如果生产者线程往队列里 put 元素，队列会一直阻塞生产者线程，直到队列可用或者响应中断退出。当队列空时，如果消费者从队列里 take 元素，队列会阻塞住消费者线程，直到队列不为空。</p>
</li>
<li><p>超时退出：当阻塞队列满时，队列会阻塞生产者线程一段时间，如果超过一定的时间，生产者线程就会退出。</p>
</li>
</ul>
<blockquote>
<p>Tips：如果是无界阻塞队列，队列不可能会出现满的情况，所以使用 add/put/offer 方法永远不会被阻塞，而且使用 offer 方法时，该方法永远返回 true。</p>
</blockquote>
<h2 id="2-基于阻塞队列的生产消费模式"><a href="#2-基于阻塞队列的生产消费模式" class="headerlink" title="2. 基于阻塞队列的生产消费模式"></a>2. 基于阻塞队列的生产消费模式</h2><h3 id="2-1-生产者消费者逻辑"><a href="#2-1-生产者消费者逻辑" class="headerlink" title="2.1 生产者消费者逻辑"></a>2.1 生产者消费者逻辑</h3><p>LinkedBlockingQueue 实现是线程安全的阻塞队列，实现了先进先出等特性，是生产者消费者模式的首选，LinkedBlockingQueue 可以指定队列容量，如果不指定，默认最大是 Integer.MAX_VALUE。用例以指定容量的有界队列为主，构造一个容量为 5 的有界的阻塞队列 LinkedBlockingQueue，并创建独立的生产者线程和消费者线程，生产者以 1s 的间隔持续的生产数据，消费者以 2s 的间隔消费数据，保证消费者的消费速度跟不上生产者的生产速度，让阻塞队列达到容量限制。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BlockQueue</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	BlockingQueue&lt;Integer&gt; queue = <span class="keyword">new</span> LinkedBlockingQueue&lt;&gt;(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 生产者</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">produce</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">			<span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">			<span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">				<span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">					<span class="keyword">try</span> &#123;</span><br><span class="line">                        <span class="comment">// 向阻塞队列中添加元素</span></span><br><span class="line">						queue.put(++i);</span><br><span class="line">						Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">                        <span class="comment">// 生产 15 个数据</span></span><br><span class="line">                        <span class="keyword">if</span> (i == <span class="number">15</span>) &#123;</span><br><span class="line">							<span class="keyword">break</span>;</span><br><span class="line">						&#125;</span><br><span class="line">					&#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">						e.printStackTrace();</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;, <span class="string">&quot;produce-thread&quot;</span>).start();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 消费者</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">consumer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">			<span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">				<span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">					<span class="keyword">try</span> &#123;</span><br><span class="line">						System.out.println(<span class="string">&quot;queue size = &quot;</span> + queue.size());</span><br><span class="line">						<span class="comment">// 从队列中取数据</span></span><br><span class="line">                        Integer ele = queue.take();</span><br><span class="line">						System.out.println(Thread.currentThread().getName() + <span class="string">&quot; = &quot;</span> + ele);</span><br><span class="line">						Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">					&#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">						e.printStackTrace();</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;, <span class="string">&quot;consumer-thread&quot;</span>).start();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		BlockQueue obj = <span class="keyword">new</span> BlockQueue();</span><br><span class="line">		obj.produce();</span><br><span class="line">		obj.consumer();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-2-插入方法和移除方法组合用例"><a href="#2-2-插入方法和移除方法组合用例" class="headerlink" title="2.2 插入方法和移除方法组合用例"></a>2.2 插入方法和移除方法组合用例</h3><p>构造不同插入方法和移除方法的测试用例，看看阻塞队列的行为表现有什么不一样。添加元素的方法有 put/offer/add，移除元素的方法有 take/poll/remove，具体介绍如下。</p>
<p>插入方法：</p>
<ul>
<li><p>put：若队列已满了则发生阻塞，等待元素加入队列。</p>
</li>
<li><p>offer：若发现队列已满无法添加的话，会直接返回false。</p>
</li>
<li><p>add：若发现队列已满直接抛出异常。</p>
</li>
</ul>
<p>移除方法：</p>
<ul>
<li><p>take：若队列为空，发生阻塞，等待有元素以取出。</p>
</li>
<li><p>poll：若队列为空，返回null。</p>
</li>
<li><p>remove：若队列为空，抛出 NoSuchElementException 异常。</p>
</li>
</ul>
<h4 id="2-2-1-put和take组合"><a href="#2-2-1-put和take组合" class="headerlink" title="2.2.1 put和take组合"></a>2.2.1 put和take组合</h4><p>运行结果如下：</p>
<ul>
<li>队列满时，put 方法阻塞等待队列有空闲位以添加元素；</li>
<li>队列空时，take 方法阻塞等待队列有新元素抵达以取出元素。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">queue size = 1</span><br><span class="line">consumer-thread = 1</span><br><span class="line">queue size = 1</span><br><span class="line">consumer-thread = 2</span><br><span class="line">queue size = 2</span><br><span class="line">consumer-thread = 3</span><br><span class="line">queue size = 3</span><br><span class="line">consumer-thread = 4</span><br><span class="line">queue size = 4</span><br><span class="line">consumer-thread = 5</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 6</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 7</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 8</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 9</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 10</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 11</span><br><span class="line">queue size = 4</span><br><span class="line">consumer-thread = 12</span><br><span class="line">queue size = 3</span><br><span class="line">consumer-thread = 13</span><br><span class="line">queue size = 2</span><br><span class="line">consumer-thread = 14</span><br><span class="line">queue size = 1</span><br><span class="line">consumer-thread = 15</span><br><span class="line">queue size = 0</span><br><span class="line">（生产者停止生产，消费者阻塞等待中～～）</span><br></pre></td></tr></table></figure>

<h4 id="2-2-2-offer和take组合"><a href="#2-2-2-offer和take组合" class="headerlink" title="2.2.2 offer和take组合"></a>2.2.2 offer和take组合</h4><p>为让 offer 添加操作效果更明显，将消费者线程 consumer 的线程等待时间调整为 4s (<code>Thread.sleep(4000);</code>)。</p>
<p>运行结果如下：</p>
<ul>
<li>队列满时，offer 方法发现队列已满添加元素返回 false，元素丢弃，不添加到队列中；</li>
<li>队列空时，take 方法阻塞等待队列有新元素抵达以取出元素。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">queue size = 1</span><br><span class="line">consumer-thread = 1</span><br><span class="line">queue size = 4</span><br><span class="line">consumer-thread = 2</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 3</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 4</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 5</span><br><span class="line">queue size = 4</span><br><span class="line">consumer-thread = 6</span><br><span class="line">queue size = 3</span><br><span class="line">consumer-thread = 7</span><br><span class="line">queue size = 2</span><br><span class="line">consumer-thread = 10</span><br><span class="line">queue size = 1</span><br><span class="line">consumer-thread = 13</span><br><span class="line">queue size = 0</span><br><span class="line">（消费者会丢失部分数据）</span><br></pre></td></tr></table></figure>

<h4 id="2-2-3-add和take组合"><a href="#2-2-3-add和take组合" class="headerlink" title="2.2.3 add和take组合"></a>2.2.3 add和take组合</h4><p>运行结果如下：</p>
<ul>
<li>队列满时，add 方法直接抛异常，生产者无法添加元素，消费者处理完队列剩余元素；</li>
<li>队列空时，take 方法阻塞等待队列有新元素抵达以取出元素。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">queue size = 1</span><br><span class="line">consumer-thread = 1</span><br><span class="line">queue size = 2</span><br><span class="line">consumer-thread = 2</span><br><span class="line">queue size = 2</span><br><span class="line">consumer-thread = 3</span><br><span class="line">queue size = 3</span><br><span class="line">consumer-thread = 4</span><br><span class="line">queue size = 4</span><br><span class="line">consumer-thread = 5</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 6</span><br><span class="line">java.lang.IllegalStateException: Queue full</span><br></pre></td></tr></table></figure>

<h4 id="2-2-4-put和poll组合"><a href="#2-2-4-put和poll组合" class="headerlink" title="2.2.4 put和poll组合"></a>2.2.4 put和poll组合</h4><p>生产者生产数据量改为 10，运行结果如下：</p>
<ul>
<li>队列满时，put 方法阻塞等待队列有空闲位以添加元素；</li>
<li>队列空时，poll 方法返回 null。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">queue size = 1</span><br><span class="line">consumer-thread = 1</span><br><span class="line">queue size = 2</span><br><span class="line">consumer-thread = 2</span><br><span class="line">queue size = 3</span><br><span class="line">consumer-thread = 3</span><br><span class="line">queue size = 4</span><br><span class="line">consumer-thread = 4</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 5</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 6</span><br><span class="line">queue size = 4</span><br><span class="line">consumer-thread = 7</span><br><span class="line">queue size = 3</span><br><span class="line">consumer-thread = 8</span><br><span class="line">queue size = 2</span><br><span class="line">consumer-thread = 9</span><br><span class="line">queue size = 1</span><br><span class="line">consumer-thread = 10</span><br><span class="line">queue size = 0</span><br><span class="line">consumer-thread = null</span><br><span class="line">queue size = 0</span><br><span class="line">consumer-thread = null</span><br></pre></td></tr></table></figure>

<h4 id="2-2-5-put和remove组合"><a href="#2-2-5-put和remove组合" class="headerlink" title="2.2.5 put和remove组合"></a>2.2.5 put和remove组合</h4><p>生产者生产数据量改为 10，运行结果如下：</p>
<ul>
<li>队列满时，put 方法阻塞等待队列有空闲位以添加元素；</li>
<li>队列空时，remove 方法抛 NoSuchElementException 异常。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">queue size = 1</span><br><span class="line">consumer-thread = 1</span><br><span class="line">queue size = 2</span><br><span class="line">consumer-thread = 2</span><br><span class="line">queue size = 2</span><br><span class="line">consumer-thread = 3</span><br><span class="line">queue size = 4</span><br><span class="line">consumer-thread = 4</span><br><span class="line">queue size = 4</span><br><span class="line">consumer-thread = 5</span><br><span class="line">queue size = 5</span><br><span class="line">consumer-thread = 6</span><br><span class="line">queue size = 4</span><br><span class="line">consumer-thread = 7</span><br><span class="line">queue size = 3</span><br><span class="line">consumer-thread = 8</span><br><span class="line">queue size = 2</span><br><span class="line">consumer-thread = 9</span><br><span class="line">queue size = 1</span><br><span class="line">consumer-thread = 10</span><br><span class="line">queue size = 0</span><br><span class="line">java.util.NoSuchElementException</span><br></pre></td></tr></table></figure>

<h3 id="2-3-小结"><a href="#2-3-小结" class="headerlink" title="2.3 小结"></a>2.3 小结</h3><p>至此，通过介绍和用例场景，对 LinkedBlockingQueue 阻塞队列的添加元素 add/put/offer 和移除元素 remove/poll/take 两类操作应该比较熟悉。简单来讲，对于有界的阻塞队列，put 和 take 组合是最完美的组合，队列满时 put 阻塞等待元素添加，队列空时 take 阻塞等待元素抵达以取出，不会出现数据丢失或抛异常的情况。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://juejin.cn/post/6844903640709201934">Java并发系列——阻塞队列</a></li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title>理解Java排序器的升序/降序</title>
    <url>/2021/12/19/%E7%90%86%E8%A7%A3Java%E6%8E%92%E5%BA%8F%E5%99%A8%E7%9A%84%E5%8D%87%E5%BA%8F-%E9%99%8D%E5%BA%8F/</url>
    <content><![CDATA[<blockquote>
<p>导读：在设计 Java 自定义排序器时，经常需要对对象的某些字段进行升序和降序，但何时返回升序结果何时返回降序经过经常容易弄混，本文主要介绍如何根据排序规则确定是升序还是降序。虽然写用例测试是不错的方式，但理解这个知识点还是很有必要的。</p>
</blockquote>
<h1 id="1-如何确定升序-降序"><a href="#1-如何确定升序-降序" class="headerlink" title="1. 如何确定升序/降序"></a>1. 如何确定升序/降序</h1><p>Java中在进行对象排序时，设计的排序器经常会对两个对象按照一定的排序规则排序，可如何确定排序规则是升序还是降序呢？笔者整理了一个简单的方法来确定排序规则。</p>
<p>o1和o2是需要表示排序的两个对象，假定比较前的默认顺序为 [o1, o2]，是升序还是降序暂时不做考虑，完全根据返回值结果表示是否需要调整当前的排序顺序，便能够理解排序的真正逻辑，以确定是升序排序还是降序排序。</p>
<p>假设我们的比较器规则如下：o1对象作为比较的前者，o2对象作为排序的后者，即比较方式为 [o1 - o2]或者 [o1.compareTo(o2)]。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ComparatorByAge</span> <span class="keyword">implements</span> <span class="title">Comparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据年龄和姓名排序</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Object o1, Object o2)</span> </span>&#123;</span><br><span class="line">        Person p1 = (Person) o1;</span><br><span class="line">        Person p2 = (Person) o2;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> tmp = p1.getAge() - p2.getAge();</span><br><span class="line">        <span class="keyword">return</span> tmp == <span class="number">0</span> ? p1.getName().compareTo(p2.getName()) : tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>升序规则：</p>
<ul>
<li>o1 &gt; o2，返回正数，true，表示需要调整顺序，升序。</li>
<li>o1 &lt; o2，返回负数，false，表示不需要调整顺序，升序。</li>
</ul>
<p>降序规则：</p>
<ul>
<li>o1 &gt; o2，返回负数，false，表示不需要调整顺序，降序。</li>
<li>o1 &lt; o2，返回正数，true，表示需要调整顺序，降序。 </li>
</ul>
<p>不排序规则：</p>
<ul>
<li>o1 = o2，返回0，按当前顺序即可，或者比较其他参数。</li>
</ul>
<h1 id="2-实际用例"><a href="#2-实际用例" class="headerlink" title="2. 实际用例"></a>2. 实际用例</h1><p>Person 是定义的需要排序的对象，包括年龄和姓名两个字段。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Person对象构建</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Comparable</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重写toString()方法，输出对象时输出格式为：name:age</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name+ <span class="string">&quot;:&quot;</span> + age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Person自带的排序规则</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">        Person p = (Person) o;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据年龄进行排序</span></span><br><span class="line">      <span class="keyword">int</span> temp = <span class="keyword">this</span>.age - p.age;</span><br><span class="line">      <span class="keyword">return</span> temp == <span class="number">0</span> ? <span class="keyword">this</span>.name.compareTo(p.name):temp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>排序规则一：定义升序排序器，先按年龄升序，再按姓名首字母升序。</strong></p>
<ul>
<li>比如 o1对象为 (zhangshan:20) ，o2 对象为 (wangwu:21) ，默认排序 [o1, o2]，由于 o1.age &lt; o2.age，比较结果返回负数，false，表示不需要调整规则，按年龄升序排序，最终排序结果为 [zhangshan:20, wangwu:21]。</li>
<li>比如 o1 对象为 (zhangshan:20) ，o2对象为 (lisi:20)，默认排序 [o1,o2]，由于 o1.age = o2.age，比较结果为0，不按年龄排序，进一步比较name，由于 o1.name &gt; o2.name，返回 true，调整排序规则，即按字母升序排序，最终排序结果为 [lisi:20, zhangshan:20]。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义升序排序器</span></span><br><span class="line"><span class="comment"> * 升序：先按年龄升序，再按姓名首字母升序</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ComparatorByAge</span> <span class="keyword">implements</span> <span class="title">Comparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据年龄排序</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Object o1, Object o2)</span> </span>&#123;</span><br><span class="line">        Person p1 = (Person) o1;</span><br><span class="line">        Person p2 = (Person) o2;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> tmp = p1.getAge() - p2.getAge();</span><br><span class="line">        <span class="keyword">return</span> tmp == <span class="number">0</span> ? p1.getName().compareTo(p2.getName()) : tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>排序规则二：定义降序排序器，先按年龄降序，再按姓名首字母降序。</strong>和升序的唯一区别就是返回结果的参数前添加了一个负号。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义降序排序器</span></span><br><span class="line"><span class="comment"> * 降序：先按年龄降序，再按姓名首字母降序</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ComparatorByAge2</span> <span class="keyword">implements</span> <span class="title">Comparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据年龄排序</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Object o1, Object o2)</span> </span>&#123;</span><br><span class="line">        Person p1 = (Person) o1;</span><br><span class="line">        Person p2 = (Person) o2;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> tmp = p1.getAge() - p2.getAge();</span><br><span class="line">        <span class="keyword">return</span> tmp == <span class="number">0</span> ? -(p1.getName().compareTo(p2.getName())) : -tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>排序 main() 方法实现，查看两种排序器的排序结果。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeSetDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        TreeSet&lt;Person&gt; ts = <span class="keyword">new</span> TreeSet(<span class="keyword">new</span> ComparatorByAge());</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">20</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;wangwu&quot;</span>, <span class="number">21</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;lisi&quot;</span>, <span class="number">20</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhouqi&quot;</span>, <span class="number">29</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhaoliu&quot;</span>, <span class="number">28</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Person person : ts) &#123;</span><br><span class="line">            System.out.println(person);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/* 结果输出</span></span><br><span class="line"><span class="comment">        lisi:20</span></span><br><span class="line"><span class="comment">        zhangsan:20</span></span><br><span class="line"><span class="comment">        wangwu:21</span></span><br><span class="line"><span class="comment">        zhaoliu:28</span></span><br><span class="line"><span class="comment">        zhouqi:29</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line"></span><br><span class="line">        ts = <span class="keyword">new</span> TreeSet(<span class="keyword">new</span> ComparatorByAge2());</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">20</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;wangwu&quot;</span>, <span class="number">21</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;lisi&quot;</span>, <span class="number">20</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhouqi&quot;</span>, <span class="number">29</span>));</span><br><span class="line">        ts.add(<span class="keyword">new</span> Person(<span class="string">&quot;zhaoliu&quot;</span>, <span class="number">28</span>));</span><br><span class="line">        <span class="keyword">for</span> (Person person : ts) &#123;</span><br><span class="line">            System.out.println(person);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/* 结果输出</span></span><br><span class="line"><span class="comment">        zhouqi:29</span></span><br><span class="line"><span class="comment">        zhaoliu:28</span></span><br><span class="line"><span class="comment">        wangwu:21</span></span><br><span class="line"><span class="comment">        zhangsan:20</span></span><br><span class="line"><span class="comment">        lisi:20</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark insert overwrite权限被修改问题分析</title>
    <url>/2022/01/22/Spark-insert-overwrite%E6%9D%83%E9%99%90%E8%A2%AB%E4%BF%AE%E6%94%B9%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<blockquote>
<p>源码版本：Apache Spark 2.4.7</p>
<p>导读：Spark 2.4.7 执行非分区表 insert overwrite 操作时目录的所属 owner 和 目录 acl 权限信息被修改（分区表操作不会出行该问题），导致 A 用户创建的表路径，实际拥有该路径读写权限的 B 用户无法正常读写，主要原因是非分区表执行 insert overwrite 操作时会删除表路径然后重建，而重建的 umask-mode 信息被覆盖导致创建的路径 mask 异常，B 用户实际有权限也无法读写。</p>
</blockquote>
<h1 id="1-问题复现"><a href="#1-问题复现" class="headerlink" title="1. 问题复现"></a>1. 问题复现</h1><p>操作 sql 建表后，正常的表路径 owner 和 acl 信息，其实 r-x 组属于读权限 acl 组，rwx 属于读写权限 acl 组。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建非分区表</span></span><br><span class="line">create table test.tmp_table_nopar_kw(id int) stored as orc;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 表acl信息</span></span><br><span class="line">[hdfs@hostname1:/var/lib/hadoop-hdfs]</span><br><span class="line"><span class="meta">$</span><span class="bash">  hdfs dfs -getfacl hdfs://nameservice/hive/warehouse/<span class="built_in">test</span>//tmp_table_nopar_kw</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> file: hdfs://nameservice/hive/warehouse/<span class="built_in">test</span>//tmp_table_nopar_kw</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> owner: wangkang</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> group: supergroup</span></span><br><span class="line">user::rwx</span><br><span class="line">group::rwx</span><br><span class="line">group:0a51e5bffea9bf698089a6060728c489:rwx</span><br><span class="line">group:bb856a73c4e759332ac2b4159766ff67:r-x</span><br><span class="line">group:hadoopadmin:rwx</span><br><span class="line">mask::rwx</span><br><span class="line">other::--x</span><br><span class="line">default:user::rwx</span><br><span class="line">default:group::rwx</span><br><span class="line">default:group:0a51e5bffea9bf698089a6060728c489:rwx</span><br><span class="line">default:group:bb856a73c4e759332ac2b4159766ff67:r-x</span><br><span class="line">default:group:hadoopadmin:rwx</span><br><span class="line">default:mask::rwx</span><br><span class="line">default:other::--x</span><br></pre></td></tr></table></figure>

<p>当使用另外一个有目录读写权限的用户 kwang 执行 insert overwrite 操作时，表路径的 owner 和 acl 信息都被修改。effective:r-x 表示实际 rwx 读写权限组的有效权限只有 r-x，这样就导致原来的表所属用户 wangkang 没有该路径的写权限。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kwang用户执行操作</span></span><br><span class="line">insert overwrite table test.tmp_table_nopar_kw values(2);</span><br><span class="line"></span><br><span class="line">[hdfs@hostname1:/var/lib/hadoop-hdfs]</span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -getfacl hdfs://nameservice/hive/warehouse/<span class="built_in">test</span>/tmp_table_nopar_kw</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> file: hdfs://nameservice/hive/warehouse/<span class="built_in">test</span>/tmp_table_nopar_kw</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> owner: kwang</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> group: supergroup</span></span><br><span class="line">user::rwx</span><br><span class="line">group::rwx      #effective:r-x</span><br><span class="line">group:0a51e5bffea9bf698089a6060728c489:rwx      #effective:r-x</span><br><span class="line">group:bb856a73c4e759332ac2b4159766ff67:r-x</span><br><span class="line">group:hadoopadmin:rwx   #effective:r-x</span><br><span class="line">mask::r-x</span><br><span class="line">other::--x</span><br><span class="line">default:user::rwx</span><br><span class="line">default:group::rwx</span><br><span class="line">default:group:0a51e5bffea9bf698089a6060728c489:rwx</span><br><span class="line">default:group:bb856a73c4e759332ac2b4159766ff67:r-x</span><br><span class="line">default:group:hadoopadmin:rwx</span><br><span class="line">default:mask::rwx</span><br><span class="line">default:other::--x</span><br></pre></td></tr></table></figure>

<h1 id="2-原因分析"><a href="#2-原因分析" class="headerlink" title="2. 原因分析"></a>2. 原因分析</h1><p>根据问题中的表现，该表路径的 owner 和 acl 信息被修改，问题出在表路径创建时的逻辑。查看 namenode 的 hdfs-audit.log 日志，发现 insert overwrite 操作时会先删除掉表路径，然后重新创建。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2022-01-19 09:54:27,618 INFO FSNamesystem.audit: allowed=true   ugi=hdfs (auth:SIMPLE)  ip=/10.110.3.14 cmd=delete      src=/hive/warehouse/bdsp_test.db/tmp_table_nopar_kw     dst=null        perm=null       proto=rpc</span><br><span class="line">2022-01-19 09:54:27,769 INFO FSNamesystem.audit: allowed=true   ugi=hdfs (auth:SIMPLE)  ip=/10.110.3.14 cmd=mkdirs      src=/hive/warehouse/bdsp_test.db/tmp_table_nopar_kw/_temporary/0        dst=null        perm=hdfs:hive:rwxrwx--x   proto=rpc</span><br></pre></td></tr></table></figure>

<p>接下来的关键就是看该表路径是在哪里删除以及如何创建的。通过 explain 查看对应 sql Physical Plan，可以确定 sql 最后执行的是 InsertIntoHadoopFsRelationCommand。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-sql (default)&gt;  explain insert overwrite table tmp_table_nopar_kw values(2);</span><br><span class="line">== Physical Plan ==</span><br><span class="line">Execute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand hdfs://nameservice/hive/warehouse/bdsp_test.db/tmp_table_nopar_kw, false, ORC, Map(serialization.format -&gt; 1), Overwrite, CatalogTable(</span><br><span class="line">Database: bdsp_test</span><br><span class="line">Table: tmp_table_nopar_kw</span><br><span class="line">Owner: hdfs</span><br><span class="line">Created Time: Tue Jan 18 15:11:36 CST 2022</span><br><span class="line">Last Access: Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">Created By: Spark 2.4.7</span><br><span class="line">Type: MANAGED</span><br><span class="line">Provider: hive</span><br><span class="line">Table Properties: [transient_lastDdlTime=1642489896]</span><br><span class="line">Location: hdfs://nameservice/hive/warehouse/bdsp_test.db/tmp_table_nopar_kw</span><br><span class="line">Serde Library: org.apache.hadoop.hive.ql.io.orc.OrcSerde</span><br><span class="line">InputFormat: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat</span><br><span class="line">OutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat</span><br><span class="line">Storage Properties: [serialization.format=1]</span><br><span class="line">Partition Provider: Catalog</span><br><span class="line">Schema: root</span><br><span class="line">-- id: integer (nullable = true)</span><br><span class="line">), org.apache.spark.sql.execution.datasources.InMemoryFileIndex@d6b27b97, [id]</span><br><span class="line">+- LocalTableScan [id#22]</span><br><span class="line">Time taken: 0.042 seconds, Fetched 1 row(s)</span><br></pre></td></tr></table></figure>

<p>InsertIntoHadoopFsRelationCommand 逻辑中有两个关键点，关键点1是删除非分区表对应的 HDFS 路径，关键点2在 overwrite 写数据时重新创建表目录，然后写数据。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertIntoHadoopFsRelationCommand</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    outputPath: <span class="type">Path</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    staticPartitions: <span class="type">TablePartitionSpec</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    ifPartitionNotExists: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    partitionColumns: <span class="type">Seq</span>[<span class="type">Attribute</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">    bucketSpec: <span class="type">Option</span>[<span class="type">BucketSpec</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">    fileFormat: <span class="type">FileFormat</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">    query: <span class="type">LogicalPlan</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    mode: <span class="type">SaveMode</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    catalogTable: <span class="type">Option</span>[<span class="type">CatalogTable</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">    fileIndex: <span class="type">Option</span>[<span class="type">FileIndex</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">    outputColumnNames: <span class="type">Seq</span>[<span class="type">String</span>]</span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">DataWritingCommand</span> &#123;</span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.catalyst.catalog.<span class="type">ExternalCatalogUtils</span>.escapePathName</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(sparkSession: <span class="type">SparkSession</span>, child: <span class="type">SparkPlan</span>): <span class="type">Seq</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">      ...</span><br><span class="line">      <span class="keyword">val</span> doInsertion = (mode, pathExists) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> (<span class="type">SaveMode</span>.<span class="type">ErrorIfExists</span>, <span class="literal">true</span>) =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(<span class="string">s&quot;path <span class="subst">$qualifiedOutputPath</span> already exists.&quot;</span>)</span><br><span class="line">      <span class="keyword">case</span> (<span class="type">SaveMode</span>.<span class="type">Overwrite</span>, <span class="literal">true</span>) =&gt;</span><br><span class="line">        <span class="keyword">if</span> (ifPartitionNotExists &amp;&amp; matchingPartitions.nonEmpty) &#123;</span><br><span class="line">          <span class="literal">false</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dynamicPartitionOverwrite) &#123;</span><br><span class="line">          <span class="comment">// For dynamic partition overwrite, do not delete partition directories ahead.</span></span><br><span class="line">          <span class="literal">true</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 关键点1：这里会删除非分区表的HDFS目录</span></span><br><span class="line">          deleteMatchingPartitions(fs, qualifiedOutputPath, customPartitionLocations, committer)</span><br><span class="line">          <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> (<span class="type">SaveMode</span>.<span class="type">Append</span>, _) | (<span class="type">SaveMode</span>.<span class="type">Overwrite</span>, _) | (<span class="type">SaveMode</span>.<span class="type">ErrorIfExists</span>, <span class="literal">false</span>) =&gt;</span><br><span class="line">        <span class="literal">true</span></span><br><span class="line">      <span class="keyword">case</span> (<span class="type">SaveMode</span>.<span class="type">Ignore</span>, exists) =&gt;</span><br><span class="line">        !exists</span><br><span class="line">      <span class="keyword">case</span> (s, exists) =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s&quot;unsupported save mode <span class="subst">$s</span> (<span class="subst">$exists</span>)&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">     	...</span><br><span class="line">      <span class="keyword">val</span> updatedPartitionPaths =</span><br><span class="line">        <span class="comment">// 关键点2：内部会新建写数据的临时目录，并commit数据到目录</span></span><br><span class="line">        <span class="type">FileFormatWriter</span>.write(</span><br><span class="line">          sparkSession = sparkSession,</span><br><span class="line">          plan = child,</span><br><span class="line">          fileFormat = fileFormat,</span><br><span class="line">          committer = committer,</span><br><span class="line">          outputSpec = <span class="type">FileFormatWriter</span>.<span class="type">OutputSpec</span>(</span><br><span class="line">            committerOutputPath.toString, customPartitionLocations, outputColumns),</span><br><span class="line">          hadoopConf = hadoopConf,</span><br><span class="line">          partitionColumns = partitionColumns,</span><br><span class="line">          bucketSpec = bucketSpec,</span><br><span class="line">          statsTrackers = <span class="type">Seq</span>(basicWriteJobStatsTracker(hadoopConf)),</span><br><span class="line">          options = options)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>关键点1的删除表路径如下。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteMatchingPartitions</span></span>(</span><br><span class="line">      fs: <span class="type">FileSystem</span>,</span><br><span class="line">      qualifiedOutputPath: <span class="type">Path</span>,</span><br><span class="line">      customPartitionLocations: <span class="type">Map</span>[<span class="type">TablePartitionSpec</span>, <span class="type">String</span>],</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// first clear the path determined by the static partition keys (e.g. /table/foo=1)</span></span><br><span class="line">    <span class="keyword">val</span> staticPrefixPath = qualifiedOutputPath.suffix(staticPartitionPrefix)</span><br><span class="line">    <span class="comment">// 删除表路径</span></span><br><span class="line">    <span class="keyword">if</span> (fs.exists(staticPrefixPath) &amp;&amp; !committer.deleteWithJob(fs, staticPrefixPath, <span class="literal">true</span>)) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">s&quot;Unable to clear output &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;directory <span class="subst">$staticPrefixPath</span> prior to writing to it&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/FileCommitProtocol.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteWithJob</span></span>(fs: <span class="type">FileSystem</span>, path: <span class="type">Path</span>, recursive: <span class="type">Boolean</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    fs.delete(path, recursive)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>关键点2新建表路径的逻辑。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      fileFormat: <span class="type">FileFormat</span>,</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>,</span><br><span class="line">      outputSpec: <span class="type">OutputSpec</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      partitionColumns: <span class="type">Seq</span>[<span class="type">Attribute</span>],</span><br><span class="line">      bucketSpec: <span class="type">Option</span>[<span class="type">BucketSpec</span>],</span><br><span class="line">      statsTrackers: <span class="type">Seq</span>[<span class="type">WriteJobStatsTracker</span>],</span><br><span class="line">      options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>])</span><br><span class="line">    : <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">      <span class="comment">// This call shouldn&#x27;t be put into the `try` block below because it only initializes and</span></span><br><span class="line">      <span class="comment">// prepares the job, any exception thrown from here shouldn&#x27;t cause abortJob() to be called.</span></span><br><span class="line">      <span class="comment">// 为 committer准备job环境信息</span></span><br><span class="line">      committer.setupJob(job)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>初始化 JobContext 和 commiter 信息。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setupJob</span></span>(jobContext: <span class="type">JobContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Setup IDs</span></span><br><span class="line">    <span class="keyword">val</span> jobId = <span class="type">SparkHadoopWriterUtils</span>.createJobID(<span class="keyword">new</span> <span class="type">Date</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> taskId = <span class="keyword">new</span> <span class="type">TaskID</span>(jobId, <span class="type">TaskType</span>.<span class="type">MAP</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> taskAttemptId = <span class="keyword">new</span> <span class="type">TaskAttemptID</span>(taskId, <span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Set up the configuration object</span></span><br><span class="line">    jobContext.getConfiguration.set(<span class="string">&quot;mapreduce.job.id&quot;</span>, jobId.toString)</span><br><span class="line">    jobContext.getConfiguration.set(<span class="string">&quot;mapreduce.task.id&quot;</span>, taskAttemptId.getTaskID.toString)</span><br><span class="line">    jobContext.getConfiguration.set(<span class="string">&quot;mapreduce.task.attempt.id&quot;</span>, taskAttemptId.toString)</span><br><span class="line">    jobContext.getConfiguration.setBoolean(<span class="string">&quot;mapreduce.task.ismap&quot;</span>, <span class="literal">true</span>)</span><br><span class="line">    jobContext.getConfiguration.setInt(<span class="string">&quot;mapreduce.task.partition&quot;</span>, <span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> taskAttemptContext = <span class="keyword">new</span> <span class="type">TaskAttemptContextImpl</span>(jobContext.getConfiguration, taskAttemptId)</span><br><span class="line">    committer = setupCommitter(taskAttemptContext)</span><br><span class="line">    <span class="comment">// 这里会去新建写数据目录</span></span><br><span class="line">    committer.setupJob(jobContext)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里是真正创建目录的地方。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.class</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Create the temporary directory that is the root of all of the task</span></span><br><span class="line"><span class="comment">   * work directories.</span></span><br><span class="line"><span class="comment">   * @param context the job&#x27;s context</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  public void setupJob(<span class="type">JobContext</span> context) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (hasOutputPath()) &#123;</span><br><span class="line">      <span class="type">Path</span> jobAttemptPath = getJobAttemptPath(context);</span><br><span class="line">      <span class="comment">// 关键：问题出在这里</span></span><br><span class="line">      <span class="type">FileSystem</span> fs = jobAttemptPath.getFileSystem(</span><br><span class="line">          context.getConfiguration());</span><br><span class="line">      <span class="keyword">if</span> (!fs.mkdirs(jobAttemptPath)) &#123;</span><br><span class="line">        <span class="type">LOG</span>.error(<span class="string">&quot;Mkdirs failed to create &quot;</span> + jobAttemptPath);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">LOG</span>.warn(<span class="string">&quot;Output Path is null in setupJob()&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>分析<code>FileSystem fs = jobAttemptPath.getFileSystem( context.getConfiguration());</code>这段代码，通过 debug 发现 context.getConfiguration() 的 conf 信息中 <code>fs.permissions.umask-mode</code> 是 006，也就是提交节点中 hdfs-site.xml 文件的配置值，但构建的 fs 对象的 <code>fs.permissions.umask-mode</code> 是 022，也就是出问题的路径权限，该值来源于 hadoop-common jar 中 core-default.xml 文件的默认值。也就说明这里构建的 fs 对象会覆盖 <code>fs.permissions.umask-mode</code> 值。</p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2022/01/22/91d408d4702da251438df06b47374221-1642563770948-88bb6886-2d8d-488e-b673-d49cd0a635f0-20220122220527718-38abe5.png" alt="img"></p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2022/01/22/91d408d4702da251438df06b47374221-1642563770948-88bb6886-2d8d-488e-b673-d49cd0a635f0-877511.png" alt="img"></p>
<p><img src="https://cdn.jsdelivr.net/gh/benkoons/blog-imgs/blog-imgs/2022/05/08/374d19f3cb7ea8bf56f721ed26c56b61-1642563777551-c0cd5116-d9ac-4301-80ae-d83f8c57588f-eafe07.png" alt="img"></p>
<h1 id="3-问题解决"><a href="#3-问题解决" class="headerlink" title="3. 问题解决"></a>3. 问题解决</h1><p>两个思路， 一个是在删除非分区表目录的时候只删除目录内部数据，保留表目录，另一个是更新 hadoop-common jar 包，去掉jar包 core-default.xml 文件或者把 umask-mode 值改成 006 重新编译更新jar包。</p>
<p>方式一：删除目录数据保留目录</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/FileCommitProtocol.scala</span></span><br><span class="line">  <span class="comment">// 默认delete操作，递归删除根目录数据及根目录</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteWithJob</span></span>(fs: <span class="type">FileSystem</span>, path: <span class="type">Path</span>, recursive: <span class="type">Boolean</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    fs.delete(path, recursive)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 修改后的delete操作，递归删除根目录数据，但保留根目录，此时目录 acl 信息不会改变</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteContainRootPath</span></span>(fs: <span class="type">FileSystem</span>, path: <span class="type">Path</span>, recursive: <span class="type">Boolean</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="type">FileUtil</span>.stat2Paths(fs.listStatus(path)).foreach( ele =&gt; fs.delete(ele, recursive))</span><br><span class="line">    fs.listStatus(path).isEmpty</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>方式二：替换覆盖配置的jar包</p>
<p>把 hadoop-common jar 包中 core-default.xml 中的<code>fs.permissions.umask-mode</code>值改为006重新打包。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>bugfix</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark commit结果数据源码分析</title>
    <url>/2022/01/22/Spark-commit%E7%BB%93%E6%9E%9C%E6%95%B0%E6%8D%AE%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<blockquote>
<p>源码版本：Apache Spark 2.4.7</p>
<p>导读：本文主要介绍 Spark 中 InsertIntoHiveTable 类操作生成数据的 commit 流程，包括整体执行流程介绍、committer 对象的生成、Task 和 Job Commit、以及最后 Hive 层 load 数据几个流程。如果想知道具体目录数据的流转情况，可以直接看总结部分。</p>
</blockquote>
<p>本文主要介绍 Spark 中执行 insert overwrite 语句结果数据的生成细节，以 orc 格式的分区表为例，向分区表中插入数据。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test.tmp_table_par_kw (</span><br><span class="line">id <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> <span class="type">int</span>)</span><br><span class="line">stored <span class="keyword">as</span> orc;</span><br><span class="line"></span><br><span class="line"># 插入数据</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> test.tmp_table_par_kw <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="number">20220113</span>) <span class="keyword">values</span>(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p>通过 exlpain 查看物理执行计划，可以发现底层执行的是 InsertIntoHiveTable 类的 Command 逻辑。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span> (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">insert</span> overwrite <span class="keyword">table</span> tmp_table_par_kw <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="number">20220113</span>) <span class="keyword">values</span>(<span class="number">1</span>);</span><br><span class="line"><span class="operator">=</span><span class="operator">=</span> Physical Plan <span class="operator">=</span><span class="operator">=</span></span><br><span class="line"><span class="keyword">Execute</span> InsertIntoHiveTable InsertIntoHiveTable `test`.`tmp_table_par_kw`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, Map(<span class="keyword">day</span> <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">Some</span>(<span class="number">20220113</span>)), <span class="literal">true</span>, <span class="literal">false</span>, [id]</span><br><span class="line"><span class="operator">+</span><span class="operator">-</span> LocalTableScan [id#<span class="number">5</span>]</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.641</span> seconds, Fetched <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<h1 id="1-InsertIntoHiveTable整体流程"><a href="#1-InsertIntoHiveTable整体流程" class="headerlink" title="1. InsertIntoHiveTable整体流程"></a>1. InsertIntoHiveTable整体流程</h1><p>通过 Physical Plan 发现最终的执行逻辑在 InsertIntoHiveTable 类，进入到 InsertIntoHiveTable 类逻辑，processInsert 方式是处理数据插入的关键。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertIntoHiveTable</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    table: <span class="type">CatalogTable</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    partition: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Option</span>[<span class="type">String</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">    query: <span class="type">LogicalPlan</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    overwrite: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    ifPartitionNotExists: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    outputColumnNames: <span class="type">Seq</span>[<span class="type">String</span>]</span>) <span class="keyword">extends</span> <span class="title">SaveAsHiveFile</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(sparkSession: <span class="type">SparkSession</span>, child: <span class="type">SparkPlan</span>): <span class="type">Seq</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> externalCatalog = sparkSession.sharedState.externalCatalog</span><br><span class="line">    <span class="keyword">val</span> hadoopConf = sparkSession.sessionState.newHadoopConf()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> hiveQlTable = <span class="type">HiveClientImpl</span>.toHiveTable(table)</span><br><span class="line">    <span class="keyword">val</span> tableDesc = <span class="keyword">new</span> <span class="type">TableDesc</span>(</span><br><span class="line">      hiveQlTable.getInputFormatClass,</span><br><span class="line">      hiveQlTable.getOutputFormatClass,</span><br><span class="line">      hiveQlTable.getMetadata</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> tableLocation = hiveQlTable.getDataLocation</span><br><span class="line">    <span class="keyword">val</span> tmpLocation = getExternalTmpPath(sparkSession, hadoopConf, tableLocation)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 关键：处理Job数据插入</span></span><br><span class="line">      processInsert(sparkSession, externalCatalog, hadoopConf, tableDesc, tmpLocation, child)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      deleteExternalTmpPath(hadoopConf)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">CommandUtils</span>.uncacheTableOrView(sparkSession, table.identifier.quotedString)</span><br><span class="line">    sparkSession.sessionState.catalog.refreshTable(table.identifier)</span><br><span class="line"></span><br><span class="line">    <span class="type">CommandUtils</span>.updateTableStats(sparkSession, table)</span><br><span class="line"></span><br><span class="line">    <span class="type">Seq</span>.empty[<span class="type">Row</span>]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>整体上分为两个步骤，首先是 SQL 语句最终生成的数据，然后调用 Hive 的 api 将数据移动到分区目录，并向 Hive MetaStore 元数据中更新分区信息。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processInsert</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      externalCatalog: <span class="type">ExternalCatalog</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      tableDesc: <span class="type">TableDesc</span>,</span><br><span class="line">      tmpLocation: <span class="type">Path</span>,</span><br><span class="line">      child: <span class="type">SparkPlan</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 关键：saveAsHiveFile写数据入口</span></span><br><span class="line">    <span class="keyword">val</span> writtenParts = saveAsHiveFile(</span><br><span class="line">      sparkSession = sparkSession,</span><br><span class="line">      plan = child,</span><br><span class="line">      hadoopConf = hadoopConf,</span><br><span class="line">      fileSinkConf = fileSinkConf,</span><br><span class="line">      outputLocation = tmpLocation.toString,</span><br><span class="line">      partitionAttributes = partitionAttributes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (partition.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">if</span> (numDynamicPartitions &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment">// 动态分区插入数据load分区</span></span><br><span class="line">        externalCatalog.loadDynamicPartitions(</span><br><span class="line">          db = table.database,</span><br><span class="line">          table = table.identifier.table,</span><br><span class="line">          tmpLocation.toString,</span><br><span class="line">          partitionSpec,</span><br><span class="line">          overwrite,</span><br><span class="line">          numDynamicPartitions)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          ...</span><br><span class="line">          <span class="comment">// 静态分区插入数据load分区</span></span><br><span class="line">          externalCatalog.loadPartition(</span><br><span class="line">            table.database,</span><br><span class="line">            table.identifier.table,</span><br><span class="line">            tmpLocation.toString,</span><br><span class="line">            partitionSpec,</span><br><span class="line">            isOverwrite = doHiveOverwrite,</span><br><span class="line">            inheritTableSpecs = inheritTableSpecs,</span><br><span class="line">            isSrcLocal = <span class="literal">false</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 非分区表load数据</span></span><br><span class="line">      externalCatalog.loadTable(</span><br><span class="line">        table.database,</span><br><span class="line">        table.identifier.table,</span><br><span class="line">        tmpLocation.toString, <span class="comment">// <span class="doctag">TODO:</span> URI</span></span><br><span class="line">        overwrite,</span><br><span class="line">        isSrcLocal = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>生成结果数据的逻辑是创建 committer 对象，用于结果数据的 commit 操作，然后通过 FileFowmatWriter.write() 方法写入数据。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">saveAsHiveFile</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      fileSinkConf: <span class="type">FileSinkDesc</span>,</span><br><span class="line">      outputLocation: <span class="type">String</span>,</span><br><span class="line">      customPartitionLocations: <span class="type">Map</span>[<span class="type">TablePartitionSpec</span>, <span class="type">String</span>] = <span class="type">Map</span>.empty,</span><br><span class="line">      partitionAttributes: <span class="type">Seq</span>[<span class="type">Attribute</span>] = <span class="type">Nil</span>): <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成committer对象</span></span><br><span class="line">    <span class="keyword">val</span> committer = <span class="type">FileCommitProtocol</span>.instantiate(</span><br><span class="line">      sparkSession.sessionState.conf.fileCommitProtocolClass,</span><br><span class="line">      jobId = java.util.<span class="type">UUID</span>.randomUUID().toString,</span><br><span class="line">      outputPath = outputLocation)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关键：write数据，内部涉及job和task commit数据的逻辑</span></span><br><span class="line">    <span class="type">FileFormatWriter</span>.write(</span><br><span class="line">      sparkSession = sparkSession,</span><br><span class="line">      plan = plan,</span><br><span class="line">      fileFormat = <span class="keyword">new</span> <span class="type">HiveFileFormat</span>(fileSinkConf),</span><br><span class="line">      committer = committer,</span><br><span class="line">      outputSpec =</span><br><span class="line">        <span class="type">FileFormatWriter</span>.<span class="type">OutputSpec</span>(outputLocation, customPartitionLocations, outputColumns),</span><br><span class="line">      hadoopConf = hadoopConf,</span><br><span class="line">      partitionColumns = partitionAttributes,</span><br><span class="line">      bucketSpec = <span class="type">None</span>,</span><br><span class="line">      statsTrackers = <span class="type">Seq</span>(basicWriteJobStatsTracker(hadoopConf)),</span><br><span class="line">      options = <span class="type">Map</span>.empty)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>数据的生成分为 Job Commit 和 Task Commit，Task 是 Job 的子集，executeTask() 方法是 write 数据的关键，然后 Task Commit 和 Job Commit 操作将 write 的数据 move 到相应的目录。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      fileFormat: <span class="type">FileFormat</span>,</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>,</span><br><span class="line">      outputSpec: <span class="type">OutputSpec</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      partitionColumns: <span class="type">Seq</span>[<span class="type">Attribute</span>],</span><br><span class="line">      bucketSpec: <span class="type">Option</span>[<span class="type">BucketSpec</span>],</span><br><span class="line">      statsTrackers: <span class="type">Seq</span>[<span class="type">WriteJobStatsTracker</span>],</span><br><span class="line">      options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>])</span><br><span class="line">    : <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Job commit准备</span></span><br><span class="line">    committer.setupJob(job)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> rdd = <span class="keyword">if</span> (orderingMatched) &#123;</span><br><span class="line">        plan.execute()</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// SPARK-21165: the `requiredOrdering` is based on the attributes from analyzed plan, and</span></span><br><span class="line">        <span class="comment">// the physical plan may have different attribute ids due to optimizer removing some</span></span><br><span class="line">        <span class="comment">// aliases. Here we bind the expression ahead to avoid potential attribute ids mismatch.</span></span><br><span class="line">        <span class="keyword">val</span> orderingExpr = requiredOrdering</span><br><span class="line">          .map(<span class="type">SortOrder</span>(_, <span class="type">Ascending</span>))</span><br><span class="line">          .map(<span class="type">BindReferences</span>.bindReference(_, outputSpec.outputColumns))</span><br><span class="line">        <span class="type">SortExec</span>(</span><br><span class="line">          orderingExpr,</span><br><span class="line">          global = <span class="literal">false</span>,</span><br><span class="line">          child = plan).execute()</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// SPARK-23271 If we are attempting to write a zero partition rdd, create a dummy single</span></span><br><span class="line">      <span class="comment">// partition rdd to make sure we at least set up one write task to write the metadata.</span></span><br><span class="line">      <span class="keyword">val</span> rddWithNonEmptyPartitions = <span class="keyword">if</span> (rdd.partitions.length == <span class="number">0</span>) &#123;</span><br><span class="line">        sparkSession.sparkContext.parallelize(<span class="type">Array</span>.empty[<span class="type">InternalRow</span>], <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        rdd</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> jobIdInstant = <span class="keyword">new</span> <span class="type">Date</span>().getTime</span><br><span class="line">      <span class="keyword">val</span> ret = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">WriteTaskResult</span>](rddWithNonEmptyPartitions.partitions.length)</span><br><span class="line">      <span class="comment">// Task commit数据逻辑</span></span><br><span class="line">      sparkSession.sparkContext.runJob(</span><br><span class="line">        rddWithNonEmptyPartitions,</span><br><span class="line">        (taskContext: <span class="type">TaskContext</span>, iter: <span class="type">Iterator</span>[<span class="type">InternalRow</span>]) =&gt; &#123;</span><br><span class="line">          executeTask(</span><br><span class="line">            description = description,</span><br><span class="line">            jobIdInstant = jobIdInstant,</span><br><span class="line">            sparkStageId = taskContext.stageId(),</span><br><span class="line">            sparkPartitionId = taskContext.partitionId(),</span><br><span class="line">            sparkAttemptNumber = taskContext.taskAttemptId().toInt &amp; <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>,</span><br><span class="line">            committer,</span><br><span class="line">            iterator = iter)</span><br><span class="line">        &#125;,</span><br><span class="line">        rddWithNonEmptyPartitions.partitions.indices,</span><br><span class="line">        (index, res: <span class="type">WriteTaskResult</span>) =&gt; &#123;</span><br><span class="line">          committer.onTaskCommit(res.commitMsg)</span><br><span class="line">          ret(index) = res</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> commitMsgs = ret.map(_.commitMsg)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 执行 Job commit 数据逻辑</span></span><br><span class="line">      committer.commitJob(job, commitMsgs)</span><br><span class="line">      logInfo(<span class="string">s&quot;Write Job <span class="subst">$&#123;description.uuid&#125;</span> committed.&quot;</span>)</span><br><span class="line"></span><br><span class="line">      processStats(description.statsTrackers, ret.map(_.summary.stats))</span><br><span class="line">      logInfo(<span class="string">s&quot;Finished processing stats for write job <span class="subst">$&#123;description.uuid&#125;</span>.&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// return a set of all the partition paths that were updated during this job</span></span><br><span class="line">      ret.map(_.summary.updatedPartitions).reduceOption(_ ++ _).getOrElse(<span class="type">Set</span>.empty)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123; <span class="keyword">case</span> cause: <span class="type">Throwable</span> =&gt;</span><br><span class="line">      logError(<span class="string">s&quot;Aborting job <span class="subst">$&#123;description.uuid&#125;</span>.&quot;</span>, cause)</span><br><span class="line">      committer.abortJob(job)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;Job aborted.&quot;</span>, cause)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，InsertIntoHiveTable 的整体流程就介绍完毕。</p>
<h1 id="2-Committer对象创建"><a href="#2-Committer对象创建" class="headerlink" title="2. Committer对象创建"></a>2. Committer对象创建</h1><p>Task Commit 和 Job Commit 是内部生成数据的关键，而生成数据是需要相应的 committer 对象作为入口，用来 commitTask 和 commitTask。</p>
<p>通过 FileCommitProtocol.instantiate() 方法生成 committer 对象，committer 对象由 <code>sparkSession.sessionState.conf.fileCommitProtoclClass</code>决定，这个值由参数 spark.sql.sources.commitProtocolClass 控制， 默认为 SQLHadoopMapReduceCommitProtocol，可以通过这里实现自定义的 commitProtocol。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">saveAsHiveFile</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      fileSinkConf: <span class="type">FileSinkDesc</span>,</span><br><span class="line">      outputLocation: <span class="type">String</span>,</span><br><span class="line">      customPartitionLocations: <span class="type">Map</span>[<span class="type">TablePartitionSpec</span>, <span class="type">String</span>] = <span class="type">Map</span>.empty,</span><br><span class="line">      partitionAttributes: <span class="type">Seq</span>[<span class="type">Attribute</span>] = <span class="type">Nil</span>): <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成committer对象</span></span><br><span class="line">    <span class="keyword">val</span> committer = <span class="type">FileCommitProtocol</span>.instantiate(</span><br><span class="line">      sparkSession.sessionState.conf.fileCommitProtocolClass,</span><br><span class="line">      jobId = java.util.<span class="type">UUID</span>.randomUUID().toString,</span><br><span class="line">      outputPath = outputLocation)</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><code>sparkSession.sessionState.conf.fileCommitProtocolClass</code>使用的具体类的定义。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/internal/SQLConf.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fileCommitProtocolClass</span></span>: <span class="type">String</span> = getConf(<span class="type">SQLConf</span>.<span class="type">FILE_COMMIT_PROTOCOL_CLASS</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> <span class="type">FILE_COMMIT_PROTOCOL_CLASS</span> =</span><br><span class="line">    buildConf(<span class="string">&quot;spark.sql.sources.commitProtocolClass&quot;</span>)</span><br><span class="line">      .internal()</span><br><span class="line">      .stringConf</span><br><span class="line">      .createWithDefault(</span><br><span class="line">        <span class="string">&quot;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>这里通过 Java 的反射机制生成 committer 对象，为 SQLHadoopMapReduceCommitProtocol 对象，SQLHadoopMapReduceCommitProtocol 类是继承自 HadoopMapReduceCommitProtocol 类，只是重写了其 setupCommitter 方法。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/FileCommitProtocol.scala</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FileCommitProtocol</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">TaskCommitMessage</span>(<span class="params">val obj: <span class="type">Any</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span></span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">EmptyTaskCommitMessage</span> <span class="keyword">extends</span> <span class="title">TaskCommitMessage</span>(<span class="params">null</span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Instantiates a FileCommitProtocol using the given className.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">instantiate</span></span>(</span><br><span class="line">      className: <span class="type">String</span>,</span><br><span class="line">      jobId: <span class="type">String</span>,</span><br><span class="line">      outputPath: <span class="type">String</span>,</span><br><span class="line">      dynamicPartitionOverwrite: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FileCommitProtocol</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(className).asInstanceOf[<span class="type">Class</span>[<span class="type">FileCommitProtocol</span>]]</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> ctor = clazz.getDeclaredConstructor(classOf[<span class="type">String</span>], classOf[<span class="type">String</span>], classOf[<span class="type">Boolean</span>])</span><br><span class="line">      logDebug(<span class="string">&quot;Using (String, String, Boolean) constructor&quot;</span>)</span><br><span class="line">      ctor.newInstance(jobId, outputPath, dynamicPartitionOverwrite.asInstanceOf[java.lang.<span class="type">Boolean</span>])</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">NoSuchMethodException</span> =&gt;</span><br><span class="line">        logDebug(<span class="string">&quot;Falling back to (String, String) constructor&quot;</span>)</span><br><span class="line">        require(!dynamicPartitionOverwrite,</span><br><span class="line">          <span class="string">&quot;Dynamic Partition Overwrite is enabled but&quot;</span> +</span><br><span class="line">            <span class="string">s&quot; the committer <span class="subst">$&#123;className&#125;</span> does not have the appropriate constructor&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> ctor = clazz.getDeclaredConstructor(classOf[<span class="type">String</span>], classOf[<span class="type">String</span>])</span><br><span class="line">        ctor.newInstance(jobId, outputPath)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在介绍 InsertIntoHiveTable 流程中，我们发现在 FileFormatWriter 类中 <code>committer.setupJob(job)</code>方法内部又会创建一个 committer 对象，其中 setupCommitter 方法的作用是生成 HadoopMapReduceCommitProtocol 内部的 committer 对象，而这个对象是 OutputCommitter 类型，其 commitTask 和 commitJob 方法会被用于 commit 每个 task 的结果和整个 job 的结果。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setupJob</span></span>(jobContext: <span class="type">JobContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Setup IDs</span></span><br><span class="line">    <span class="keyword">val</span> jobId = <span class="type">SparkHadoopWriterUtils</span>.createJobID(<span class="keyword">new</span> <span class="type">Date</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> taskId = <span class="keyword">new</span> <span class="type">TaskID</span>(jobId, <span class="type">TaskType</span>.<span class="type">MAP</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> taskAttemptId = <span class="keyword">new</span> <span class="type">TaskAttemptID</span>(taskId, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set up the configuration object</span></span><br><span class="line">    jobContext.getConfiguration.set(<span class="string">&quot;mapreduce.job.id&quot;</span>, jobId.toString)</span><br><span class="line">    jobContext.getConfiguration.set(<span class="string">&quot;mapreduce.task.id&quot;</span>, taskAttemptId.getTaskID.toString)</span><br><span class="line">    jobContext.getConfiguration.set(<span class="string">&quot;mapreduce.task.attempt.id&quot;</span>, taskAttemptId.toString)</span><br><span class="line">    jobContext.getConfiguration.setBoolean(<span class="string">&quot;mapreduce.task.ismap&quot;</span>, <span class="literal">true</span>)</span><br><span class="line">    jobContext.getConfiguration.setInt(<span class="string">&quot;mapreduce.task.partition&quot;</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> taskAttemptContext = <span class="keyword">new</span> <span class="type">TaskAttemptContextImpl</span>(jobContext.getConfiguration, taskAttemptId)</span><br><span class="line">    committer = setupCommitter(taskAttemptContext)</span><br><span class="line">    committer.setupJob(jobContext)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>注意，这里有两层 committer 对象，一层就是 HadoopMapReduceCommitProtocol 本身，另一层是 HadoopMapReduceCommitProtocol 内部的 committer，为 OutputCommitter 类型。在 HadoopMapReduceCommitProtocol 的 commitTask 和 commitJob 方法中都会直接或间接地调用其内部 commiter 对象的对应方法。</p>
<h1 id="3-Task-Commit流程"><a href="#3-Task-Commit流程" class="headerlink" title="3. Task Commit流程"></a>3. Task Commit流程</h1><p>Task Commit 逻辑是用于生成一个 Job 的结果数据，并通过 commit 逻辑生成数据到相关目录。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      fileFormat: <span class="type">FileFormat</span>,</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>,</span><br><span class="line">      outputSpec: <span class="type">OutputSpec</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      partitionColumns: <span class="type">Seq</span>[<span class="type">Attribute</span>],</span><br><span class="line">      bucketSpec: <span class="type">Option</span>[<span class="type">BucketSpec</span>],</span><br><span class="line">      statsTrackers: <span class="type">Seq</span>[<span class="type">WriteJobStatsTracker</span>],</span><br><span class="line">      options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>])</span><br><span class="line">    : <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">			...</span><br><span class="line">      <span class="comment">// Task commit数据逻辑</span></span><br><span class="line">      sparkSession.sparkContext.runJob(</span><br><span class="line">        rddWithNonEmptyPartitions,</span><br><span class="line">        (taskContext: <span class="type">TaskContext</span>, iter: <span class="type">Iterator</span>[<span class="type">InternalRow</span>]) =&gt; &#123;</span><br><span class="line">          executeTask(</span><br><span class="line">            description = description,</span><br><span class="line">            jobIdInstant = jobIdInstant,</span><br><span class="line">            sparkStageId = taskContext.stageId(),</span><br><span class="line">            sparkPartitionId = taskContext.partitionId(),</span><br><span class="line">            sparkAttemptNumber = taskContext.taskAttemptId().toInt &amp; <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>,</span><br><span class="line">            committer,</span><br><span class="line">            iterator = iter)</span><br><span class="line">        &#125;,</span><br><span class="line">        rddWithNonEmptyPartitions.partitions.indices,</span><br><span class="line">        (index, res: <span class="type">WriteTaskResult</span>) =&gt; &#123;</span><br><span class="line">          committer.onTaskCommit(res.commitMsg)</span><br><span class="line">          ret(index) = res</span><br><span class="line">        &#125;)</span><br><span class="line">			...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>executeTask() 中会通过 committer 对象初始化 Task Commit 相关环境信息，接着通过 DataWriter 写入数据，最后调用 dataWriter.commit() 执行 Task 的 Commit 逻辑。这里的 commiter 对象是 HadoopMapReduceCommitProtocol 类型。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line">  <span class="comment">/** Writes data out in a single Spark task. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">executeTask</span></span>(</span><br><span class="line">      description: <span class="type">WriteJobDescription</span>,</span><br><span class="line">      jobIdInstant: <span class="type">Long</span>,</span><br><span class="line">      sparkStageId: <span class="type">Int</span>,</span><br><span class="line">      sparkPartitionId: <span class="type">Int</span>,</span><br><span class="line">      sparkAttemptNumber: <span class="type">Int</span>,</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>,</span><br><span class="line">      iterator: <span class="type">Iterator</span>[<span class="type">InternalRow</span>]): <span class="type">WriteTaskResult</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置Task信息</span></span><br><span class="line">    committer.setupTask(taskAttemptContext)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataWriter =</span><br><span class="line">      <span class="keyword">if</span> (sparkPartitionId != <span class="number">0</span> &amp;&amp; !iterator.hasNext) &#123;</span><br><span class="line">        <span class="comment">// In case of empty job, leave first partition to save meta for file format like parquet.</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">EmptyDirectoryDataWriter</span>(description, taskAttemptContext, committer)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (description.partitionColumns.isEmpty &amp;&amp; description.bucketIdExpression.isEmpty) &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SingleDirectoryDataWriter</span>(description, taskAttemptContext, committer)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">DynamicPartitionDataWriter</span>(description, taskAttemptContext, committer)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="type">Utils</span>.tryWithSafeFinallyAndFailureCallbacks(block = &#123;</span><br><span class="line">        <span class="comment">// Execute the task to write rows out and commit the task.</span></span><br><span class="line">        <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">          <span class="comment">//通过 DataWriter写数据</span></span><br><span class="line">          dataWriter.write(iterator.next())</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 写完数据后进行commit</span></span><br><span class="line">        dataWriter.commit()</span><br><span class="line">      &#125;)(catchBlock = &#123;</span><br><span class="line">        <span class="comment">// If there is an error, abort the task</span></span><br><span class="line">        dataWriter.abort()</span><br><span class="line">        logError(<span class="string">s&quot;Job <span class="subst">$jobId</span> aborted.&quot;</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">FetchFailedException</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;Task failed while writing rows.&quot;</span>, t)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里的 commiter 对象还是 HadoopMapReduceCommitProtocol 类型，通过 committer 对象的 commitTask 提交 Task Commit。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatDataWriter.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">commit</span></span>(): <span class="type">WriteTaskResult</span> = &#123;</span><br><span class="line">    releaseResources()</span><br><span class="line">    <span class="keyword">val</span> summary = <span class="type">ExecutedWriteSummary</span>(</span><br><span class="line">      updatedPartitions = updatedPartitions.toSet,</span><br><span class="line">      stats = statsTrackers.map(_.getFinalStats()))</span><br><span class="line">    <span class="type">WriteTaskResult</span>(committer.commitTask(taskAttemptContext), summary)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>通过 SparkHadoopMapRedUtil 类的 commitTask 间接执行 commitTask 操作。此时 commitTask 方法中的 committer 参数已经是 HadoopMapReduceCommitProtocol 类内部的 commiter 对象，为 OutputCommitter 类型。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">commitTask</span></span>(taskContext: <span class="type">TaskAttemptContext</span>): <span class="type">TaskCommitMessage</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> attemptId = taskContext.getTaskAttemptID</span><br><span class="line">    <span class="type">SparkHadoopMapRedUtil</span>.commitTask(</span><br><span class="line">      committer, taskContext, attemptId.getJobID.getId, attemptId.getTaskID.getId)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">TaskCommitMessage</span>(addedAbsPathFiles.toMap -&gt; partitionPaths.toSet)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里通过 HadoopMapReduceCommitProtocol 类内部的 OutputCommitter 类型的 commiter 对象，调用 committer.commitTask() 方法，当然 commit 前还会进行一些 check 操作，判断是否能够执行 commit 操作。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/mapred/SparkHadoopMapRedUtil.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">commitTask</span></span>(</span><br><span class="line">      committer: <span class="type">MapReduceOutputCommitter</span>,</span><br><span class="line">      mrTaskContext: <span class="type">MapReduceTaskAttemptContext</span>,</span><br><span class="line">      jobId: <span class="type">Int</span>,</span><br><span class="line">      splitId: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mrTaskAttemptID = mrTaskContext.getTaskAttemptID</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Called after we have decided to commit</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">performCommit</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        committer.commitTask(mrTaskContext)</span><br><span class="line">        logInfo(<span class="string">s&quot;<span class="subst">$mrTaskAttemptID</span>: Committed&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> cause: <span class="type">IOException</span> =&gt;</span><br><span class="line">          logError(<span class="string">s&quot;Error committing the output of task: <span class="subst">$mrTaskAttemptID</span>&quot;</span>, cause)</span><br><span class="line">          committer.abortTask(mrTaskContext)</span><br><span class="line">          <span class="keyword">throw</span> cause</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// First, check whether the task&#x27;s output has already been committed by some other attempt</span></span><br><span class="line">    <span class="keyword">if</span> (committer.needsTaskCommit(mrTaskContext)) &#123;</span><br><span class="line">      <span class="keyword">val</span> shouldCoordinateWithDriver: <span class="type">Boolean</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="type">SparkEnv</span>.get.conf</span><br><span class="line">        sparkConf.getBoolean(<span class="string">&quot;spark.hadoop.outputCommitCoordination.enabled&quot;</span>, defaultValue = <span class="literal">true</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (shouldCoordinateWithDriver) &#123;</span><br><span class="line">        <span class="keyword">val</span> outputCommitCoordinator = <span class="type">SparkEnv</span>.get.outputCommitCoordinator</span><br><span class="line">        <span class="keyword">val</span> ctx = <span class="type">TaskContext</span>.get()</span><br><span class="line">        <span class="keyword">val</span> canCommit = outputCommitCoordinator.canCommit(ctx.stageId(), ctx.stageAttemptNumber(),</span><br><span class="line">          splitId, ctx.attemptNumber())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (canCommit) &#123;</span><br><span class="line">          performCommit()</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> message =</span><br><span class="line">            <span class="string">s&quot;<span class="subst">$mrTaskAttemptID</span>: Not committed because the driver did not authorize commit&quot;</span></span><br><span class="line">          logInfo(message)</span><br><span class="line">          <span class="comment">// We need to abort the task so that the driver can reschedule new attempts, if necessary</span></span><br><span class="line">          committer.abortTask(mrTaskContext)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">CommitDeniedException</span>(message, ctx.stageId(), splitId, ctx.attemptNumber())</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 执行commit逻辑</span></span><br><span class="line">        performCommit()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Some other attempt committed the output, so we do nothing and signal success</span></span><br><span class="line">      logInfo(<span class="string">s&quot;No need to commit output of task because needsTaskCommit=false: <span class="subst">$mrTaskAttemptID</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>commitTask 的真正执行逻辑，也就是前面提到的 Task Commit 操作，这里会将 Task 任务结束 write 的数据移动到 Task Commit 逻辑的目标目录。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.class</span></span><br><span class="line">  public void commitTask(<span class="type">TaskAttemptContext</span> context) </span><br><span class="line">  <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">    commitTask(context, <span class="literal">null</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void commitTask(<span class="type">TaskAttemptContext</span> context, <span class="type">Path</span> taskAttemptPath) </span><br><span class="line">  <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">    <span class="type">TaskAttemptID</span> attemptId = context.getTaskAttemptID();</span><br><span class="line">    <span class="keyword">if</span> (hasOutputPath()) &#123;</span><br><span class="line">      context.progress();</span><br><span class="line">      <span class="keyword">if</span>(taskAttemptPath == <span class="literal">null</span>) &#123;</span><br><span class="line">        taskAttemptPath = getTaskAttemptPath(context);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">Path</span> committedTaskPath = getCommittedTaskPath(context);</span><br><span class="line">      <span class="type">FileSystem</span> fs = taskAttemptPath.getFileSystem(context.getConfiguration());</span><br><span class="line">      <span class="keyword">if</span> (fs.exists(taskAttemptPath)) &#123;</span><br><span class="line">        <span class="keyword">if</span>(fs.exists(committedTaskPath)) &#123;</span><br><span class="line">          <span class="keyword">if</span>(!fs.delete(committedTaskPath, <span class="literal">true</span>)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">&quot;Could not delete &quot;</span> + committedTaskPath);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关键：将 Task 生成的数据 move 到目标 HDFS 目录</span></span><br><span class="line">        <span class="keyword">if</span>(!fs.rename(taskAttemptPath, committedTaskPath)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">&quot;Could not rename &quot;</span> + taskAttemptPath + <span class="string">&quot; to &quot;</span></span><br><span class="line">              + committedTaskPath);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">LOG</span>.info(<span class="string">&quot;Saved output of task &#x27;&quot;</span> + attemptId + <span class="string">&quot;&#x27; to &quot;</span> + </span><br><span class="line">            committedTaskPath);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">LOG</span>.warn(<span class="string">&quot;No Output found for &quot;</span> + attemptId);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">LOG</span>.warn(<span class="string">&quot;Output Path is null in commitTask()&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，Task Commit 的逻辑已执行结束。</p>
<h1 id="4-Job-Commit流程"><a href="#4-Job-Commit流程" class="headerlink" title="4. Job Commit流程"></a>4. Job Commit流程</h1><p>Job Commit 逻辑并不涉及到写数据的逻辑，然后将 Task Commit 生成的数据进一步移动到目标目录。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      fileFormat: <span class="type">FileFormat</span>,</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>,</span><br><span class="line">      outputSpec: <span class="type">OutputSpec</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      partitionColumns: <span class="type">Seq</span>[<span class="type">Attribute</span>],</span><br><span class="line">      bucketSpec: <span class="type">Option</span>[<span class="type">BucketSpec</span>],</span><br><span class="line">      statsTrackers: <span class="type">Seq</span>[<span class="type">WriteJobStatsTracker</span>],</span><br><span class="line">      options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>])</span><br><span class="line">    : <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Job commit准备</span></span><br><span class="line">    committer.setupJob(job)</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 执行 Job commit 数据逻辑</span></span><br><span class="line">    committer.commitJob(job, commitMsgs)</span><br><span class="line">    logInfo(<span class="string">s&quot;Write Job <span class="subst">$&#123;description.uuid&#125;</span> committed.&quot;</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>  Job Commit 逻辑依赖内部 OutputCommitter 类型的 commiter 对象，并进行相应的 commitJob 操作。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">commitJob</span></span>(jobContext: <span class="type">JobContext</span>, taskCommits: <span class="type">Seq</span>[<span class="type">TaskCommitMessage</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Job Commit 逻辑依赖内部的 commiter 对象（即 OutputCommitter 类型的对象）</span></span><br><span class="line">    committer.commitJob(jobContext)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (hasValidPath) &#123;</span><br><span class="line">      <span class="keyword">val</span> (allAbsPathFiles, allPartitionPaths) =</span><br><span class="line">        taskCommits.map(_.obj.asInstanceOf[(<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>], <span class="type">Set</span>[<span class="type">String</span>])]).unzip</span><br><span class="line">      <span class="keyword">val</span> fs = stagingDir.getFileSystem(jobContext.getConfiguration)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> filesToMove = allAbsPathFiles.foldLeft(<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]())(_ ++ _)</span><br><span class="line">      logDebug(<span class="string">s&quot;Committing files staged for absolute locations <span class="subst">$filesToMove</span>&quot;</span>)</span><br><span class="line">      <span class="keyword">if</span> (dynamicPartitionOverwrite) &#123;</span><br><span class="line">        <span class="keyword">val</span> absPartitionPaths = filesToMove.values.map(<span class="keyword">new</span> <span class="type">Path</span>(_).getParent).toSet</span><br><span class="line">        logDebug(<span class="string">s&quot;Clean up absolute partition directories for overwriting: <span class="subst">$absPartitionPaths</span>&quot;</span>)</span><br><span class="line">        absPartitionPaths.foreach(fs.delete(_, <span class="literal">true</span>))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">for</span> ((src, dst) &lt;- filesToMove) &#123;</span><br><span class="line">        fs.rename(<span class="keyword">new</span> <span class="type">Path</span>(src), <span class="keyword">new</span> <span class="type">Path</span>(dst))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (dynamicPartitionOverwrite) &#123;</span><br><span class="line">        <span class="keyword">val</span> partitionPaths = allPartitionPaths.foldLeft(<span class="type">Set</span>[<span class="type">String</span>]())(_ ++ _)</span><br><span class="line">        logDebug(<span class="string">s&quot;Clean up default partition directories for overwriting: <span class="subst">$partitionPaths</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> (part &lt;- partitionPaths) &#123;</span><br><span class="line">          <span class="keyword">val</span> finalPartPath = <span class="keyword">new</span> <span class="type">Path</span>(path, part)</span><br><span class="line">          <span class="keyword">if</span> (!fs.delete(finalPartPath, <span class="literal">true</span>) &amp;&amp; !fs.exists(finalPartPath.getParent)) &#123;</span><br><span class="line">            fs.mkdirs(finalPartPath.getParent)</span><br><span class="line">          &#125;</span><br><span class="line">          fs.rename(<span class="keyword">new</span> <span class="type">Path</span>(stagingDir, part), finalPartPath)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      fs.delete(stagingDir, <span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>前面也说了 commitJob 并不 write 数据，而是将 Task Commit 生成的数据移动到目标目录，所以 Job Commit 的关键就在于移动所有 Task 任务生成的数据。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.class</span></span><br><span class="line">  public void commitJob(<span class="type">JobContext</span> context) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (hasOutputPath()) &#123;</span><br><span class="line">      <span class="type">Path</span> finalOutput = getOutputPath();</span><br><span class="line">      <span class="type">FileSystem</span> fs = finalOutput.getFileSystem(context.getConfiguration());</span><br><span class="line">      <span class="keyword">for</span>(<span class="type">FileStatus</span> stat: getAllCommittedTaskPaths(context)) &#123;</span><br><span class="line">        <span class="comment">// 将所有 Task Commit 生成的数据移动到目标目录</span></span><br><span class="line">        mergePaths(fs, stat, finalOutput);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// delete the _temporary folder and create a _done file in the o/p folder</span></span><br><span class="line">      cleanupJob(context);</span><br><span class="line">      <span class="comment">// True if the job requires output.dir marked on successful job.</span></span><br><span class="line">      <span class="comment">// Note that by default it is set to true.</span></span><br><span class="line">      <span class="keyword">if</span> (context.getConfiguration().getBoolean(<span class="type">SUCCESSFUL_JOB_OUTPUT_DIR_MARKER</span>, <span class="literal">true</span>)) &#123;</span><br><span class="line">        <span class="type">Path</span> markerPath = <span class="keyword">new</span> <span class="type">Path</span>(outputPath, <span class="type">SUCCEEDED_FILE_NAME</span>);</span><br><span class="line">        <span class="comment">// 创建 _SUCCESS 文件，表示 Job 执行成功</span></span><br><span class="line">        fs.create(markerPath).close();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">LOG</span>.warn(<span class="string">&quot;Output Path is null in commitJob()&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="5-Hive-load数据流程"><a href="#5-Hive-load数据流程" class="headerlink" title="5. Hive load数据流程"></a>5. Hive load数据流程</h1><p>InsertIntoHiveTable 的 processInsert() 方法会调用 SaveAsHiveFile.saveAsHiveFile 进行 Hive 文件的写入，写入的文件最终都会 commit 到 ./.hive-staging_*/-ext-10000 目录中，那么 .hive-staging_*目录又是怎么被 move 到 Hive table 的 location 目录下的呢？这个工作是在 processInsert 方法调用完 SaveAsHiveFile.saveAsHiveFile 方法后，再通过调用 org.apache.hadoop.hive.ql.metadata.Hive 的 load 方法完成的。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processInsert</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      externalCatalog: <span class="type">ExternalCatalog</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      tableDesc: <span class="type">TableDesc</span>,</span><br><span class="line">      tmpLocation: <span class="type">Path</span>,</span><br><span class="line">      child: <span class="type">SparkPlan</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 关键：saveAsHiveFile写数据入口</span></span><br><span class="line">    <span class="keyword">val</span> writtenParts = saveAsHiveFile(</span><br><span class="line">      sparkSession = sparkSession,</span><br><span class="line">      plan = child,</span><br><span class="line">      hadoopConf = hadoopConf,</span><br><span class="line">      fileSinkConf = fileSinkConf,</span><br><span class="line">      outputLocation = tmpLocation.toString,</span><br><span class="line">      partitionAttributes = partitionAttributes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (partition.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">if</span> (numDynamicPartitions &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment">// 动态分区插入数据load分区</span></span><br><span class="line">        externalCatalog.loadDynamicPartitions(</span><br><span class="line">          db = table.database,</span><br><span class="line">          table = table.identifier.table,</span><br><span class="line">          tmpLocation.toString,</span><br><span class="line">          partitionSpec,</span><br><span class="line">          overwrite,</span><br><span class="line">          numDynamicPartitions)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          ...</span><br><span class="line">          <span class="comment">// 静态分区插入数据load分区</span></span><br><span class="line">          externalCatalog.loadPartition(</span><br><span class="line">            table.database,</span><br><span class="line">            table.identifier.table,</span><br><span class="line">            tmpLocation.toString,</span><br><span class="line">            partitionSpec,</span><br><span class="line">            isOverwrite = doHiveOverwrite,</span><br><span class="line">            inheritTableSpecs = inheritTableSpecs,</span><br><span class="line">            isSrcLocal = <span class="literal">false</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 非分区表load数据</span></span><br><span class="line">      externalCatalog.loadTable(</span><br><span class="line">        table.database,</span><br><span class="line">        table.identifier.table,</span><br><span class="line">        tmpLocation.toString, <span class="comment">// <span class="doctag">TODO:</span> URI</span></span><br><span class="line">        overwrite,</span><br><span class="line">        isSrcLocal = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h1><p>Spark SQL 中 InsertIntoHiveTable commit 数据流程大致分为如下几步：</p>
<ul>
<li><p>构造 committer 对象，包括 HadoopMapReduceCommitProtocol 自身的 committer 对象和内部的 OutputCommiter 的 committer 对象；</p>
</li>
<li><p>commit 流程分为 Task Commit 和 Job Commit，Task Commit 先操作 Task 级别的数据，然后 Job Commit 把 Task Commit 的结果数据进行操作；</p>
</li>
<li><p>commit 前创建<code>/db/table/.hive-staging_hive_*/-ext-10000/</code>目录；</p>
</li>
<li><p>Task Commit 的操作逻辑为把 <code>/db/table/.hive-staging_*/-ext-10000/_temporary/0/_temporary/task_20220119172519_0006_m_000000/</code>目录移动到 <code>/db/table/.hive-staging_*/-ext-10000/_temporary/0/task_20220119172519_0006_m_000000/</code>目录，如果有多个 Task 目录则执行相同的操作；</p>
</li>
<li><p>Job Commit 的操作逻辑为把<code>/db/table/.hive-staging_*/-ext-10000/_temporary/0/task_20220119172519_0006_m_000000/</code>目录的数据移动到<code>/db/table/.hive-staging_*/-ext-10000/</code>目录，然后生成 <code>_SUCCESS</code>文件；</p>
</li>
<li><p>Hive load 数据的操作逻辑为把<code>/db/table/.hive-staging_*/-ext-10000/</code>目录的数据 load 到对应分区目录 <code>/db/table/.hive-staging_*/-ext-10000/pr=520</code>，并添加对应分区信息。</p>
</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://www.jianshu.com/p/01ab5f0f22df">Spark InsertIntoHiveTable如何commit结果数据</a></li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark源码</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark HDFS路径血缘关系实现</title>
    <url>/2022/05/08/Spark-HDFS%E8%B7%AF%E5%BE%84%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<blockquote>
<p>源码：Apache Spark 3.2.0</p>
<p>导读：在 Spark 引擎中，针对 SQL 语句我们通常可以去分析 SQL 的逻辑计划，提取 SQL 语句的表或字段信息，实现 SQL 层面的血缘关系，以此对数据源进行分析。但对于用户的 jar 任务，可能读取的数据源和写入的数据源都是直接操作 HDFS 路径，此时则无法通过逻辑计划去拿到对于的输入输出路径关系，本文则通过修改 Spark 源码，实现 jar 任务层面的 HDFS 路径血缘关系（即输入路径和输出路径映射关系）。</p>
</blockquote>
<h1 id="1-操作HDFS-API接口"><a href="#1-操作HDFS-API接口" class="headerlink" title="1. 操作HDFS API接口"></a>1. 操作HDFS API接口</h1><p>Spark 中提供了三种 API 接口供用户对 HDFS 数据进行读取和写入，分别是：</p>
<ul>
<li><p>sc.textFile 和 saveTextFile 方式。</p>
</li>
<li><p>sc.newAPIHadoopFile 和 saveAsNewAPIHadoopFile 方式。</p>
</li>
<li><p>spark.read.textFile 和 write.save 方式。</p>
</li>
</ul>
<p>前面两种是 RDD 提供的新旧两种操作方式，后面是 DataSet 提供的操作方式，每一种操作方式都是由 Spark 的 Transformations 算子和 Action 算子组合而成，也就是说 Job 的生成都是由最后的 Action 算子决定。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  <span class="keyword">val</span> warehouseLocation = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;spark-warehouse&quot;</span>).getAbsolutePath</span><br><span class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .appName(<span class="string">&quot;Spark HDFS Lineage&quot;</span>)</span><br><span class="line">    .config(conf)</span><br><span class="line">    .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, warehouseLocation)</span><br><span class="line">    .enableHiveSupport()</span><br><span class="line">    .getOrCreate()</span><br><span class="line">  <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> inputPath1 = <span class="string">&quot;hdfs://bj04-region07/region07/8543/app/develop/input1/&quot;</span></span><br><span class="line">  <span class="keyword">val</span> inputPath2 = <span class="string">&quot;hdfs://bj04-region07/region07/8543/app/develop/input2/&quot;</span></span><br><span class="line">  <span class="keyword">val</span> outputPath = <span class="string">&quot;hdfs://bj04-region07/region07/8543/app/develop/output/&quot;</span></span><br><span class="line"></span><br><span class="line">  println(<span class="string">&quot;=========================sc.textFile=========================&quot;</span>)</span><br><span class="line">  <span class="type">Utils</span>.deleteOutputHDFSPath(outputPath3)</span><br><span class="line">  <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(inputPath1)</span><br><span class="line">  <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(inputPath2)</span><br><span class="line">  rdd1.intersection(rdd2).saveAsTextFile(outputPath)</span><br><span class="line">  </span><br><span class="line">  println(<span class="string">&quot;=========================sc.newApi=========================&quot;</span>)</span><br><span class="line">  <span class="type">Utils</span>.deleteOutputHDFSPath(outputPath2)</span><br><span class="line">  <span class="keyword">val</span> newRdd1: <span class="type">RDD</span>[(<span class="type">LongWritable</span>, <span class="type">Text</span>)] = sc.newAPIHadoopFile[<span class="type">LongWritable</span>, <span class="type">Text</span>, <span class="type">TextInputFormat</span>](inputPath1)</span><br><span class="line">  <span class="keyword">val</span> ret1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Text</span>)] = newRdd1.map(x =&gt; (x._1 + <span class="string">&quot;1&quot;</span>, x._2))</span><br><span class="line">  ret1.saveAsNewAPIHadoopFile[<span class="type">TextOutputFormat</span>[<span class="type">String</span>, <span class="type">Text</span>]](outputPath)</span><br><span class="line">  </span><br><span class="line">  println(<span class="string">&quot;======================spark.read.text============================&quot;</span>)</span><br><span class="line">  <span class="type">Utils</span>.deleteOutputHDFSPath(outputPath1)</span><br><span class="line">  <span class="keyword">val</span> ds1 = spark.read.textFile(inputPath1)</span><br><span class="line">  <span class="keyword">val</span> ds2: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(inputPath2)</span><br><span class="line">  <span class="keyword">val</span> ds22: <span class="type">Dataset</span>[<span class="type">String</span>] = ds2.repartition(<span class="number">10</span>).coalesce(<span class="number">2</span>).map(x =&gt; x)</span><br><span class="line">  <span class="keyword">val</span> ret: <span class="type">Dataset</span>[<span class="type">String</span>] = ds1.intersect(ds22).filter(x =&gt; x != <span class="literal">null</span>)</span><br><span class="line">  ret.write.format(<span class="string">&quot;text&quot;</span>).save(outputPath)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="2-路径血缘关系收集"><a href="#2-路径血缘关系收集" class="headerlink" title="2. 路径血缘关系收集"></a>2. 路径血缘关系收集</h1><p>HDFS 路径血缘关系主要是收集用户 jar 任务的数据读取路径和数据写入路径，以此获取到用户任务对数据操作的流向。首先想到的实现方式是在 Job 提交后生成 DAG 图之前针对 finalRDD 进行回溯解析，获取到对于的 HDFS 输入路径和输出路径，这种方式并不区分 RDD 还是 DataSet 形式。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/scheduler/DAGScheduler.scala</span></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">      finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">      partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">      callSite: <span class="type">CallSite</span>,</span><br><span class="line">      listener: <span class="type">JobListener</span>,</span><br><span class="line">      properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></span><br><span class="line">      <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></span><br><span class="line">      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>但在后续实现过程中，发现 RDD 形式可以根据 finalRDD 解析获取到血缘关系，但 DataSet 方式的执行流程是走 sql 逻辑，最后生成的 FileScanRDD 对象在 Spark core 模块中并不能访问到，FileScanRDD 类位于 Spark sql 的 execution 模块，并不只是在 Spark core 模块中访问，其实这也符合 Spark 架构的设计思想，底层模块（core 模块）可以供上层模块（sql 模块）访问，但反过来则会破坏架构的整体设计思想，因此两种方式需要单独收集。</p>
<h2 id="2-1-RDD方式实现"><a href="#2-1-RDD方式实现" class="headerlink" title="2.1 RDD方式实现"></a>2.1 RDD方式实现</h2><p>Spark RDD 新旧两类 saveAs* 操作最终写数据的方式是通过 SparkHadoopWriter#write 实现，因此可以考虑在作业提交前根据 finalRDD 递归实现。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/rdd/PairRDDFunctions.scala</span></span><br><span class="line">  <span class="comment">// saveAsNewAPIHadoopFile 操作底层入口</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">saveAsNewAPIHadoopDataset</span></span>(conf: <span class="type">Configuration</span>): <span class="type">Unit</span> = self.withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">HadoopMapReduceWriteConfigUtil</span>[<span class="type">K</span>, <span class="type">V</span>](<span class="keyword">new</span> <span class="type">SerializableConfiguration</span>(conf))</span><br><span class="line">    <span class="type">SparkHadoopWriter</span>.write(</span><br><span class="line">      rdd = self,</span><br><span class="line">      config = config)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// saveAsNewAPIHadoopFile 操作底层入口</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">saveAsHadoopDataset</span></span>(conf: <span class="type">JobConf</span>): <span class="type">Unit</span> = self.withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">HadoopMapRedWriteConfigUtil</span>[<span class="type">K</span>, <span class="type">V</span>](<span class="keyword">new</span> <span class="type">SerializableJobConf</span>(conf))</span><br><span class="line">    <span class="type">SparkHadoopWriter</span>.write(</span><br><span class="line">      rdd = self,</span><br><span class="line">      config = config)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在 write 方法提交作业前新增 collectHDFSPathLineage 方法，实现 HDFS 路径血缘关系的收集。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/internal/io/SparkHadoopWriter.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>[<span class="type">K</span>, <span class="type">V</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)],</span><br><span class="line">      config: <span class="type">HadoopWriteConfigUtil</span>[<span class="type">K</span>, <span class="type">V</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// job提交前根据finalRDD解析路径血缘</span></span><br><span class="line">    collectHDFSPathLineage(rdd)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> committer = config.createCommitter(commitJobId)</span><br><span class="line">    committer.setupJob(jobContext)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Try to write all RDD partitions as a Hadoop OutputFormat.</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> ret = sparkContext.runJob(rdd, (context: <span class="type">TaskContext</span>, iter: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)]) =&gt; &#123;</span><br><span class="line">        <span class="comment">// SPARK-24552: Generate a unique &quot;attempt ID&quot; based on the stage and task attempt numbers.</span></span><br><span class="line">        <span class="comment">// Assumes that there won&#x27;t be more than Short.MaxValue attempts, at least not concurrently.</span></span><br><span class="line">        <span class="keyword">val</span> attemptId = (context.stageAttemptNumber &lt;&lt; <span class="number">16</span>) | context.attemptNumber</span><br><span class="line"></span><br><span class="line">        executeTask(</span><br><span class="line">          context = context,</span><br><span class="line">          config = config,</span><br><span class="line">          jobTrackerId = jobTrackerId,</span><br><span class="line">          commitJobId = commitJobId,</span><br><span class="line">          sparkPartitionId = context.partitionId,</span><br><span class="line">          sparkAttemptNumber = attemptId,</span><br><span class="line">          committer = committer,</span><br><span class="line">          iterator = iter)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">      logInfo(<span class="string">s&quot;Start to commit write Job <span class="subst">$&#123;jobContext.getJobID&#125;</span>.&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> (_, duration) = <span class="type">Utils</span>.timeTakenMs &#123; committer.commitJob(jobContext, ret) &#125;</span><br><span class="line">      logInfo(<span class="string">s&quot;Write Job <span class="subst">$&#123;jobContext.getJobID&#125;</span> committed. Elapsed time: <span class="subst">$duration</span> ms.&quot;</span>)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> cause: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        logError(<span class="string">s&quot;Aborting job <span class="subst">$&#123;jobContext.getJobID&#125;</span>.&quot;</span>, cause)</span><br><span class="line">        committer.abortJob(jobContext)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;Job aborted.&quot;</span>, cause)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>输入路径的收集，我们通过 finalRDD 递归访问未划分依赖的 DAG 图，直至遍历到最底层的 RDD，即 HadoopRDD 和 NewHadoopRDD，这是数据最原始的 RDD，可以拿到作业操作的输入路径列表。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//位置：org/apache/spark/internal/io/SparkHadoopWriter.scala</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">collectInputPath</span></span>[<span class="type">T</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   rdd <span class="keyword">match</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> hadoopRDD: <span class="type">HadoopRDD</span>[_, _] =&gt;</span><br><span class="line">       <span class="keyword">if</span> (hadoopRDD.name != <span class="literal">null</span> &amp;&amp; !inputHDFSPath.contains(hadoopRDD.name)) &#123;</span><br><span class="line">         inputHDFSPath.add(hadoopRDD.name)</span><br><span class="line">       &#125;</span><br><span class="line">     <span class="keyword">case</span> newHadoopRDD: <span class="type">NewHadoopRDD</span>[_, _] =&gt;</span><br><span class="line">       <span class="keyword">if</span> (newHadoopRDD.name != <span class="literal">null</span> &amp;&amp; !inputHDFSPath.contains(newHadoopRDD.name)) &#123;</span><br><span class="line">         inputHDFSPath.add(newHadoopRDD.name)</span><br><span class="line">       &#125;</span><br><span class="line">     <span class="keyword">case</span> mapPartitionsRDD: <span class="type">MapPartitionsRDD</span>[_, _] =&gt; collectInputPath(mapPartitionsRDD.prev)</span><br><span class="line">     <span class="keyword">case</span> coalescedRDD: <span class="type">CoalescedRDD</span>[_] =&gt; collectInputPath(coalescedRDD.prev)</span><br><span class="line">     <span class="keyword">case</span> shuffledRDD: <span class="type">ShuffledRDD</span>[_, _, _] =&gt; collectInputPath(shuffledRDD.prev)</span><br><span class="line">     <span class="keyword">case</span> zippedPartitionsBaseRdd: <span class="type">ZippedPartitionsBaseRDD</span>[_] =&gt;</span><br><span class="line">       zippedPartitionsBaseRdd.rdds.foreach(collectInputPath(_))</span><br><span class="line">     <span class="keyword">case</span> coGroupedRdd: <span class="type">CoGroupedRDD</span>[_] =&gt; coGroupedRdd.rdds.foreach(collectInputPath(_))</span><br><span class="line">     <span class="keyword">case</span> unionRDD: <span class="type">UnionRDD</span>[_] =&gt; unionRDD.rdds.foreach(collectInputPath(_))</span><br><span class="line">     <span class="keyword">case</span> subtractedRDD: <span class="type">SubtractedRDD</span>[_, _, _] =&gt;</span><br><span class="line">       collectInputPath(subtractedRDD.rdd1)</span><br><span class="line">       collectInputPath(subtractedRDD.rdd2)</span><br><span class="line">     <span class="keyword">case</span> _ =&gt;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>对于输出路径，在整个 RDD 依赖关系中并没有对应地方保存，而是通过 FileOutputFormat#setOutputPath 方式设置到 OutputFormat 对象中，而在 FileOutputFormat 中提供有 getOutputPath(JobConf conf) 方法获取输出路径，因此可以把保存有输出路径的 JobConf 更新到 SparkContext 对象中，以便其他位置可以访问。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/rdd/PairRDDFunctions.scala</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">saveAsHadoopFile</span></span>(</span><br><span class="line">      path: <span class="type">String</span>,</span><br><span class="line">      keyClass: <span class="type">Class</span>[_],</span><br><span class="line">      valueClass: <span class="type">Class</span>[_],</span><br><span class="line">      outputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">OutputFormat</span>[_, _]],</span><br><span class="line">      conf: <span class="type">JobConf</span> = <span class="keyword">new</span> <span class="type">JobConf</span>(self.context.hadoopConfiguration),</span><br><span class="line">      codec: <span class="type">Option</span>[<span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>]] = <span class="type">None</span>): <span class="type">Unit</span> = self.withScope &#123;</span><br><span class="line">    <span class="comment">// Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038).</span></span><br><span class="line">    <span class="comment">// 从SparkContext中复制一份haoopConf，并更新自己需要的配置，而原始hadoopConf保存不变</span></span><br><span class="line">    <span class="keyword">val</span> hadoopConf = conf</span><br><span class="line">    hadoopConf.setOutputKeyClass(keyClass)</span><br><span class="line">    hadoopConf.setOutputValueClass(valueClass)</span><br><span class="line">    conf.setOutputFormat(outputFormatClass)</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 将输出路径更新到复制出来的hadoopConf中</span></span><br><span class="line">    <span class="type">FileOutputFormat</span>.setOutputPath(hadoopConf,</span><br><span class="line">      <span class="type">SparkHadoopWriterUtils</span>.createPathFromString(path, hadoopConf))</span><br><span class="line">    <span class="comment">// 在这里将最新的JobConf更新到SparkContext中</span></span><br><span class="line">    self.context.setJobConfig(hadoopConf)</span><br><span class="line">    saveAsHadoopDataset(hadoopConf)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>SparkContext 中添加更新 JobConfig 的方法，对外提供访问入口。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/SparkContext.scala</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _jobConf: <span class="type">JobConf</span> = _</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setJobConfig</span></span>(jobConf: <span class="type">JobConf</span>): <span class="type">Unit</span> = <span class="keyword">this</span>._jobConf = jobConf</span><br></pre></td></tr></table></figure>

<p>前面将最新的 JobConf 更新到 SparkContext 中了，同时 FileOutputFormat 中也提供 getOutputPath(JobConf conf) 方法获取输出路径，此时收集输出路径的方式也比较容易。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collectOutputPath</span></span>[<span class="type">T</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">HashSet</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> outputHDFSPath = <span class="type">HashSet</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">if</span> (rdd.context.getJobConf ne <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> outputPath = <span class="type">FileOutputFormat</span>.getOutputPath(rdd.context.getJobConf)</span><br><span class="line">    <span class="keyword">if</span> (outputPath != <span class="literal">null</span> &amp;&amp; !outputHDFSPath.contains(outputPath.toString)) &#123;</span><br><span class="line">      outputHDFSPath.add(outputPath.toString)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  outputHDFSPath</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此，saveAsHadoopFile 方式已经能够收集到任务的输入输出路径血缘关系，saveAsNewAPIHadoopFile 也类似。</p>
<h2 id="2-2-DataSet方式实现"><a href="#2-2-DataSet方式实现" class="headerlink" title="2.2 DataSet方式实现"></a>2.2 DataSet方式实现</h2><p>DataSet 方式 write.save 逻辑最终通过执行 InsertIntoHadoopFsRelationCommand 命令，调用 FileFormatWriter#write 方法实现数据的写入，整体思路和 RDD 方式类似，先设置包含输出路径的 JobConf 到 SparkContext 中，然后在作业提交前收集路径血缘关系。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(</span><br><span class="line">      sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">      plan: <span class="type">SparkPlan</span>,</span><br><span class="line">      fileFormat: <span class="type">FileFormat</span>,</span><br><span class="line">      committer: <span class="type">FileCommitProtocol</span>,</span><br><span class="line">      outputSpec: <span class="type">OutputSpec</span>,</span><br><span class="line">      hadoopConf: <span class="type">Configuration</span>,</span><br><span class="line">      partitionColumns: <span class="type">Seq</span>[<span class="type">Attribute</span>],</span><br><span class="line">      bucketSpec: <span class="type">Option</span>[<span class="type">BucketSpec</span>],</span><br><span class="line">      statsTrackers: <span class="type">Seq</span>[<span class="type">WriteJobStatsTracker</span>],</span><br><span class="line">      options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">      table: <span class="type">Option</span>[<span class="type">CatalogTable</span>] = <span class="type">None</span>)</span><br><span class="line">    : <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> job = <span class="type">Job</span>.getInstance(hadoopConf)</span><br><span class="line">    job.setOutputKeyClass(classOf[<span class="type">Void</span>])</span><br><span class="line">    job.setOutputValueClass(classOf[<span class="type">InternalRow</span>])</span><br><span class="line">    <span class="comment">// 和RDD方式一样，复制出hadoopConf然后做自定义修改</span></span><br><span class="line">    <span class="type">FileOutputFormat</span>.setOutputPath(job, <span class="keyword">new</span> <span class="type">Path</span>(outputSpec.outputPath))</span><br><span class="line">    <span class="comment">// 这里复用RDD方式，将更新的JobConf设置到SparkContext，以获取输出路径</span></span><br><span class="line">    sparkSession.sparkContext.setJobConfig(<span class="keyword">new</span> <span class="type">JobConf</span>(job.getConfiguration))</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 和RDD方式一样，作业提交前收集路径血缘关系</span></span><br><span class="line">    collectHDFSPathLineage(rdd, plan)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobIdInstant = <span class="keyword">new</span> <span class="type">Date</span>().getTime</span><br><span class="line">    <span class="keyword">val</span> ret = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">WriteTaskResult</span>](rddWithNonEmptyPartitions.partitions.length)</span><br><span class="line">    sparkSession.sparkContext.runJob(</span><br><span class="line">      rddWithNonEmptyPartitions,</span><br><span class="line">      (taskContext: <span class="type">TaskContext</span>, iter: <span class="type">Iterator</span>[<span class="type">InternalRow</span>]) =&gt; &#123;</span><br><span class="line">        executeTask(</span><br><span class="line">          description = description,</span><br><span class="line">          jobIdInstant = jobIdInstant,</span><br><span class="line">          sparkStageId = taskContext.stageId(),</span><br><span class="line">          sparkPartitionId = taskContext.partitionId(),</span><br><span class="line">          sparkAttemptNumber = taskContext.taskAttemptId().toInt &amp; <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>,</span><br><span class="line">          committer,</span><br><span class="line">          iterator = iter,</span><br><span class="line">          concurrentOutputWriterSpec = concurrentOutputWriterSpec)</span><br><span class="line">      &#125;,</span><br><span class="line">      rddWithNonEmptyPartitions.partitions.indices,</span><br><span class="line">      (index, res: <span class="type">WriteTaskResult</span>) =&gt; &#123;</span><br><span class="line">        committer.onTaskCommit(res.commitMsg)</span><br><span class="line">        ret(index) = res</span><br><span class="line">      &#125;)</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>唯一和 RDD 收集输入路径不同的是，DataSet 方式不是通过 finalRDD 递归收集，而是通过 SparkPlan 物理计划解析，这里主要是考虑到 finalRDD 的关系图并不能直接获取到输入路径信息，而是路径下文件的 split 信息，这样就需要对大量 split 文件进行解析然后提取真实路径，比起 SparkPlan 复杂很多。实现方式如下。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 位置：org/apache/spark/sql/execution/datasources/FileFormatWriter.scala</span></span><br><span class="line">  <span class="comment">// 输入路径获取，通过解析SparkPlan实现</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">collectInputPath</span></span>[<span class="type">T</span>](sparkPlan: <span class="type">SparkPlan</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    sparkPlan <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> fileScan: <span class="type">FileSourceScanExec</span> =&gt;</span><br><span class="line">        <span class="keyword">val</span> fileSeq = fileScan.relation.location.rootPaths.map(_.toString)</span><br><span class="line">        fileSeq.foreach(inputHDFSPath.add(_))</span><br><span class="line">      <span class="keyword">case</span> unaryExecNode: <span class="type">UnaryExecNode</span> =&gt;</span><br><span class="line">        collectInputPath(unaryExecNode.child)</span><br><span class="line">      <span class="keyword">case</span> binaryExecNode: <span class="type">BinaryExecNode</span> =&gt;</span><br><span class="line">        collectInputPath(binaryExecNode.left)</span><br><span class="line">        collectInputPath(binaryExecNode.right)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">UnionExec</span>(child) =&gt; child.foreach(collectInputPath(_))</span><br><span class="line">      <span class="keyword">case</span> adaptiveSparkPlanExec: <span class="type">AdaptiveSparkPlanExec</span> =&gt;</span><br><span class="line">        collectInputPath(adaptiveSparkPlanExec.inputPlan)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 输出路径获取与RDD方式相同</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">collectOutputPath</span></span>[<span class="type">T</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">HashSet</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> outputHDFSPath = <span class="type">HashSet</span>[<span class="type">String</span>]()</span><br><span class="line">    <span class="keyword">if</span> (rdd.context.getJobConf ne <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> outputPath = <span class="type">FileOutputFormat</span>.getOutputPath(rdd.context.getJobConf)</span><br><span class="line">      <span class="keyword">if</span> (outputPath != <span class="literal">null</span> &amp;&amp; !outputHDFSPath.contains(outputPath.toString)) &#123;</span><br><span class="line">        outputHDFSPath.add(outputPath.toString)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    outputHDFSPath</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此，也拿到了 DataSet 方式的路径血缘关系。（完）</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
</search>
